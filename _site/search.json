[
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html",
    "href": "posts/2017-05-10-aws-rds-r/index.html",
    "title": "Amazon RDS + R",
    "section": "",
    "text": "Intro\nWelcome to my first post! To start things off at Data Insights, I’m going to show you how to connect to an AWS RDS instance from R.\nFor those of you who don’t know, RDS is an easy way to create a database in the cloud. In this post, I won’t be showing you how to setup an RDS instance, but I will show you how to connect to it if you have one running.\nLet’s get started.\n\n\nStep 1: The one where you got connected\nYou’ll need a few packages to get started.\n\nDBI and RMySQL are used to connect to the database, although RMySQL is usually called without explicitely loading it (that’s just the standard)\ntidyquant is just there to help us download some data to put in and get out of our database\ndplyr will be used to show off an alternate way to query from the database. Note that you should get the most recent github version of dplyr, along with the database specific pieces from dbplyr.\n\n\nlibrary(DBI)\n# library(RMySQL)\n\nlibrary(tidyquant)\n\n# devtools::install_github(\"tidyverse/dplyr\")\n# devtools::install_github(\"tidyverse/dbplyr\")\nlibrary(dplyr)\n# library(dbplyr)\n\nGetting connected isn’t too hard once you know what you’re looking for.\n\ncn <- dbConnect(drv      = RMySQL::MySQL(), \n                username = \"user1\", \n                password = \"testpassword\", \n                host     = \"davisdbinstance.crarljboc8to.us-west-2.rds.amazonaws.com\", \n                port     = 3306, \n                dbname   = \"firstdb\")\n\nLet’s go through the arguments to dbConnect(), the function from DBI that we used to connect.\n\ndrv - The driver I used is from the RMySQL package, an implementation of the general interface provided by DBI. I’ll leave it to the experts to explain all of this.\nusername / password - You’ll have to have created a user and password on AWS first, but then you can use them here.\nhost The host name is the Endpoint of your RDS server, without the port on the end. I’ve attached a screenshot to show where to find this. Basically, on the RDS Dashboard Instances page, hit the drop down arrow beside “MySQL” to show the Endpoint.\n\n\n\nport - The rest of the Endpoint shows the port that you’ll need to access your RDS instance through. That goes here.\ndbname - Finally, you’ll need the DB Name you used when setting up the instance. This can be found by clicking Instance Actions -> See Details, and then under Configuration Details you’ll find DB Name.\n\n\n\nStep 2: The one where you take it for a test spin\nWell, alright…that was…cool? How do we know it’s working? Let’s get some data to load into the database. We will use some Apple stock data retrieved through tidyquant.\n\naapl <- tq_get(\"AAPL\")\n\nslice(aapl, 1:10)\n\n\n\n\nTo write the tibble (data frame) to the database, we will use another function called dbWriteTable(). It’s pretty straightforward. “name” is the name of the table you are creating, and “value” is the data frame you want to write.\n\ndbWriteTable(cn, name = \"apple\", value = aapl)\n\nNow the fun part! Let’s use a SQL query to pull it back down with dbGetQuery(). This function is a combination of dbSendQuery(), which returns a result set for your query, and dbFetch() which returns the rows from that result set.\n\napple_from_db <- dbGetQuery(cn, \"SELECT * FROM apple;\")\n\n# This effectively is the same as\n# dbReadTable(cn, \"apple\")\n\nslice(apple_from_db, 1:10)\n\nThere are a huge number of functions from DBI that you can use to communicate with databases. Maybe I will cover more in a separate post, but for now, let’s move on to dplyr.\n\n\nStep 3: The one with the pliers\nBefore dplyr 0.6.0 was announced, you’d have to disconnect, and then reconnect through a dplyr specific function, src_mysql(). That would look something like the code below. Since then, however, you can now use the DBI connection with dplyr!\n\n# There is no need for this code anymore!\ndbDisconnect(cn)\n\ncn <- src_mysql(user     = \"user1\",\n                password = \"testpassword\",\n                host     = \"davisdbinstance.crarljboc8to.us-west-2.rds.amazonaws.com\",\n                port     = 3306,\n                dbname   = \"firstdb\")\n\nSelect the apple table from the database. This does not actually pull the data into memory. It just makes a connection!\n\n# With dplyr 0.6.0 we can just use the DBI connection!\napple_table <- tbl(cn, \"apple\")\n\n# By default the first 1000 rows are displayed\napple_table\n\nThe best part is that we can use almost any dplyr command with this! It queries the database, and does not do the manipulation in R. All of the familiar syntax of dplyr, but with databases. Let’s use filter() to get all of the rows after January 1, 2009.\n\nfilter(apple_table, date > \"2009-01-01\")\n\nTo do any serious manipulation outside of dplyr, you’ll likely have pull the data into memory to be able to use it with other R functions. Here, I’ll use the dplyr equivalent to dbWriteTable() to add the stock prices for IBM to the database.\n\nibm <- tq_get(\"IBM\")\n\ncopy_to(cn, df = ibm, temporary = FALSE)\n\nTo actually retrieve the data to memory, first make the connection using tbl() like before, and then use collect() to create the in memory tibble. Unfortunately, dates are stored as characters in the table, and collect() won’t try to fix that, so I’ll also take advantage of the readr package’s type_convert() function to do the thinking for me.\nOnce we have the data in memory, we can calculate the daily return with tidyquant and tq_mutate().\n\n# Connection\nibm_table <- tbl(cn, \"ibm\")\n\n# Collect to tibble\nreal_tibble <- collect(ibm_table) %>%\n  readr::type_convert()\n\n# Daily return\nreal_tibble <- real_tibble %>% \n  tq_mutate(select     = adjusted, \n            mutate_fun = periodReturn, \n            period     = \"daily\")\n\nreal_tibble\n\nAlways disconnect when you’re finished!\n\ndbDisconnect(cn)\n\n\n\nLast words\nHopefully I’ve been able to show you the power of DBI + dplyr with Amazon RDS. This integration has come a long way, and is just one of the huge advancements that the RStudio team has been working on in collaboration with other R users in the community.\nUntil next time!"
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "",
    "text": "1 AWS EC2 server with an elastic IP address\n1 Route 55 Amazon domain linked to the EC2 elastic IP (davisvaughan.com)\nRStudio Server\nShiny Server\n\nIn case I ever have to go through this madness again, or if anyone else wants to, I’ve compiled some step by step notes on the setup. It’s definitely worth it, though, so that you can have your own RStudio and Shiny servers!\n(I know that some others have already done posts like this, but I went into even more laborious detail on some of the basics.)\nIn this post, I will walk you through getting up and running with an RStudio server. In the next post, you’ll learn to get Shiny server working.\n\nStep 1: Setup an AWS Account\nAmazon is nice enough to provide 1 year’s worth of access to their Free Tier for AWS. There are a huge number of options available, but the important one is that they provide a free 750 hours/month to deploy an EC2 instance. That’s just enough to keep 1 EC2 instance active 24/7, since 24 hours x 31 days = 744.\nIf you aren’t familiar with EC2, think of it as your own personal always-on Linux computer that you can connect to through SSH, and access through the web by using an IP address. One step further and you can access it through a custom domain name.\nCreate your free AWS account, and come back when you’ve finished. You should be able to click on the giant sign in button, and sign in to your console.\n\nIf all goes well, you’ll be at the console.\n\nWe won’t do anything else yet, just stay signed in.\n\n\nStep 2: Setup the RStudio Amazon Machine Image\nIt’s worth it to get familiar with setting up your own EC2 server, but we won’t have to do that here. Luckily, Louis Aslett has created an Amazon Machine Image (AMI) to take care of all of the hard work for us. It’s basically some preconfigured settings that at the time of writing install the following:\n\nRStudio Server 0.99.903\nR 3.3.1\nShiny Server\nJulia 0.4.6\nPython 3.5.2\nGit\n\nYou can find the link to the image here. Click one of the links on the right to start the setup, I normally click the one closest to me regionally.\nThe Virginia link takes me here:\n\nYou can click through the settings, but to just get setup, click “Review and Launch.” It will let you review one last time, and will likely warn you about security, we will change all that later, just click “Launch.”\nImportant! Amazon will pop up a message box that talks about a key pair. This is how you will SSH into your server later on. This is really important, as you only get this screen one time, and can never come back to it. Setup a new key pair name (it can be anything), and click “Download Key Pair.”\n\nStore the .pem key pair file somewhere on your local computer. This should be a secure location, but somewhere you can remember the file path to. Then click Launch Instances.\nAt the top of the next screen, click Services, and then select EC2. This will take you to the EC2 Dashboard. You should see that you have “1 Running Instance.”\n\nClick on “1 Running Instance,” and you’ll see your server starting up. Below, it’s the one that says “running.”\n\nThere’s one last thing to do before we can access the server. We have to setup the security to allow HTTP (web browser) access. In the “Description” tab in the bottom half of the above image, scroll down until you see “Security Groups.” You’ll likely have something like “launch-wizard-1” there. Click on that.\nOn the next screen, click the “Inbound” tab down where “Description” is listed. As you can see, only the SSH option is available for accessing the instance. Let’s change that.\nClick:\n\nEDIT -> Add Rule -> Type set it to HTTP -> change the source from Custom to Anywhere -> Save\n\nNote that this is not a secure option, but it’ll get you going.\nFinally, to check that you’re up and running, go back to your instances tab (the same image as above). See the Public DNS (IPv4) box? Copy that, and paste it into your browser as a URL. It should take you to an authentication page for RStudio Server. Congrats! You’ve figured something out that took me hours.\nDefaults:\n\nUsername - rstudio\nPassword - rstudio\n\nClever, right?\n\nIf it worked, you should see this.\n\n\n\nStep 3: New password for RStudio Server\nIt’s advised that you immediately change the password. There are two ways to do so. The first way is easy. In the Welcome.R file that is shown above, you’ll see a description for how to library(\"RStudioAMI\") and then run passwd(). You can do that, but eventually you’ll have to SSH into your server for something, so you may as well learn how now.\nHave you still got the AWS Console Instances page up? The one where you found the Public DNS (IPv4). Here it is again.\n\nAWS has made it pretty easy to connect through SSH. Click the “Connect” button. A window should pop up with some pretty detailed instructions. Do you have the path to your .pem file lying around? You’re going to need it!\n\nI run on a Mac, so I’ll be using Terminal. If you run on Windows, you’ll need to download PuTTY. Open up Terminal, and type in the following for step 3:\n\nchmod 400 path_to_file/file.pem\n\nNote that you actually need to locate your pem file, and pass Terminal the path. This command hides the file, and is necessary to connect.\nNext you’ll connect to your instance by typing:\n\nssh -i \"path_to_file/file.pem\" ubuntu@ec2-IPADDRESS.compute-1.amazonaws.com\n\nAgain, you’ll have to type in the correct path, but the IP address shown for you should be correct.\nWhen you connect for the first time, it might give you a prompt basically saying, “Are you sure?” Type yes. Hopefully you’ll see something like this:\n\n\n\n\n\nTo update the password for the rstudio user:\n\nsudo passwd rstudio\n\nThen follow the prompts. Type exit to disconnect from the server, and go back to your RStudio Server site. Try and login with the new password.\n\n\nStep 4: Update everything\nUnfortunately, the Amazon Images are only updated every few releases of RStudio Server. However, it’s not too hard to get the newest release installed straight from RStudio’s site.\nYou’ll need to first set the CRAN mirror on your Ubuntu server so that you can actually download the latest version of R. This part is a bit of a pain, requiring you to work with some text editors through Terminal, but bear with me.\nSign back into your Linux server through Terminal following the above instructions. When you’re done, type:\n\nls\n# rstudio-server-1.0.143-amd64.deb  shiny-server-1.5.3.838-amd64.deb\n\nAnd you should see a few .deb files, one for rstudio-server and one for shiny-server (mine are already upgraded). If you don’t, well, hopefully you can still try and follow along (Maybe cd ~ will get you there? Maybe go back to step 1?).\nNow, we need to navigate to the correct file and add the CRAN mirror to it. That is located at /etc/apt/sources.list for you pros. For the rest of us, follow along.\nFirst navigate up two levels:\n\ncd ../..\nls\n# bin   etc         initrd.img.old            lib         media  proc  sbin  sys  var\n# boot  home        jupyterhub_cookie_secret  lib64       mnt    root  snap  tmp  vmlinuz\n# dev   initrd.img  jupyterhub.sqlite         lost+found  opt    run   srv   usr  vmlinuz.old\n\nThen, we need to get into etc/apt:\n\ncd etc/apt\nls\n# apt.conf.d     sources.list    sources.list.save  trusted.gpg~\n# preferences.d  sources.list.d  trusted.gpg        trusted.gpg.d\n\nI don’t have a whole lot of experience with terminal editors, but I know enough to get by. I will use nano, which I believe comes on every Mac, to open up my sources.list file. sudo is likely needed to give admin privelages so you can save the file afterwards.\n\nsudo nano sources.list\n\nA file should open, scroll all the way down to the bottom, and on a new line paste:\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu/ xenial/\n\nThere are a number of different versions of this command here, but this specific one works because the Amazon Image you downloaded uses Xenial (tbh I don’t really know what that means, trial and error and a bit of common sense got it to work).\nNow you have to escape from nano, a first-timer’s nightmare. Follow this sequence of commands:\n\n^X # Control+X      This is used to \"Quit\"\nY  # Yes            This is used to save the file when it asks you\n# Then click Enter/Return on your keyboard to resave the file with the same name\n\nNow that that is taken care of, navigate back to:\n\ncd ~\n\nAnd you can update all of the linux apps, and then download the latest version of R using the two commands:\n\nsudo apt-get update\nsudo apt-get install r-base\n\nFinally, you’ll update to the latest version of RStudio Server. At the time of writing, this is 1.0.143, but it updates regularly, so go here and scroll down to find the latest update for 64bit Ubuntu. The commands generally look like:\n\nsudo apt-get install gdebi-core\nwget https://download2.rstudio.org/rstudio-server-1.0.143-amd64.deb\nsudo gdebi rstudio-server-1.0.143-amd64.deb\n\nAll done? Great! exit out of your Linux server, and reload your RStudio Server in the browser. When you login, you should be able to run version to see the latest version of R, and go to Help -> About RStudio to see the updated version of RStudio Server!\n\n\nLast words\nThis was quite the struggle. There are a few other resources out there to help, but I still struggled through some pieces of this one. Hopefully it wasn’t near as bad for you! In the next post, I’ll show you how to update your Shiny Server and start hosting your own apps on there (with no 5 app limit like shinyapps.io)!\nHere are some additional resources that I found helpful when setting up my server:\n\nhttps://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/#user-libraries"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Davis Vaughan",
    "section": "",
    "text": "Aug 28, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Davis, a Software Engineer at RStudio working on creating user friendly R packages with the rest of the tidyverse team."
  },
  {
    "objectID": "about.html#what-else",
    "href": "about.html#what-else",
    "title": "About",
    "section": "What else?",
    "text": "What else?\nI’ve developed a few R packages:\n\nfurrr\nslider\nclock\nalmanac\nhardhat\nworkflows\nprobably\n\nAnd I work on many more, including:\n\ntidyr\nvctrs\nyardstick"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html",
    "href": "posts/2017-08-16-hadley-pleased/index.html",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "",
    "text": "Required packages\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rvest)\nlibrary(xml2)\n\n\n\nExtract the HTML from the RStudio blog archive\nTo be able to extract the text from each blog post, we first need to have a link to that blog post. Luckily, RStudio keeps an up to date archive page that we can scrape. Using xml2, we can get the HTML off that page.\n\narchive_page <- \"https://blog.rstudio.com/archives/\"\n\narchive_html <- read_html(archive_page)\n\n# Doesn't seem very useful...yet\narchive_html\n\n\n\n{xml_document}\n<html lang=\"en-us\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\\n<meta charset=\"u ...\n[2] <body>\\n    <nav class=\"menu\"><svg version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xli ...\n\n\nNow we use a bit of rvest magic combined with the HTML inspector in Chrome to figure out which elements contain the info we need (I also highly recommend SelectorGadget for this kind of work). Looking at the image below, you can see that all of the links are contained within the main tag as a tags (links).\n\nThe code below extracts all of the links, and then adds the prefix containing the base URL of the site.\n\nlinks <- archive_html %>%\n  \n  # Only the \"main\" body of the archive\n  html_nodes(\"main\") %>%\n  \n  # Grab any node that is a link\n  html_nodes(\"a\") %>%\n  \n  # Extract the hyperlink reference from those link tags\n  # The hyperlink is an attribute as opposed to a node\n  html_attr(\"href\") %>%\n  \n  # Prefix them all with the base URL\n  paste0(\"http://blog.rstudio.com\", .)\n\nhead(links)\n\n\n\n[1] \"http://blog.rstudio.com/2017/08/25/rstudio-conf-2018-early-bird-pricing/\"    \n[2] \"http://blog.rstudio.com/2017/08/22/rstudio-v1-1-preview-object-explorer/\"    \n[3] \"http://blog.rstudio.com/2017/08/18/google-cloud-platform/\"                   \n[4] \"http://blog.rstudio.com/2017/08/16/rstudio-preview-connections/\"             \n[5] \"http://blog.rstudio.com/2017/08/15/contributed-talks-diversity-scholarships/\"\n[6] \"http://blog.rstudio.com/2017/08/15/shiny-1-0-4/\"                             \n\n\n\n\nHTML from each blog post\nNow that we have every link, we’re ready to extract the HTML from each individual blog post. To make things more manageable, we start by creating a tibble, and then using the mutate + map combination to created a column of XML Nodesets (we will use this combination a lot). Each nodeset contains the HTML for that blog post (exactly like the HTML for the archive page).\n\nblog_data <- tibble(links)\n\nblog_data <- blog_data %>%\n  mutate(main = map(\n                    # Iterate through every link\n                    .x = links, \n                    \n                    # For each link, read the HTML for that page, and return the main section \n                    .f = ~read_html(.) %>%\n                            html_nodes(\"main\")\n                    )\n         )\n\nblog_data$main[1]\n\n\n\n[[1]]\n{xml_nodeset (1)}\n[1] <main><div class=\"article-meta\">\\n<h1><span class=\"title\">Newer to R? rstudio::conf 2018 is fo ...\n\n\n\n\nMeta information\nBefore extracting the blog post itself, lets grab the meta information about each post, specifically:\n\nAuthor\nTitle\nDate\nCategory\nTags\n\nIn the exploratory analysis, we will use author and title, but the other information might be useful for future analysis.\nLooking at the first blog post, the Author, Date, and Title are all HTML class names that we can feed into rvest to extract that information.\n\nIn the code below, an example of extracting the author information is shown. To select a HTML class (like “author”) as opposed to a tag (like “main”), we have to put a period in front of the class name. Once the html node we are interested in has been identified, we can extract the text for that node using html_text().\n\nblog_data$main[[1]] %>%\n  html_nodes(\".author\") %>%\n  html_text()\n\n\n\n[1] \"Roger Oberg\"\n\n\nTo scale up to grab the author for all posts, we use map_chr() since we want a character of the author’s name returned.\n\nmap_chr(.x = blog_data$main,\n        .f = ~html_nodes(.x, \".author\") %>%\n                html_text()) %>%\n  head(10)\n\n\n\n [1] \"Roger Oberg\"        \"Kevin Ushey\"        \"Roger Oberg\"       \n [4] \"Jonathan McPherson\" \"Hadley Wickham\"     \"Winston Chang\"     \n [7] \"Gary Ritchie\"       \"Roger Oberg\"        \"Jeff Allen\"        \n[10] \"Javier Luraschi\"   \n\n\nFinally, notice that if we switch \".author\" with \".title\" or \".date\" then we can grab that information as well. This kind of thinking means that we should create a function for extracting these pieces of information!\n\nextract_info <- function(html, class_name) {\n  map_chr(\n          # Given the list of main HTMLs\n          .x = html,\n          \n          # Extract the text we are interested in for each one \n          .f = ~html_nodes(.x, class_name) %>%\n                  html_text())\n}\n\n# Extract the data\nblog_data <- blog_data %>%\n  mutate(\n     author = extract_info(main, \".author\"),\n     title  = extract_info(main, \".title\"),\n     date   = extract_info(main, \".date\")\n    )\n\n\nselect(blog_data, author, date)\n\n\n\n# A tibble: 253 × 2\n   author             date      \n   <chr>              <chr>     \n 1 Roger Oberg        2017-08-25\n 2 Kevin Ushey        2017-08-22\n 3 Roger Oberg        2017-08-18\n 4 Jonathan McPherson 2017-08-16\n 5 Hadley Wickham     2017-08-15\n 6 Winston Chang      2017-08-15\n 7 Gary Ritchie       2017-08-11\n 8 Roger Oberg        2017-08-10\n 9 Jeff Allen         2017-08-03\n10 Javier Luraschi    2017-07-31\n# … with 243 more rows\n\n\n\nselect(blog_data, title)\n\n\n\n# A tibble: 253 × 1\n   title                                                                        \n   <chr>                                                                        \n 1 Newer to R? rstudio::conf 2018 is for you! Early bird pricing ends August 31.\n 2 RStudio v1.1 Preview - Object Explorer                                       \n 3 RStudio Server Pro is ready for BigQuery on the Google Cloud Platform        \n 4 RStudio 1.1 Preview - Data Connections                                       \n 5 rstudio::conf(2018): Contributed talks, e-posters, and diversity scholarships\n 6 Shiny 1.0.4                                                                  \n 7 RStudio v1.1 Preview: Terminal                                               \n 8 Building tidy tools workshop                                                 \n 9 RStudio Connect v1.5.4 - Now Supporting Plumber!                             \n10 sparklyr 0.6                                                                 \n# … with 243 more rows\n\n\n\n\nCategories and tags\nThe other bits of meta data that might be interesting are the categories and tags that the post falls under. This is a little bit more involved, because both the categories and tags fall under the same class, \".terms\". To separate them, we need to look into the href to see if the information is either a tag or a category (href = “/categories/” VS href = “/tags/”).\n\nThe function below extracts either the categories or the tags, depending on the argument, by:\n\nExtracting the \".terms\" class, and then all of the links inside of it (a tags).\nChecking each link to see if the hyperlink reference contains “categories” or “tags” depending on the one that we are interested in. If it does, it returns the text corresponding to that link, otherwise it returns NAs which are then removed.\n\nThe final step results in two list columns containing character vectors of varying lengths corresponding to the categories and tags of each post.\n\nextract_tag_or_cat <- function(html, info_name) {\n  \n  # Extract the links under the terms class\n  cats_and_tags <- map(.x = html, \n                       .f = ~html_nodes(.x, \".terms\") %>%\n                              html_nodes(\"a\"))\n  \n  # For each link, if the href contains the word categories/tags \n  # return the text corresponding to that link\n  map(cats_and_tags, \n    ~if_else(condition = grepl(info_name, html_attr(.x, \"href\")), \n             true      = html_text(.x), \n             false     = NA_character_) %>%\n      .[!is.na(.)])\n}\n\n# Apply our new extraction function\nblog_data <- blog_data %>%\n  mutate(\n    categories = extract_tag_or_cat(main, \"categories\"),\n    tags       = extract_tag_or_cat(main, \"tags\")\n  )\n\n\nselect(blog_data, categories, tags)\n\n\n\n# A tibble: 253 × 2\n   categories tags     \n   <list>     <list>   \n 1 <chr [3]>  <chr [1]>\n 2 <chr [1]>  <chr [0]>\n 3 <chr [2]>  <chr [4]>\n 4 <chr [1]>  <chr [0]>\n 5 <chr [1]>  <chr [0]>\n 6 <chr [2]>  <chr [0]>\n 7 <chr [1]>  <chr [3]>\n 8 <chr [3]>  <chr [8]>\n 9 <chr [3]>  <chr [2]>\n10 <chr [1]>  <chr [3]>\n# … with 243 more rows\n\n\n\nblog_data %>%\n  filter(title == \"Building tidy tools workshop\") %>%\n  pull(categories)\n\n\n\n[[1]]\n[1] \"Packages\"  \"tidyverse\" \"Training\" \n\n\n\nblog_data %>%\n  filter(title == \"Building tidy tools workshop\") %>%\n  pull(tags)\n\n\n\n[[1]]\n[1] \"Advanced R\"       \"data science\"     \"ggplot2\"          \"Hadley Wickham\"  \n[5] \"R\"                \"RStudio Workshop\" \"r training\"       \"tutorial\"        \n\n\n\n\nThe blog post itself\nFinally, to extract the blog post itself, we can notice that each piece of text in the post is inside of a paragraph tag (p). Being careful to avoid the \".terms\" class that contained the categories and tags, which also happens to be in a paragraph tag, we can extract the full blog posts. To ignore the \".terms\" class, use the :not() selector.\n\nblog_data <- blog_data %>%\n  mutate(\n    text = map_chr(main, ~html_nodes(.x, \"p:not(.terms)\") %>%\n                 html_text() %>%\n                 # The text is returned as a character vector. \n                 # Collapse them all into 1 string.\n                 paste0(collapse = \" \"))\n  )\n\n\n\n\n\nselect(blog_data, text)\n\n# A tibble: 253 × 1\n   text                                                                         \n   <chr>                                                                        \n 1 \"Immersion is among the most effective ways to learn any language. Immersing…\n 2 \"Today, we’re continuing our blog series on new features in RStudio 1.1. If …\n 3 \"RStudio is excited to announce the availability of RStudio Server Pro on th…\n 4 \"Today, we’re continuing our blog series on new features in RStudio 1.1. If …\n 5 \"rstudio::conf, the conference on all things R and RStudio, will take place …\n 6 \"Shiny 1.0.4 is now available on CRAN. To install it, run: For most Shiny us…\n 7 \"Today we’re excited to announce availability of our first Preview Release f…\n 8 \"Have you embraced the tidyverse? Do you now want to expand it to meet your …\n 9 \"We’re thrilled to announce support for hosting Plumber APIs in RStudio Conn…\n10 \"We’re excited to announce a new release of the sparklyr package, available …\n# … with 243 more rows\n\n\n\n\nWho writes the most posts?\nNow that we have all of this data, what can we do with it? To start with, who writes the most posts?\n\nblog_data %>%\n  group_by(author) %>%\n  summarise(count = n()) %>%\n  mutate(author = reorder(author, count)) %>%\n  \n  # Create a bar graph of author counts\n  ggplot(mapping = aes(x = author, y = count)) + \n  geom_col() +\n  coord_flip() +\n  labs(title    = \"Who writes the most RStudio blog posts?\",\n       subtitle = \"By a huge margin, Hadley!\") +\n  # Shoutout to Bob Rudis for the always fantastic themes\n  hrbrthemes::theme_ipsum(grid = \"Y\")\n\n\n\n\n\n\nTidytext\nI’ve never used tidytext before today, but to get our feet wet, let’s create a tokenized tidy version of our data. By using unnest_tokens() the data will be reshaped to a long format holding 1 word per row, for each blog post. This tidy format lends itself to all manner of analysis, and a number of them are outlined in Julia Silge and David Robinson’s Text Mining with R.\n\ntokenized_blog <- blog_data %>%\n  mutate(short_title = str_sub(title, end = 15)) %>%\n  select(title, short_title, author, date, text) %>%\n  unnest_tokens(output = word, input = text)\n\nselect(tokenized_blog, short_title, word)\n\n# A tibble: 85,761 × 2\n   short_title     word     \n   <chr>           <chr>    \n 1 Newer to R? rst immersion\n 2 Newer to R? rst is       \n 3 Newer to R? rst among    \n 4 Newer to R? rst the      \n 5 Newer to R? rst most     \n 6 Newer to R? rst effective\n 7 Newer to R? rst ways     \n 8 Newer to R? rst to       \n 9 Newer to R? rst learn    \n10 Newer to R? rst any      \n# … with 85,751 more rows\n\n\n\n\nRemove stop words\nA number of words like “a” or “the” are included in the blog that don’t really add value to a text analysis. These stop words can be removed using an anti_join() with the stop_words dataset that comes with tidytext. After removing stop words, the number of rows was cut in half!\n\ntokenized_blog <- tokenized_blog %>%\n  anti_join(stop_words, by = \"word\") %>%\n  arrange(desc(date))\n\nselect(tokenized_blog, short_title, word)\n\n# A tibble: 40,315 × 2\n   short_title     word     \n   <chr>           <chr>    \n 1 Newer to R? rst immersion\n 2 Newer to R? rst effective\n 3 Newer to R? rst learn    \n 4 Newer to R? rst language \n 5 Newer to R? rst immersing\n 6 Newer to R? rst advanced \n 7 Newer to R? rst users    \n 8 Newer to R? rst improve  \n 9 Newer to R? rst language \n10 Newer to R? rst rare     \n# … with 40,305 more rows\n\n\n\n\nTop 15 words overall\nOut of pure curiousity, what are the top 15 words for all of the blog posts?\n\ntokenized_blog %>%\n  count(word, sort = TRUE) %>%\n  slice(1:15) %>%\n  mutate(word = reorder(word, n)) %>%\n  \n  ggplot(aes(word, n)) +\n  geom_col() + \n  coord_flip() + \n  labs(title = \"Top 15 words overall\") +\n  hrbrthemes::theme_ipsum(grid = \"Y\")\n\n\n\n\n\n\nIs Hadley more “pleased” than everyone else?\nAs mentioned at the beginning of the post, Hadley apparently uses the word “pleased” in his blog posts an above average number of times. Can we verify this statistically?\nOur null hypothesis is that the proportion of blog posts that use the word “pleased” written by Hadley is less than or equal to the proportion of those written by the rest of the RStudio team.\nMore simply, our null is that Hadley uses “pleased” less than or the same as the rest of the team.\nLet’s check visually to compare the two groups of posts.\n\npleased <- tokenized_blog %>%\n  \n  # Group by blog post\n  group_by(title) %>%\n  \n  # If the blog post contains \"pleased\" put yes, otherwise no\n  # Add a column checking if the author was Hadley\n  mutate(\n    contains_pleased = case_when(\n      \"pleased\" %in% word ~ \"Yes\",\n      TRUE                ~ \"No\"),\n    \n    is_hadley = case_when(\n      author == \"Hadley Wickham\" ~ \"Hadley\",\n      TRUE                       ~ \"Not Hadley\")\n    ) %>%\n  \n  # Remove all duplicates now\n  distinct(title, contains_pleased, is_hadley)\n\npleased %>%\n  ggplot(aes(x = contains_pleased)) +\n  geom_bar() +\n  facet_wrap(~is_hadley, scales = \"free_y\") +\n  labs(title    = \"Does this blog post contain 'pleased'?\", \n       subtitle = \"Nearly half of Hadley's do!\",\n       x        = \"Contains 'pleased'\",\n       y        = \"Count\") +\n  hrbrthemes::theme_ipsum(grid = \"Y\")\n\n\n\n\n\n\nIs there a statistical difference here?\nTo check if there is a statistical difference, we will use a test for difference in proportions contained in the R function, prop.test(). First, we need a continency table of the counts. Given the current form of our dataset, this isn’t too hard with the table() function from base R.\n\ncontingency_table <- pleased %>%\n  ungroup() %>%\n  select(is_hadley, contains_pleased) %>%\n  # Order the factor so Yes is before No for easy interpretation\n  mutate(contains_pleased = factor(contains_pleased, levels = c(\"Yes\", \"No\"))) %>%\n  table()\n\ncontingency_table\n\n            contains_pleased\nis_hadley    Yes  No\n  Hadley      43  45\n  Not Hadley  17 148\n\n\nFrom our null hypothesis, we want to perform a one sided test. The alternative to our null is that Hadley uses “pleased” more than the rest of the RStudio team. For this reason, we specify alternative = \"greater\".\n\ntest_prop <- contingency_table %>%\n  prop.test(alternative = \"greater\")\n\ntest_prop\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  .\nX-squared = 45.063, df = 1, p-value = 9.541e-12\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.2809899 1.0000000\nsample estimates:\n   prop 1    prop 2 \n0.4886364 0.1030303 \n\n\nWe could also tidy this up with broom if we were inclined to.\n\nbroom::tidy(test_prop)\n\n# A tibble: 1 × 9\n  estimate1 estimate2 statistic  p.value parameter conf.low conf.high method    \n      <dbl>     <dbl>     <dbl>    <dbl>     <dbl>    <dbl>     <dbl> <chr>     \n1     0.489     0.103      45.1 9.54e-12         1    0.281         1 2-sample …\n# … with 1 more variable: alternative <chr>\n\n\n\n\nTest conclusion\n\n48.86% of Hadley’s posts contain “pleased”\n10.3% of the rest of the RStudio team’s posts contain “pleased”\nWith a p-value of 9.5414477^{-12}, we reject the null that Hadley uses “pleased” less than or the same as the rest of the team. The evidence supports the idea that he has a much higher preference for it!\n\nHadley uses “pleased” quite a bit!\n\n\nConclusion\nThis post used a lot of different tools, but that’s the beauty of having over 12,000 R packages at our disposal. I think that this dataset could be used in a number of other ways, so be on the lookout for more posts!"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "",
    "text": "What are we doing today?\nIn this post, I’ll work through simulating paths of a stock that follows the log normal distribution used in the Black Scholes model. Importantly, I’ll explain my thought process as I tried to optimize the implementation from loops to vectors.\nAs an added bonus, at the very bottom is some extra content on a basic function that I have created to replicate the concept of broadcasting from Python. Someone could (should?) probably create an entire package out of this idea.\n\n\nThe Model\nUnder the Black Scholes model, the stock price, \\(S_t\\), at time t follows a Geometric Brownian Motion, which means that it satisfies the Stochastic Differential Equation:\n\\[ dS_t = r S_t dt + \\sigma S_t dW_t \\]\nWhere:\n\n\\(r =\\) Drift - The average log return per time unit\n\\(\\sigma =\\) Volatility - How much variance is in that drift\n\\(W_t =\\) Brownian Motion - Random noise from a normal distribution with mean 0 and variance t\n\nInterestingly, we actually have the solution to this equation (one of the few we have analytical solutions for):\n\\[ S_t = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t + \\sigma W_t} \\]\nMore generally, this can be written as a formula providing us with the recursive equation:\n\\[ S_{t_i} = S_{t_{i-1}} \\times e^{(r - \\frac{1}{2} \\sigma^2) ({t_i - t_{i-1}}) + \\sigma (W_{t_i} - W_{t_{i-1}}) } \\]\nIf you want to know how to get the solution, this is a pretty good explanation, but be prepared to learn about Ito’s Lemma.\n\n\nReally, dude?\nOkay, that’s a lot without any explanation, and I get that. But the point of this post is more to explain how to simulate paths of \\(S_t\\). So how do we do that?\n\nWe will start from time 0 with an initial stock price, then we will generate the next stock price from that using the recursive formula, and so on.\nThe only random piece is the brownian motion increment (dW), which we will generate at each time point using draws from a normal distribution.\nThe time increment will be a constant value (dt) to keep things simple.\n\nI was given some starting parameters:\n\n# Set a seed too so I can reproduce the exact numbers\nset.seed(123)\n\n# Parameters\nr       <- 0.028\nsigma   <- 0.255\ntime_T  <- 0.5\ndt      <- 1/12\nt_total <- time_T / dt\ns_0     <- 100\nN       <- 10000\n\nWhere time_T and dt mean each simulation path will go from time 0 to time_T by increments of dt. Dividing time_T / dt gets us the total number of time steps required. N is the number of paths to simulate.\n\n\nFirst attempt\nWe all know loops are to be avoided when you can in R, and that you should instead vectorize the operations. At first, I thought this wasn’t going to be possible, as this is a recursive type of formula where the next value relies on the previous one. With that in mind, I created the following implementation.\nFirst off, set up a matrix to fill with the 10000 simulations (one per row), each one having 6 time steps (7 columns total including the initial stock price).\n\n# Create a 10000x7 matrix of NA's to fill in\n# Each row is a simulation\ns <- matrix(NA_real_, nrow = N, ncol = t_total+1)\n\n# The first column is just the initial price\ns[,1] <- s_0\n\nhead(s)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]  100   NA   NA   NA   NA   NA   NA\n[2,]  100   NA   NA   NA   NA   NA   NA\n[3,]  100   NA   NA   NA   NA   NA   NA\n[4,]  100   NA   NA   NA   NA   NA   NA\n[5,]  100   NA   NA   NA   NA   NA   NA\n[6,]  100   NA   NA   NA   NA   NA   NA\n\n\nSo far so good, now let’s create a matrix for dW, our brownian motion increments. A very important fact is that these are all independent of each other, so the generation of them is straightforward. Each increment:\n\\[ W_{t_i} - W_{t_{i-1}} \\]\ncan be drawn from a normal distribution with mean 0 and variance \\(t_i - t_{i-1}\\) (which is what I have defined as dt because it is a constant).\n\n# ~N(0, dt)\n# To fill in 10000 simulations, and move forward 6 steps, we need a 10000x6 matrix\ndW <- rnorm(N * t_total, mean = 0, sd = sqrt(dt))\ndW <- matrix(dW, N, t_total)\nhead(dW)\n\n            [,1]        [,2]        [,3]        [,4]        [,5]        [,6]\n[1,] -0.16179538  0.68436942 -0.24141807 -0.05588936  0.13929365  0.07493710\n[2,] -0.06644652 -0.04815446 -0.06367394  0.07452070  0.20824004  0.26486253\n[3,]  0.44996033  0.26759071 -0.60723241 -0.15539745 -0.14658701 -0.20851535\n[4,]  0.02035402 -0.16401128 -0.48145457 -0.34036612 -0.01867981 -0.23333150\n[5,]  0.03732215  0.06497791 -0.31695458  0.25999452  0.37589000 -0.04080481\n[6,]  0.49509662  0.32677618 -0.48082343 -0.00469084 -0.06311503  0.65154366\n\n\nBased on this setup, I thought I would need a loop. The algorithm would step through the 10000 simulations all at once, but would have to loop through the 6 time steps one at a time, because each time step depended on the previous one. So, following the formula (below again for reference) I did this:\n\\[ S_{t_i} = S_{t_{i-1}} \\times e^{(r - \\frac{1}{2} \\sigma^2) ({t_i - t_{i-1}}) + \\sigma (W_{t_i} - W_{t_{i-1}}) } \\]\n\nfor(i in 1:(t_total)) {\n  s[,i+1] <- s[,i] * exp((r - 1/2 * sigma^2) * dt + sigma * dW[,i])\n}\n\nhead(s)\n\n     [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,]  100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n[2,]  100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n[3,]  100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n[4,]  100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n[5,]  100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n[6,]  100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\n\nAnd that does work! But can we avoid the loop? YES WE CAN!\n\n\nMATH\nTo avoid the loop, we are going to manipulate a simple case, and then apply it generally. One key element for this vectorization is that brownian motion increments are independent. Let’s think about what actually happens from time 0->1 and from 1->2.\n\\[ S_1 = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0) + \\sigma (W_1 - W_0)} \\] \\[ S_2 = S_1 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_1) + \\sigma (W_2 - W_1)} \\]\nIf we plug the equation for S_1 into the equation for S_2…\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0) + \\sigma (W_1 - W_0)}) \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_1) + \\sigma (W_2 - W_1)} \\]\nAnd then combine exponents…\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0 + t_2 - t_1) + \\sigma (W_1 - W_0 + W_2 - W_1)}) \\]\nNotice that some of the t and W terms cancel:\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_0) + \\sigma (W_2 - W_0)}) \\]\nAnd by definition t_0 and W_0 are 0:\n\\[ S_2 = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t_2 + \\sigma W_2} \\]\nThis is actually the form that was proposed as the solution to the geometric brownian motion stochastic differential equation:\n\\[ S_t = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t + \\sigma W_t} \\]\nIt looks like we can actually generate S_2 without needing to know S_1 at all. Notice that the exponent now contains t_2 and W_2. t_2 is known beforehand, but W_2 seems like it would rely on W_1 in a way that has to be recursively calculated. Actually, if we think of W_2 as a sum of brownian motion increments (I told you this would help):\n\\[ W_2 = (W_2 - W_1) + (W_1 - W_0) = dW_2 + dW_1 \\]\nthen W_2 is just the cumulative sum of the increments, and, by definition, each increment is independent of the previous increment so we can generate them all before hand (we already did this when we created the dW matrix).\n\n# Rowwise cumulative sum of dW generates W1, W2, W3, ... for each simulation\nW  <- plyr::aaply(dW, 1, cumsum)\nhead(W)\n\n   \nX1            1          2          3           4          5          6\n  1 -0.16179538  0.5225740  0.2811560  0.22526661  0.3645603  0.4394974\n  2 -0.06644652 -0.1146010 -0.1782749 -0.10375422  0.1044858  0.3693483\n  3  0.44996033  0.7175510  0.1103186 -0.04507883 -0.1916658 -0.4001812\n  4  0.02035402 -0.1436573 -0.6251118 -0.96547795 -0.9841578 -1.2174893\n  5  0.03732215  0.1023001 -0.2146545  0.04534000  0.4212300  0.3804252\n  6  0.49509662  0.8218728  0.3410494  0.33635853  0.2732435  0.9247872\n\n\nUnlike the recursive formula from before where dt was used, the time that we are currently at, t, is used instead so we will need that as well.\n\ntime_steps <- matrix(seq(from = dt, to = time_T, by = dt), \n                     nrow = N, ncol = t_total, byrow = TRUE)\nhead(time_steps)\n\n           [,1]      [,2] [,3]      [,4]      [,5] [,6]\n[1,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n[2,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n[3,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n[4,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n[5,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n[6,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n\n\nNow it’s a vectorized one-liner to calculate the stock price at each time!\n\n# Stock price simulation\ns_t <- s_0 * exp((r - 1/2 * sigma^2) * time_steps + sigma * W)\n\n# Add the original stock price onto the front\ns_t <- cbind(s_0, s_t)\n\n# Add 0 as the column name for initial value (it's important I promise)\ncolnames(s_t)[1] <- \"0\"\n\nhead(s_t)\n\n    0         1         2         3         4         5         6\n1 100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n2 100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n3 100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n4 100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n5 100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n6 100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\n\nJust as a sanity check, this should have produced the same results as the for loop\n\n# ignore the dimname attributes\nall.equal(s, s_t, check.attributes = FALSE)\n\n[1] TRUE\n\n\n\n\nNow what?\nThere are a number of interesting things we could do with these results. One is to calculate the fair price of a European Option on this stock. I think I’ll save that for the next post.\nSomething else we might do is visualize the distribution of \\(S_T\\), the stock price at the terminal (final) time. Because the stock price is modeled as an exponential of a normal random variable (W_t), the stock price itself has a log-normal distribution. For practicality, this means that it is right tailed and can’t drop below 0 (good properties of a stock).\n\nlibrary(tidyverse)\n\n# let's just take a moment and admire the fact that I can put LaTeX in ggplots\nlibrary(latex2exp) \n\ntibble::tibble(s_T = s_t[,6]) %>%\n  ggplot(mapping = aes(x=s_T)) +\n  geom_histogram(bins = 500) + \n  labs(x = TeX('S_T'), y = NULL, title = TeX('Log-normal Distribution of S_T') )\n\n\n\n\nWe could also look at the 6-step path of 100 of our simulations.\n\nas_tibble(s_t) %>%\n  rownames_to_column(var = \"iteration\") %>%\n  gather(time_step, stock_price, -iteration) %>%\n  mutate(time_step = as.numeric(time_step),\n         iteration = as.factor(iteration)) %>%\n  filter(iteration %in% 1:100) %>%\n  \n  ggplot(aes(x = time_step, y = stock_price, group = iteration)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Stock Price\", title = \"100 Simulated Paths\")\n\n\n\n\n\n\nConclusion + Extra Content\nIf I haven’t bored you to tears yet, allow me to thank you for sticking around this long. I think these posts are useful because they force me to try and understand a concept a bit more than if I was just reading it from a book.\nAs you may have noted in the post above, I had to create a large matrix time_steps for R to perform the matrix addition I wanted correctly. I thought it would have been simple. Ideally I could create a 1x6 matrix of times, and add it to a 10000x6 matrix of the brownian motions and have the times matrix broadcasted to each row of the brownian motion matrix, adding element by element to each row. This works in Python and Matlab, but R has a mind of it’s own.\n\n# First try with a vector\nx1 <- c(1,2)\nx1\n\n[1] 1 2\n\nx2 <- matrix(c(1,2,3,4), nrow = 2, ncol = 2, byrow = TRUE)\nx2\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n# I want to add c(1,2) to each row of x2\nx1 + x2\n\n     [,1] [,2]\n[1,]    2    3\n[2,]    5    6\n\n\nThat’s not right, it’s adding 1 to the first column of x2, then 2 to the second column of x2. To get what I want I could butcher it like this:\n\nt(x1+t(x2))\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    4    6\n\n\nIf x1 was a matrix instead of a vector, then it gives a non-conformable array error.\n\nx1 <- matrix(x1, ncol = 2)\nx1\n\n     [,1] [,2]\n[1,]    1    2\n\nx1 + x2\n\nError in x1 + x2: non-conformable arrays\n\n\nSo R is super strict here. That’s fine and all, but with other languages able to do this, and with it being such a natural way of thinking about this type of addition, I decided to roll my own function that allows me to add matrices together that meet certain conditions by broadcasting one of them over the other.\n\n# Broadcast addition\n# One of the special % % operators\n`%+%` <- function(e1, e2) {\n  \n  stopifnot(is.matrix(e1))\n  stopifnot(is.matrix(e2))\n  \n  # e1 - e2 & 1 has more rows & equal cols\n  if(nrow(e1) >= nrow(e2) & ncol(e1) == ncol(e2)) {\n    case <- \"1\"\n    \n    # e1 - e2 & 2 has more rows & equal cols\n  } else if(nrow(e1) < nrow(e2) & ncol(e1) == ncol(e2)) {\n    case <- \"2\"\n    \n    # e1 - e2 & 1 has more cols & equal rows\n  } else if(ncol(e1) >= ncol(e2) & nrow(e1) == nrow(e2)) {\n    case <- \"3\"\n    \n    # e1 - e2 & 2 has more cols & equal rows\n  } else if(ncol(e1) < ncol(e2) & nrow(e1) == nrow(e2)) {\n    case <- \"4\"\n    \n    # Fail\n  } else {\n    stop(\"Incorrect dims\")\n  }\n  \n  switch(case,\n         \"1\" = t(apply(e1, 1, function(x) {x + e2})),\n         \"2\" = t(apply(e2, 1, function(x) {x + e1})),\n         \"3\" = t(apply(e1, 2, function(x) {x + e2})),\n         \"4\" = t(apply(e2, 2, function(x) {x + e1})))\n}\n\nLet’s see what this thing can do!\n\nx1 %+% x2\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    4    6\n\n\nNice! That’s what I want. One thing to note is that order of operations don’t work quite as you’d expect because of the precedence of the special %+% operator in relation to + and *, so you have to be really explicit.\n\n# This tries to do addition first\nx1 * 2 %+% x2\n\nError in 2 %+% x2: is.matrix(e1) is not TRUE\n\n# Explicit parenthesis\n(x1 * 2) %+% x2\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    5    8\n\n\nArmed with the ability to broadcast addition, let’s redo the last step of the stock price simulation.\n\n# Instead of a massive matrix, just create a 1x6\ntime_steps <- matrix(seq(from = dt, to = time_T, by = dt), nrow = 1, ncol = t_total)\ntime_steps\n\n           [,1]      [,2] [,3]      [,4]      [,5] [,6]\n[1,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n\n# Remember that W is 10000x6\nhead(W)\n\n   \nX1            1          2          3           4          5          6\n  1 -0.16179538  0.5225740  0.2811560  0.22526661  0.3645603  0.4394974\n  2 -0.06644652 -0.1146010 -0.1782749 -0.10375422  0.1044858  0.3693483\n  3  0.44996033  0.7175510  0.1103186 -0.04507883 -0.1916658 -0.4001812\n  4  0.02035402 -0.1436573 -0.6251118 -0.96547795 -0.9841578 -1.2174893\n  5  0.03732215  0.1023001 -0.2146545  0.04534000  0.4212300  0.3804252\n  6  0.49509662  0.8218728  0.3410494  0.33635853  0.2732435  0.9247872\n\n# Add using broadcasted addition, making sure to be careful about parenthesis!\ns_t <- s_0 * exp(((r - 1/2 * sigma^2) * time_steps) %+% (sigma * W))\n\ns_t <- cbind(s_0, s_t, deparse.level = 0)\nhead(s_t)\n\n  [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n1  100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n2  100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n3  100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n4  100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n5  100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n6  100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\n\nSo much better! I have used this a few times in the past month or so. Credit to Alex Hayes for teaching me a bit about why broadcasting is awesome. I created the base for %+% in response to his comments here."
  }
]