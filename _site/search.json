[
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html",
    "href": "posts/2019-03-02-now-you-c-me/index.html",
    "title": "Now You C Me",
    "section": "",
    "text": "This post is designed to help you get up and running with an R package that uses C code, teaching the absolute minimum required to get going. Over the past few days, I learned a lot about working with packages that call C. It’s not quite as pleasant as working with C++ and Rcpp, but as you start to figure it out, it really feels like a superpower. Superpower or not, as Jim Hester says, if you have the choice it is much easier to use Rcpp, and my advice would also be to start there. I created this guide because I was contributing to a project that used C, and was frustrated by the lack of resources for beginners.\nThat said, it’s a steep learning curve. Yes, Writing R Extensions has much of what you need to know, but I’m not big on the whole RTFM idea, and I think posts that actually get you up and running so you can start exploring on your own are valuable."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#package",
    "href": "posts/2019-03-02-now-you-c-me/index.html#package",
    "title": "Now You C Me",
    "section": "Package",
    "text": "Package\nSo let’s go. I’m going to create a C function that takes one numeric argument of length 1, and adds 1 to it. Then, I’ll show how to call it from R so users of the package can actually interface with it. Exciting.\nWe need the structure for a package, so let’s use some usethis helpers to get started. We’ll call the package addr, and you should replace path with the path to the location you want the package created at.\n\nusethis::create_package(path = \"~/path/to/addr\")\n\nIf you work with RStudio, this should open a new RStudio instance with the addr package opened up. Switch over to that."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#optional-git",
    "href": "posts/2019-03-02-now-you-c-me/index.html#optional-git",
    "title": "Now You C Me",
    "section": "Optional git",
    "text": "Optional git\nIf you are so inclined and have everything hooked up (you’ve used git before on your computer), you can set up git and GitHub for this repo. I’ll do it so you can see the final product, along with all of the commits along the way. You can find my end result here.\n\n# This will restart RStudio\nusethis::use_git()\n\n# Then call this to use github\n# for this package\nusethis::use_github()"
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#roxygen2",
    "href": "posts/2019-03-02-now-you-c-me/index.html#roxygen2",
    "title": "Now You C Me",
    "section": "roxygen2",
    "text": "roxygen2\nSome of the usethis functions we are going to call require roxygen2 to be used for creating documentation (and it makes our lives a heck of a lot easier). Let’s set that up next. The easiest way is to just call:\n\ndevtools::document()\n\nAlternatively, on a Mac I can call CMD+Shift+D. This should just add some information about roxygen2 to the DESCRIPTION file.\nThe other thing we will want to do is create a “package doc” .R file. Other usethis functions called later will use this file to store useful information automatically. Basically, this is just a .R file named addr-package.R.\n\nusethis::use_package_doc()\n\nThe contents of this look like:\n\n#' @keywords internal\n\"_PACKAGE\"\n\n# The following block is used by usethis to automatically manage\n# roxygen namespace tags. Modify with care!\n## usethis namespace: start\n## usethis namespace: end\nNULL\n\nThe usethis information is going to be inserted between the usethis namespace: start and usethis namespace: end lines as we go along."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#can-you-c",
    "href": "posts/2019-03-02-now-you-c-me/index.html#can-you-c",
    "title": "Now You C Me",
    "section": "Can you C?",
    "text": "Can you C?\nLet’s work on our first C function. Assuming you are in RStudio and in the current addr project, let’s create our first C file. There’s a usethis helper for that too.\n\nusethis::use_c(\"add\")\n\nWow. That did a lot more than just create a C file. I get something like this:\n\n#> ✔ Creating 'src/'\n#> ✔ Adding '*.o', '*.so', '*.dll' to 'src/.gitignore'\n#> ✔ Adding '@useDynLib addr, .registration = TRUE' to 'R/addr-package.R'\n#> ● Run `devtools::document()` to update 'NAMESPACE'\n#> ✔ Writing 'src/add.c'\n#> ● Modify 'src/add.c'\n\nSo what happened here?\n\nA new directory was created, src/. This is where all of your C files go.\nMultiple types of files were added to a .gitignore in src/. This is very helpful, as these 3 types of files, .o, .so and .dll are ones that would be created by our package when the C code is compiled, or “built” if you are familiar with R packages. In the same way that we wouldn’t commit a built R package to github, we don’t commit these files either.\nSome information was added to addr-package.R, and we get a suggestion to document().\nA new file, src/add.c was added, and then it opened in RStudio. Great! We will work on this file in a moment.\n\nWhat’s this information that got added to addr-package.R? If we open up addr-package.R right now, we will see:\n\n## usethis namespace: start\n#' @useDynLib addr, .registration = TRUE\n## usethis namespace: end\nNULL\n\nThe second line here is a roxygen comment added by use_c(), and it is an important one! Basically, it is the way we eventually tell R that we should be looking for any C routines (functions) that we have “registered” (i.e. exposed to the R side), so we can actually call them from R.\nReally, this information needs to be in the package NAMESPACE file, which contains the information on what functions are imported to and exported from your package, along with information about external code like this. That’s why a suggestion to document popped up, as roxygen2 will take care of that for you:\n\ndevtools::document()\n\nIf you got an error while documenting that looked like getDLLRegisteredRoutines.DLLInfo(), don’t be alarmed. We just don’t have any C code for it to load.\nCheck out the NAMESPACE file and you should see:\n\nuseDynLib(addr, .registration = TRUE)"
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#addr_add_one",
    "href": "posts/2019-03-02-now-you-c-me/index.html#addr_add_one",
    "title": "Now You C Me",
    "section": "addr_add_one()",
    "text": "addr_add_one()\nNow we are ready to work on our C function. Open up that add.c file if it isn’t open already. You should see this at the top:\n\n#define R_NO_REMAP\n#include <R.h>\n#include <Rinternals.h>\n\nThe second two #include lines give us access to the C-level R API. If you aren’t familiar with includes or header files, for now think of them as calling library() on a package to get access to its functions for your own use.\nThe first line, #define R_NO_REMAP, purposefully comes before the other two, and prevents a “re-mapping” of the API functions from a standardized name of Rf_<fn> to just <fn>. I think it’s pretty good practice to prevent this remapping, as it makes it clear to us what functions are from the R API (and it works really well for finding new functions with auto-complete!).\nOKAY, now let’s write some C code. Go ahead and add these lines to the file:\n\n#define R_NO_REMAP\n#include <R.h>\n#include <Rinternals.h>\n\nSEXP addr_add_one(SEXP a) {\n\n  SEXP out = PROTECT(Rf_allocVector(REALSXP, 1));\n\n  REAL(out)[0] = Rf_asReal(a) + 1;\n\n  UNPROTECT(1);\n\n  return out;\n}\n\n…okay. This looks foreign. So many questions, I know, I had them too. I’ll tackle as many as I can think of. First, a broad overview of what this code does:\n\nTake in a single argument, a.\nCreate an object that we will assign the result to, out. This is a numeric vector of length 1. We also protect that object from the garbage collector.\nAdd 1 to a and assign it to out.\nUnprotect our result, out, as we are about to return it.\nReturn the result.\n\nWhat is a SEXP?\nGood question. It’s called an S-Expression, don’t ask me how to pronounce the shorthand notation, and I think it originated in the functional language, Lisp.\nJust think of a SEXP as a container that is able to represent any kind of R object, but at the C level. This means that an R list can be a SEXP, so can a matrix, so can a single logical value.\nWhy are there so many explicit types?\nMeaning, why is the argument SEXP a and not just a? Why is the return value, out, created as SEXP out = not just out =?\nC is a statically typed language, unlike R. Whereas in R we have the flexibility to create variables of any type without specifying the type in advance, in C we have to specify the type of every single variable ahead of time. This comes with the benefit of speed, and is one of the reasons C is so much faster than R.\nAllocation and protection\nIn C, creating new R objects is a bit more complicated than what you’d do at the R level. We know that we are going to take in a numeric vector of length 1, a, and we want to add 1 to it and return that result, which is also a numeric vector of length 1. So we need a place to put that result.\nTo do so, we have to create a new numeric vector of length 1 at the C level. The easiest way to do this is using a function from the R API, Rf_allocVector(). It takes in two arguments, a SEXPTYPE (the type of the SEXP to make) and a R_xlen_t (the length of the SEXP) and returns a SEXP of the type and length requested.\nThere are 7 SEXPTYPEs you can use, and you can find information about them at Hadley’s R Internals documentation repo. We used the type for a “real” (numeric) vector, REALSXP. We also specified the length to be 1.\nGreat, so we used Rf_allocVector(REALSXP, 1) to create a numeric vector of length 1. All good? Not quite. We also have to protect that numeric vector. From what? Well, if we don’t protect it by wrapping it in PROTECT() immediately, then when R’s garbage collector runs (which I think of as happening randomly), then that object gets cleaned up and removed 😢. If not managed correctly, this can cause great heartache.\nAs if it wasn’t difficult enough, we also have to manage unprotecting all of these objects. We do that here by calling UNPROTECT(1) at the end of the function, right before we return our result. UNPROTECT() takes an integer value of the number of things to unprotect, so if you had created 3 new objects, you could call UNPROTECT(3) instead. We don’t have to protect and unprotect the arguments to the function, as R knows to protect these automatically.\nAutocomplete ❤️\nYou might be wondering, “How did he know about Rf_allocVector()?” Or, “How do I find more of these neat C level API functions?” Great question! Luckily, RStudio’s autocomplete has your back. By typing Rf_ and pressing tab, we get the following pop up that we can scroll through.\n\nUsing this, we can find new functions to research google and learn about.\nSimilarly, how did I know what type Rf_allocVector() took and returned? Well…\n\nThe popup we get from RStudio tells us not only the name of the C function, but also the types of the arguments and return value!\nIn a moment, we will use Rf_asReal().\n\nAddition\nOkay, so about the actual addition line…\n\nREAL(out)[0] = Rf_asReal(a) + 1;\n\nThat seems like a lot of work to add 1.\nRf_asReal() takes a SEXP, and returns a single double value corresponding to the first element in the SEXP. A double is the C type that is somewhat equivalent to an R numeric. So this converts our numeric vector of length 1, a, into an object that can be manipulated at the C level with C operations like addition.\nNext, we add 1 to that double that Rf_asReal() gives us. After that, we have to put it somewhere. If you thought we could just do out = Rf_asReal(a) + 1, well, haven’t you learned by now that it isn’t ever that simple?\nout is a SEXP, and we can’t assign a double straight to a SEXP. What we actually need is a way to access the double underlying the out SEXP we created. That is what REAL(out) gets us. Technically, it gives us a double* (a double pointer) that points to the actual double array that out abstracts away into an R numeric vector. Jargon aside, it gives us something that we can kind of treat like an R vector, where we can index into it with [] and assign values to those slots.\nOne more note is that C is 0-index based, while R is 1-index based, so rather than doing REAL(out)[1] to access the first position, we really do REAL(out)[0].\nAlright, so REAL(out) gave us access to the double*, and REAL(out)[0] gave us access to the actual double at the 1st position in the vector. This is a double, so we can assign our result to this.\nReturn\nFinally, after calling UNPROTECT(1) as described above, we return out with return out. Unlike R, you actually have to specify the return in C (it’s generally optional in R).\nNote that in the function signature, we specified SEXP addr_add_one(...). The SEXP at the beginning there was our way of telling C that we are going to be returning a SEXP object."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#registration",
    "href": "posts/2019-03-02-now-you-c-me/index.html#registration",
    "title": "Now You C Me",
    "section": "Registration",
    "text": "Registration\nUpdate) I have since learned that this registration section can be generated automatically! I highly recommend still reading this post in order to understand what the registration piece does, but check out the Automatic Registration section at the end of the post to learn how to do it automatically. Thanks, Hadley & Jim and Metin.\nGreat, so we have a C function that can add 1. Can we call this from R yet? Almost, but still no. If you run a devtools::load_all(), then you should see the code compile. For me that looks like this, but I’ve manually trimmed it so you aren’t overloaded with output:\n\n#> Loading addr\n#> Re-compiling addr\n#> ─  installing *source* package ‘addr’ ...\n#>    ** libs\n#>    clang <trimmed>  -c add.c -o add.o\n#>    clang -dynamiclib <trimmed> -o addr.so add.o\n#>    installing to /private/var/<trimmed>/addr/libs\n#> ─  DONE (addr)\n\nThis is telling us that the:\n\n\nadd.c was compiled into an “object” file, add.o.\nThat one object file was used to make an addr.so “shared object”. You can maybe think of this kind of like a “built” R package.\n\nNevertheless, if you try and do addr:: after running load_all(), you will be disappointed, and you won’t see addr_add_one anywhere! This is because we still have to register the routine, aka expose these functions to the R side. This is related to that @useDynLib roxygen2 tag we added at the beginning. That tells R to look for the C functions, but we still have to actually expose them too. Let’s do that.\nCreate a new file, init.c. This is the standard name for the file where this “routine registration” happens:\n\nusethis::use_c(\"init\")\n\nAdd the following to what is already there:\n\n#define R_NO_REMAP\n#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\n/* .Call calls */\nextern SEXP addr_add_one(SEXP);\n\nstatic const R_CallMethodDef CallEntries[] = {\n  {\"addr_add_one\", (DL_FUNC) &addr_add_one, 1},\n  {NULL, NULL, 0}\n};\n\nvoid R_init_addr(DllInfo *dll) {\n  R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n  R_useDynamicSymbols(dll, FALSE);\n}\n\nThis is the how C level functions are registered as something that can be called from the R side. Ugly, right? I won’t go into much detail here (I don’t know everything about it myself anyways), and you’ll generally only ever change 3 things. First, take a look at the section starting with “.Call calls”.\n\n/* .Call calls */\nextern SEXP addr_add_one(SEXP);\n\nHere, we have to list all of our C level functions that we want to expose to the R side, along with their full function signature (the argument types and return type), and we have to prefix it with extern. Our function takes 1 SEXP, a, as an argument, so we only have 1 SEXP specified here. If your function has 2 arguments, you would do extern SEXP my_fun(SEXP, SEXP). Generally the only things you will pass back and forth between R and C will be SEXP objects. If you have another function to export, you just add another extern call below this one.\nNext, we have to construct an “array of call method definitions”, named CallEntries.\n\nstatic const R_CallMethodDef CallEntries[] = {\n  {\"addr_add_one\", (DL_FUNC) &addr_add_one, 1},\n  {NULL, NULL, 0}\n};\n\nIn this, you’ll specify the address of the function you specified above (that’s what &addr_add_one does), and from what I understand you convert it into a dynamically loadable function. Basically, you’ll add one line per function you are exporting, and it is of the form:\n\n{\"<function-name>\", (DL_FUNC) &<function-name>, num_args}\n\nIf you have a second function to export, add another line after the first one, but before the {NULL, NULL, 0}.\nLastly, there is a function that R will automatically call for you that actually does the registration of these functions. R looks for a C function called R_init_<pkg>() to call to register these routines. So the only thing you’d ever change here is to change <pkg> to your current package name.\n\nvoid R_init_addr(DllInfo *dll) {\n  R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n  R_useDynamicSymbols(dll, FALSE);\n}\n\nNote how we pass the CallEntries into R_registerRoutines(). This seems to be passing along all of the information required about how to create the entry points into the C code that we will call from the R side."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#call-it",
    "href": "posts/2019-03-02-now-you-c-me/index.html#call-it",
    "title": "Now You C Me",
    "section": ".Call() it!",
    "text": ".Call() it!\nTry running devtools::load_all() one more time. You should now have access to addr::addr_add_one. It’s not a function, so don’t try and call it with addr_add_one(). Let’s print it out.\n\naddr::addr_add_one\n#> $name\n#> [1] \"addr_add_one\"\n#> \n#> $address\n#> <pointer: 0x7fea6b4da160>\n#> attr(,\"class\")\n#> [1] \"RegisteredNativeSymbol\"\n#> \n#> $dll\n#> DLL name: addr\n#> Filename: /Users/davis/Desktop/r/projects/data-insights-package/addr/src/addr.so\n#> Dynamic lookup: FALSE\n#> \n#> $numParameters\n#> [1] 1\n#> \n#> attr(,\"class\")\n#> [1] \"CallRoutine\"      \"NativeSymbolInfo\"\n\nSo this is really just a list of class \"CallRoutine\" holding information about where to find the actual C function we need.\nSince you can’t call this like a function, how do you use it? The magic is with the function .Call(), which serves as the function that let’s us call this addr_add_one entry point along with any arguments that we might need to pass through. Try the following:\n\n.Call(addr_add_one, 2)\n#> [1] 3\n\nWoah! So that just called our C function to add 1 to 2, so we get 3."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#now-what",
    "href": "posts/2019-03-02-now-you-c-me/index.html#now-what",
    "title": "Now You C Me",
    "section": "Now what?",
    "text": "Now what?\nWell, we need a better way to expose this to our users. The best way is to create a function that wraps this that we can export and document. Also, we did no error checking at the C level, so if we pass any bad or unexpected inputs in to this .Call(), it could actually crash R completely. Not just error, crash. This isn’t the worst thing in the world, and it’s a pretty normal part of the development process of connecting R and C (I do it all the time), but it isn’t fun for the user. To fix that, we will also add some error checking to our function.\nCreate a file named add.R.\n\nusethis::use_r(\"add\")\n\nNow add the following:\n\n#' Add 1 to a single numeric\n#'\n#' `add_one()` adds 1 to a single numeric value.\n#'\n#' @param a A single numeric value.\n#'\n#' @examples\n#'\n#' add_one(2)\n#'\n#' @export\nadd_one <- function(a) {\n\n  ok <- is.numeric(a) & length(a) == 1L\n\n  if (!ok) {\n    stop(\"`a` must be a single numeric value.\", call. = FALSE)\n  }\n\n  .Call(addr_add_one, a)\n}\n\nAt this point, call load_all() again and you should have access to add_one().\n\nadd_one(2)\n#> [1] 3\n\n\nadd_one(c(1, 2))\n#> Error: `a` must be a single numeric value.\n\nAwesome! Since we have added the @export tag, to actually export this function we just need to call:\n\ndevtools::document()\n\nwhich will create a .Rd help page for the file, and will add export(add_one) to the NAMESPACE file."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#cran-check",
    "href": "posts/2019-03-02-now-you-c-me/index.html#cran-check",
    "title": "Now You C Me",
    "section": "CRAN Check!",
    "text": "CRAN Check!\nThe only thing left is to see if it passes a cran check! Plot twist, it won’t quite yet. We need to add a license first.\n\nusethis::use_mit_license(name = \"Davis Vaughan\")\n\nOkay, now try it:\n\ndevtools::check()\n\n#> 0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\n😎"
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#resources",
    "href": "posts/2019-03-02-now-you-c-me/index.html#resources",
    "title": "Now You C Me",
    "section": "Resources",
    "text": "Resources\nIf you want to learn more about R’s C interface, there are a few resources out there for you!\n\nHadley’s R Internals documentation\nAdvanced R’s old section on C\nThe massive but thorough Writing R Extensions. I would focus on section 5.9 on Handling R objects in C.\nThe full addr package is on GitHub. The commit history attempts to follow this post."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#automatic-registration",
    "href": "posts/2019-03-02-now-you-c-me/index.html#automatic-registration",
    "title": "Now You C Me",
    "section": "Automatic Registration",
    "text": "Automatic Registration\nAs mentioned in the Registration section, you don’t actually have to create the init.c file “by hand,” which is great because it’s the thing I forget to do most. I’ve left this section until the end, rather than replacing the current Registration section, because I think the order of how you link things up makes more sense when done the manual way (expose to R with the init.c file, then .Call() it). So if you saw that note in Registration and instantly skipped to here, I’d advice going back and reading the rest of that section and the rest of the blog post first. As you get more familiar with working in C, you can use the methods described here.\nTo work with automatic registration, follow the blog post like usual, but skip the Registration and .Call it! sections and go straight to the Now What? section. Generate the add.R file that looks like this:\n\n#' Add 1 to a single numeric\n#'\n#' `add_one()` adds 1 to a single numeric value.\n#'\n#' @param a A single numeric value.\n#'\n#' @examples\n#'\n#' add_one(2)\n#'\n#' @export\nadd_one <- function(a) {\n\n  ok <- is.numeric(a) & length(a) == 1L\n\n  if (!ok) {\n    stop(\"`a` must be a single numeric value.\", call. = FALSE)\n  }\n\n  .Call(addr_add_one, a)\n}\n\nAt this point you should have an add.R file, but no init.c file. Try running:\n\ndevtools::load_all()\n\nYou should have access to add_one(), but if you call it, you get…\n\nadd_one(1)\n#> Error in add_one(1) : object 'addr_add_one' not found\n\nThis makes sense, because we have no init.c file, so the C function was not exposed to the R side.\nAt this point, we can generate the init.c file automatically using pkgbuild::compile_dll(). The key is to run it with register_routines = TRUE, which will take care of automatically setting up init.c. You may also have to run it with force = TRUE. If you have previously compiled all of the C code already and nothing has changed, it won’t try and do it again (this is generally a good thing!), and the function will exit early. But we want to trigger the recompilation to make it generate the init.c file for us, so we should force it.\n\npkgbuild::compile_dll(force = TRUE, register_routines = TRUE)\n#> Re-compiling addr\n#> ─  installing *source* package ‘addr’ ...\n#>    ** libs\n#>    clang <trimmed>  -c init.c -o init.o\n#>    clang <trimmed> -o addr.so add.o init.o\n#>    installing to /private/var/<trimmed>/addr/libs\n#> ─  DONE (addr)\n\nThis output looks similar to what was generated with devtools::load_all(), and that’s because this function is called from it. But this time, you can see that it compiled an init.c file as well, one that it created for us! Let’s take a look at init.c:\n\n#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\n\n/* Section generated by pkgbuild, do not edit */\n/* .Call calls */\nextern SEXP addr_add_one(SEXP);\n\nstatic const R_CallMethodDef CallEntries[] = {\n    {\"addr_add_one\", (DL_FUNC) &addr_add_one, 1},\n    {NULL, NULL, 0}\n};\n/* End section generated by pkgbuild */\n\nvoid R_init_addr(DllInfo *dll)\n{\n    R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n    R_useDynamicSymbols(dll, FALSE);\n}\n\nAwesome, so this entire file was created automatically, and looks essentially the same as the one that we made manually. Now that we have all the pieces, call devtools::load_all() one more time, which will sync everything up. Then you should be able to do:\n\nadd_one(1)\n#> [1] 2\n\nYou’ll also want to call devtools::document() again as well to add export(add_one) to the NAMESPACE file if you haven’t already.\nThe reason we have to create add_one() first, before calling pkgbuild::compile_dll(), is because of the way the information is found to generate the init.c file. It looks into your .R files, and scans for any calls to .Call(). The information there, along with the name of your package, is enough to completely generate the init.c file! Internally, compile_dll() calls tools::package_native_routine_registration_skeleton() (yes, this is a mouthful), which is what generates the skeleton for init.c, using the information it located. compile_dll() performs a few extra steps on top of that to clean up.\nIf you add another C based function to your package, just call compile_dll(register_routines = TRUE) again, and it will update the information in the init.c file."
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "",
    "text": "School has started up, and I’m in a class called Financial Computing. I thought it might be interesting to share some of my assignments and explain what I learn along the way. Most of the posts won’t be describing the Stochastic Calculus involved in each assignment, but will instead focus on the details of the implementation in R. I don’t claim to have the best way for any of these assignments, but perhaps you can learn something!"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#what-are-we-doing-today",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#what-are-we-doing-today",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "What are we doing today?",
    "text": "What are we doing today?\nIn this post, I’ll work through simulating paths of a stock that follows the log normal distribution used in the Black Scholes model. Importantly, I’ll explain my thought process as I tried to optimize the implementation from loops to vectors.\nAs an added bonus, at the very bottom is some extra content on a basic function that I have created to replicate the concept of broadcasting from Python. Someone could (should?) probably create an entire package out of this idea."
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#the-model",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#the-model",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "The Model",
    "text": "The Model\nUnder the Black Scholes model, the stock price, \\(S_t\\), at time t follows a Geometric Brownian Motion, which means that it satisfies the Stochastic Differential Equation:\n\\[ dS_t = r S_t dt + \\sigma S_t dW_t \\]\nWhere:\n\n\n\\(r =\\) Drift - The average log return per time unit\n\n\\(\\sigma =\\) Volatility - How much variance is in that drift\n\n\\(W_t =\\) Brownian Motion - Random noise from a normal distribution with mean 0 and variance t\n\n\nInterestingly, we actually have the solution to this equation (one of the few we have analytical solutions for):\n\\[ S_t = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t + \\sigma W_t} \\]\nMore generally, this can be written as a formula providing us with the recursive equation:\n\\[ S_{t_i} = S_{t_{i-1}} \\times e^{(r - \\frac{1}{2} \\sigma^2) ({t_i - t_{i-1}}) + \\sigma (W_{t_i} - W_{t_{i-1}}) } \\]\nIf you want to know how to get the solution, this is a pretty good explanation, but be prepared to learn about Ito’s Lemma."
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#really-dude",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#really-dude",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "Really, dude?",
    "text": "Really, dude?\nOkay, that’s a lot without any explanation, and I get that. But the point of this post is more to explain how to simulate paths of \\(S_t\\). So how do we do that?\n\nWe will start from time 0 with an initial stock price, then we will generate the next stock price from that using the recursive formula, and so on.\nThe only random piece is the brownian motion increment (dW), which we will generate at each time point using draws from a normal distribution.\nThe time increment will be a constant value (dt) to keep things simple.\n\nI was given some starting parameters:\n\n# Set a seed too so I can reproduce the exact numbers\nset.seed(123)\n\n# Parameters\nr       <- 0.028\nsigma   <- 0.255\ntime_T  <- 0.5\ndt      <- 1/12\nt_total <- time_T / dt\ns_0     <- 100\nN       <- 10000\n\nWhere time_T and dt mean each simulation path will go from time 0 to time_T by increments of dt. Dividing time_T / dt gets us the total number of time steps required. N is the number of paths to simulate."
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#first-attempt",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#first-attempt",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "First attempt",
    "text": "First attempt\nWe all know loops are to be avoided when you can in R, and that you should instead vectorize the operations. At first, I thought this wasn’t going to be possible, as this is a recursive type of formula where the next value relies on the previous one. With that in mind, I created the following implementation.\nFirst off, set up a matrix to fill with the 10000 simulations (one per row), each one having 6 time steps (7 columns total including the initial stock price).\n\n# Create a 10000x7 matrix of NA's to fill in\n# Each row is a simulation\ns <- matrix(NA_real_, nrow = N, ncol = t_total+1)\n\n# The first column is just the initial price\ns[,1] <- s_0\n\nhead(s)\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#> [1,]  100   NA   NA   NA   NA   NA   NA\n#> [2,]  100   NA   NA   NA   NA   NA   NA\n#> [3,]  100   NA   NA   NA   NA   NA   NA\n#> [4,]  100   NA   NA   NA   NA   NA   NA\n#> [5,]  100   NA   NA   NA   NA   NA   NA\n#> [6,]  100   NA   NA   NA   NA   NA   NA\n\nSo far so good, now let’s create a matrix for dW, our brownian motion increments. A very important fact is that these are all independent of each other, so the generation of them is straightforward. Each increment:\n\\[ W_{t_i} - W_{t_{i-1}} \\]\ncan be drawn from a normal distribution with mean 0 and variance \\(t_i - t_{i-1}\\) (which is what I have defined as dt because it is a constant).\n\n# ~N(0, dt)\n# To fill in 10000 simulations, and move forward 6 steps, we need a 10000x6 matrix\ndW <- rnorm(N * t_total, mean = 0, sd = sqrt(dt))\ndW <- matrix(dW, N, t_total)\nhead(dW)\n#>             [,1]        [,2]        [,3]        [,4]        [,5]        [,6]\n#> [1,] -0.16179538  0.68436942 -0.24141807 -0.05588936  0.13929365  0.07493710\n#> [2,] -0.06644652 -0.04815446 -0.06367394  0.07452070  0.20824004  0.26486253\n#> [3,]  0.44996033  0.26759071 -0.60723241 -0.15539745 -0.14658701 -0.20851535\n#> [4,]  0.02035402 -0.16401128 -0.48145457 -0.34036612 -0.01867981 -0.23333150\n#> [5,]  0.03732215  0.06497791 -0.31695458  0.25999452  0.37589000 -0.04080481\n#> [6,]  0.49509662  0.32677618 -0.48082343 -0.00469084 -0.06311503  0.65154366\n\nBased on this setup, I thought I would need a loop. The algorithm would step through the 10000 simulations all at once, but would have to loop through the 6 time steps one at a time, because each time step depended on the previous one. So, following the formula (below again for reference) I did this:\n\\[ S_{t_i} = S_{t_{i-1}} \\times e^{(r - \\frac{1}{2} \\sigma^2) ({t_i - t_{i-1}}) + \\sigma (W_{t_i} - W_{t_{i-1}}) } \\]\n\nfor(i in 1:(t_total)) {\n  s[,i+1] <- s[,i] * exp((r - 1/2 * sigma^2) * dt + sigma * dW[,i])\n}\n\nhead(s)\n#>      [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n#> [1,]  100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n#> [2,]  100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n#> [3,]  100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n#> [4,]  100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n#> [5,]  100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n#> [6,]  100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\nAnd that does work! But can we avoid the loop? YES WE CAN!"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#math",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#math",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "MATH",
    "text": "MATH\nTo avoid the loop, we are going to manipulate a simple case, and then apply it generally. One key element for this vectorization is that brownian motion increments are independent. Let’s think about what actually happens from time 0->1 and from 1->2.\n\\[ S_1 = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0) + \\sigma (W_1 - W_0)} \\] \\[ S_2 = S_1 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_1) + \\sigma (W_2 - W_1)} \\]\nIf we plug the equation for S_1 into the equation for S_2…\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0) + \\sigma (W_1 - W_0)}) \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_1) + \\sigma (W_2 - W_1)} \\]\nAnd then combine exponents…\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0 + t_2 - t_1) + \\sigma (W_1 - W_0 + W_2 - W_1)}) \\]\nNotice that some of the t and W terms cancel:\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_0) + \\sigma (W_2 - W_0)}) \\]\nAnd by definition t_0 and W_0 are 0:\n\\[ S_2 = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t_2 + \\sigma W_2} \\]\nThis is actually the form that was proposed as the solution to the geometric brownian motion stochastic differential equation:\n\\[ S_t = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t + \\sigma W_t} \\]\nIt looks like we can actually generate S_2 without needing to know S_1 at all. Notice that the exponent now contains t_2 and W_2. t_2 is known beforehand, but W_2 seems like it would rely on W_1 in a way that has to be recursively calculated. Actually, if we think of W_2 as a sum of brownian motion increments (I told you this would help):\n\\[ W_2 = (W_2 - W_1) + (W_1 - W_0) = dW_2 + dW_1 \\]\nthen W_2 is just the cumulative sum of the increments, and, by definition, each increment is independent of the previous increment so we can generate them all before hand (we already did this when we created the dW matrix).\n\n# Rowwise cumulative sum of dW generates W1, W2, W3, ... for each simulation\nW  <- plyr::aaply(dW, 1, cumsum)\nhead(W)\n#>    \n#> X1            1          2          3           4          5          6\n#>   1 -0.16179538  0.5225740  0.2811560  0.22526661  0.3645603  0.4394974\n#>   2 -0.06644652 -0.1146010 -0.1782749 -0.10375422  0.1044858  0.3693483\n#>   3  0.44996033  0.7175510  0.1103186 -0.04507883 -0.1916658 -0.4001812\n#>   4  0.02035402 -0.1436573 -0.6251118 -0.96547795 -0.9841578 -1.2174893\n#>   5  0.03732215  0.1023001 -0.2146545  0.04534000  0.4212300  0.3804252\n#>   6  0.49509662  0.8218728  0.3410494  0.33635853  0.2732435  0.9247872\n\nUnlike the recursive formula from before where dt was used, the time that we are currently at, t, is used instead so we will need that as well.\n\ntime_steps <- matrix(seq(from = dt, to = time_T, by = dt), \n                     nrow = N, ncol = t_total, byrow = TRUE)\nhead(time_steps)\n#>            [,1]      [,2] [,3]      [,4]      [,5] [,6]\n#> [1,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [2,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [3,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [4,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [5,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [6,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n\nNow it’s a vectorized one-liner to calculate the stock price at each time!\n\n# Stock price simulation\ns_t <- s_0 * exp((r - 1/2 * sigma^2) * time_steps + sigma * W)\n\n# Add the original stock price onto the front\ns_t <- cbind(s_0, s_t)\n\n# Add 0 as the column name for initial value (it's important I promise)\ncolnames(s_t)[1] <- \"0\"\n\nhead(s_t)\n#>     0         1         2         3         4         5         6\n#> 1 100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n#> 2 100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n#> 3 100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n#> 4 100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n#> 5 100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n#> 6 100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\nJust as a sanity check, this should have produced the same results as the for loop\n\n# ignore the dimname attributes\nall.equal(s, s_t, check.attributes = FALSE)\n#> [1] TRUE"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#now-what",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#now-what",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "Now what?",
    "text": "Now what?\nThere are a number of interesting things we could do with these results. One is to calculate the fair price of a European Option on this stock. I think I’ll save that for the next post.\nSomething else we might do is visualize the distribution of \\(S_T\\), the stock price at the terminal (final) time. Because the stock price is modeled as an exponential of a normal random variable (W_t), the stock price itself has a log-normal distribution. For practicality, this means that it is right tailed and can’t drop below 0 (good properties of a stock).\n\nlibrary(tidyverse)\n\n# let's just take a moment and admire the fact that I can put LaTeX in ggplots\nlibrary(latex2exp) \n\ntibble::tibble(s_T = s_t[,6]) %>%\n  ggplot(mapping = aes(x=s_T)) +\n  geom_histogram(bins = 500) + \n  labs(x = TeX('S_T'), y = NULL, title = TeX('Log-normal Distribution of S_T') )\n\n\n\n\nWe could also look at the 6-step path of 100 of our simulations.\n\nas_tibble(s_t) %>%\n  rownames_to_column(var = \"iteration\") %>%\n  gather(time_step, stock_price, -iteration) %>%\n  mutate(time_step = as.numeric(time_step),\n         iteration = as.factor(iteration)) %>%\n  filter(iteration %in% 1:100) %>%\n  \n  ggplot(aes(x = time_step, y = stock_price, group = iteration)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Stock Price\", title = \"100 Simulated Paths\")"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#conclusion-extra-content",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#conclusion-extra-content",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "Conclusion + Extra Content",
    "text": "Conclusion + Extra Content\nIf I haven’t bored you to tears yet, allow me to thank you for sticking around this long. I think these posts are useful because they force me to try and understand a concept a bit more than if I was just reading it from a book.\nAs you may have noted in the post above, I had to create a large matrix time_steps for R to perform the matrix addition I wanted correctly. I thought it would have been simple. Ideally I could create a 1x6 matrix of times, and add it to a 10000x6 matrix of the brownian motions and have the times matrix broadcasted to each row of the brownian motion matrix, adding element by element to each row. This works in Python and Matlab, but R has a mind of it’s own.\n\n# First try with a vector\nx1 <- c(1,2)\nx1\n#> [1] 1 2\n\nx2 <- matrix(c(1,2,3,4), nrow = 2, ncol = 2, byrow = TRUE)\nx2\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n\n# I want to add c(1,2) to each row of x2\nx1 + x2\n#>      [,1] [,2]\n#> [1,]    2    3\n#> [2,]    5    6\n\nThat’s not right, it’s adding 1 to the first column of x2, then 2 to the second column of x2. To get what I want I could butcher it like this:\n\nt(x1+t(x2))\n#>      [,1] [,2]\n#> [1,]    2    4\n#> [2,]    4    6\n\nIf x1 was a matrix instead of a vector, then it gives a non-conformable array error.\n\nx1 <- matrix(x1, ncol = 2)\nx1\n#>      [,1] [,2]\n#> [1,]    1    2\n\nx1 + x2\n#> Error in x1 + x2: non-conformable arrays\n\nSo R is super strict here. That’s fine and all, but with other languages able to do this, and with it being such a natural way of thinking about this type of addition, I decided to roll my own function that allows me to add matrices together that meet certain conditions by broadcasting one of them over the other.\n\n# Broadcast addition\n# One of the special % % operators\n`%+%` <- function(e1, e2) {\n  \n  stopifnot(is.matrix(e1))\n  stopifnot(is.matrix(e2))\n  \n  # e1 - e2 & 1 has more rows & equal cols\n  if(nrow(e1) >= nrow(e2) & ncol(e1) == ncol(e2)) {\n    case <- \"1\"\n    \n    # e1 - e2 & 2 has more rows & equal cols\n  } else if(nrow(e1) < nrow(e2) & ncol(e1) == ncol(e2)) {\n    case <- \"2\"\n    \n    # e1 - e2 & 1 has more cols & equal rows\n  } else if(ncol(e1) >= ncol(e2) & nrow(e1) == nrow(e2)) {\n    case <- \"3\"\n    \n    # e1 - e2 & 2 has more cols & equal rows\n  } else if(ncol(e1) < ncol(e2) & nrow(e1) == nrow(e2)) {\n    case <- \"4\"\n    \n    # Fail\n  } else {\n    stop(\"Incorrect dims\")\n  }\n  \n  switch(case,\n         \"1\" = t(apply(e1, 1, function(x) {x + e2})),\n         \"2\" = t(apply(e2, 1, function(x) {x + e1})),\n         \"3\" = t(apply(e1, 2, function(x) {x + e2})),\n         \"4\" = t(apply(e2, 2, function(x) {x + e1})))\n}\n\nLet’s see what this thing can do!\n\nx1 %+% x2\n#>      [,1] [,2]\n#> [1,]    2    4\n#> [2,]    4    6\n\nNice! That’s what I want. One thing to note is that order of operations don’t work quite as you’d expect because of the precedence of the special %+% operator in relation to + and *, so you have to be really explicit.\n\n# This tries to do addition first\nx1 * 2 %+% x2\n#> Error in 2 %+% x2: is.matrix(e1) is not TRUE\n\n# Explicit parenthesis\n(x1 * 2) %+% x2\n#>      [,1] [,2]\n#> [1,]    3    6\n#> [2,]    5    8\n\nArmed with the ability to broadcast addition, let’s redo the last step of the stock price simulation.\n\n# Instead of a massive matrix, just create a 1x6\ntime_steps <- matrix(seq(from = dt, to = time_T, by = dt), nrow = 1, ncol = t_total)\ntime_steps\n#>            [,1]      [,2] [,3]      [,4]      [,5] [,6]\n#> [1,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n\n# Remember that W is 10000x6\nhead(W)\n#>    \n#> X1            1          2          3           4          5          6\n#>   1 -0.16179538  0.5225740  0.2811560  0.22526661  0.3645603  0.4394974\n#>   2 -0.06644652 -0.1146010 -0.1782749 -0.10375422  0.1044858  0.3693483\n#>   3  0.44996033  0.7175510  0.1103186 -0.04507883 -0.1916658 -0.4001812\n#>   4  0.02035402 -0.1436573 -0.6251118 -0.96547795 -0.9841578 -1.2174893\n#>   5  0.03732215  0.1023001 -0.2146545  0.04534000  0.4212300  0.3804252\n#>   6  0.49509662  0.8218728  0.3410494  0.33635853  0.2732435  0.9247872\n\n# Add using broadcasted addition, making sure to be careful about parenthesis!\ns_t <- s_0 * exp(((r - 1/2 * sigma^2) * time_steps) %+% (sigma * W))\n\ns_t <- cbind(s_0, s_t, deparse.level = 0)\nhead(s_t)\n#>   [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n#> 1  100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n#> 2  100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n#> 3  100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n#> 4  100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n#> 5  100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n#> 6  100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\nSo much better! I have used this a few times in the past month or so. Credit to Alex Hayes for teaching me a bit about why broadcasting is awesome. I created the base for %+% in response to his comments here."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html",
    "href": "posts/2017-05-10-aws-rds-r/index.html",
    "title": "Amazon RDS + R",
    "section": "",
    "text": "The code and advice in this post is still valid, but I have shut down the AWS database that this post pulled from, so it no longer runs. Additionally, the output is no longer shown."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#intro",
    "href": "posts/2017-05-10-aws-rds-r/index.html#intro",
    "title": "Amazon RDS + R",
    "section": "Intro",
    "text": "Intro\nWelcome to my first post! To start things off at Data Insights, I’m going to show you how to connect to an AWS RDS instance from R.\nFor those of you who don’t know, RDS is an easy way to create a database in the cloud. In this post, I won’t be showing you how to setup an RDS instance, but I will show you how to connect to it if you have one running.\nLet’s get started."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#step-1-the-one-where-you-got-connected",
    "href": "posts/2017-05-10-aws-rds-r/index.html#step-1-the-one-where-you-got-connected",
    "title": "Amazon RDS + R",
    "section": "Step 1: The one where you got connected",
    "text": "Step 1: The one where you got connected\nYou’ll need a few packages to get started.\n\n\nDBI and RMySQL are used to connect to the database, although RMySQL is usually called without explicitely loading it (that’s just the standard)\n\ntidyquant is just there to help us download some data to put in and get out of our database\n\ndplyr will be used to show off an alternate way to query from the database. Note that you should get the most recent github version of dplyr, along with the database specific pieces from dbplyr.\n\n\nlibrary(DBI)\n# library(RMySQL)\n\nlibrary(tidyquant)\n\n# devtools::install_github(\"tidyverse/dplyr\")\n# devtools::install_github(\"tidyverse/dbplyr\")\nlibrary(dplyr)\n# library(dbplyr)\n\nGetting connected isn’t too hard once you know what you’re looking for.\n\ncn <- dbConnect(drv      = RMySQL::MySQL(), \n                username = \"user1\", \n                password = \"testpassword\", \n                host     = \"davisdbinstance.crarljboc8to.us-west-2.rds.amazonaws.com\", \n                port     = 3306, \n                dbname   = \"firstdb\")\n\nLet’s go through the arguments to dbConnect(), the function from DBI that we used to connect.\n\ndrv - The driver I used is from the RMySQL package, an implementation of the general interface provided by DBI. I’ll leave it to the experts to explain all of this.\nusername / password - You’ll have to have created a user and password on AWS first, but then you can use them here.\nhost The host name is the Endpoint of your RDS server, without the port on the end. I’ve attached a screenshot to show where to find this. Basically, on the RDS Dashboard Instances page, hit the drop down arrow beside “MySQL” to show the Endpoint.\n\n\n\nport - The rest of the Endpoint shows the port that you’ll need to access your RDS instance through. That goes here.\ndbname - Finally, you’ll need the DB Name you used when setting up the instance. This can be found by clicking Instance Actions -> See Details, and then under Configuration Details you’ll find DB Name."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#step-2-the-one-where-you-take-it-for-a-test-spin",
    "href": "posts/2017-05-10-aws-rds-r/index.html#step-2-the-one-where-you-take-it-for-a-test-spin",
    "title": "Amazon RDS + R",
    "section": "Step 2: The one where you take it for a test spin",
    "text": "Step 2: The one where you take it for a test spin\nWell, alright…that was…cool? How do we know it’s working? Let’s get some data to load into the database. We will use some Apple stock data retrieved through tidyquant.\n\naapl <- tq_get(\"AAPL\")\n\nslice(aapl, 1:10)\n\n\n\n\nTo write the tibble (data frame) to the database, we will use another function called dbWriteTable(). It’s pretty straightforward. “name” is the name of the table you are creating, and “value” is the data frame you want to write.\n\ndbWriteTable(cn, name = \"apple\", value = aapl)\n\nNow the fun part! Let’s use a SQL query to pull it back down with dbGetQuery(). This function is a combination of dbSendQuery(), which returns a result set for your query, and dbFetch() which returns the rows from that result set.\n\napple_from_db <- dbGetQuery(cn, \"SELECT * FROM apple;\")\n\n# This effectively is the same as\n# dbReadTable(cn, \"apple\")\n\nslice(apple_from_db, 1:10)\n\nThere are a huge number of functions from DBI that you can use to communicate with databases. Maybe I will cover more in a separate post, but for now, let’s move on to dplyr."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#step-3-the-one-with-the-pliers",
    "href": "posts/2017-05-10-aws-rds-r/index.html#step-3-the-one-with-the-pliers",
    "title": "Amazon RDS + R",
    "section": "Step 3: The one with the pliers",
    "text": "Step 3: The one with the pliers\nBefore dplyr 0.6.0 was announced, you’d have to disconnect, and then reconnect through a dplyr specific function, src_mysql(). That would look something like the code below. Since then, however, you can now use the DBI connection with dplyr!\n\n# There is no need for this code anymore!\ndbDisconnect(cn)\n\ncn <- src_mysql(user     = \"user1\",\n                password = \"testpassword\",\n                host     = \"davisdbinstance.crarljboc8to.us-west-2.rds.amazonaws.com\",\n                port     = 3306,\n                dbname   = \"firstdb\")\n\nSelect the apple table from the database. This does not actually pull the data into memory. It just makes a connection!\n\n# With dplyr 0.6.0 we can just use the DBI connection!\napple_table <- tbl(cn, \"apple\")\n\n# By default the first 1000 rows are displayed\napple_table\n\nThe best part is that we can use almost any dplyr command with this! It queries the database, and does not do the manipulation in R. All of the familiar syntax of dplyr, but with databases. Let’s use filter() to get all of the rows after January 1, 2009.\n\nfilter(apple_table, date > \"2009-01-01\")\n\nTo do any serious manipulation outside of dplyr, you’ll likely have pull the data into memory to be able to use it with other R functions. Here, I’ll use the dplyr equivalent to dbWriteTable() to add the stock prices for IBM to the database.\n\nibm <- tq_get(\"IBM\")\n\ncopy_to(cn, df = ibm, temporary = FALSE)\n\nTo actually retrieve the data to memory, first make the connection using tbl() like before, and then use collect() to create the in memory tibble. Unfortunately, dates are stored as characters in the table, and collect() won’t try to fix that, so I’ll also take advantage of the readr package’s type_convert() function to do the thinking for me.\nOnce we have the data in memory, we can calculate the daily return with tidyquant and tq_mutate().\n\n# Connection\nibm_table <- tbl(cn, \"ibm\")\n\n# Collect to tibble\nreal_tibble <- collect(ibm_table) %>%\n  readr::type_convert()\n\n# Daily return\nreal_tibble <- real_tibble %>% \n  tq_mutate(select     = adjusted, \n            mutate_fun = periodReturn, \n            period     = \"daily\")\n\nreal_tibble\n\nAlways disconnect when you’re finished!\n\ndbDisconnect(cn)"
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#last-words",
    "href": "posts/2017-05-10-aws-rds-r/index.html#last-words",
    "title": "Amazon RDS + R",
    "section": "Last words",
    "text": "Last words\nHopefully I’ve been able to show you the power of DBI + dplyr with Amazon RDS. This integration has come a long way, and is just one of the huge advancements that the RStudio team has been working on in collaboration with other R users in the community.\nUntil next time!"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html",
    "href": "posts/2017-08-16-hadley-pleased/index.html",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "",
    "text": "Awhile back, I saw a conversation on twitter about how Hadley uses the word “pleased” very often when introducing a new blog post (I couldn’t seem to find this tweet anymore. Can anyone help?). Out of curiousity, and to flex my R web scraping muscles a bit, I’ve decided to analyze the 240+ blog posts that RStudio has put out since 2011. This post will do a few things:\n\nScrape the RStudio blog archive page to construct URL links to each blog post\nScrape the blog post text and metadata from each post\nUse a bit of tidytext for some exploratory analysis\nPerform a statistical test to compare Hadley’s use of “pleased” to the other blog post authors\n\nSpoiler alert: Hadley uses “pleased” ALOT."
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#required-packages",
    "href": "posts/2017-08-16-hadley-pleased/index.html#required-packages",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Required packages",
    "text": "Required packages\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rvest)\nlibrary(xml2)"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#extract-the-html-from-the-rstudio-blog-archive",
    "href": "posts/2017-08-16-hadley-pleased/index.html#extract-the-html-from-the-rstudio-blog-archive",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Extract the HTML from the RStudio blog archive",
    "text": "Extract the HTML from the RStudio blog archive\nTo be able to extract the text from each blog post, we first need to have a link to that blog post. Luckily, RStudio keeps an up to date archive page that we can scrape. Using xml2, we can get the HTML off that page.\n\narchive_page <- \"https://blog.rstudio.com/archives/\"\n\narchive_html <- read_html(archive_page)\n\n# Doesn't seem very useful...yet\narchive_html\n\n\n#> {xml_document}\n#> <html lang=\"en-us\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\\n<meta charset=\"u ...\n#> [2] <body>\\n    <nav class=\"menu\"><svg version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xli ...\n\nNow we use a bit of rvest magic combined with the HTML inspector in Chrome to figure out which elements contain the info we need (I also highly recommend SelectorGadget for this kind of work). Looking at the image below, you can see that all of the links are contained within the main tag as a tags (links).\n\nThe code below extracts all of the links, and then adds the prefix containing the base URL of the site.\n\nlinks <- archive_html %>%\n  \n  # Only the \"main\" body of the archive\n  html_nodes(\"main\") %>%\n  \n  # Grab any node that is a link\n  html_nodes(\"a\") %>%\n  \n  # Extract the hyperlink reference from those link tags\n  # The hyperlink is an attribute as opposed to a node\n  html_attr(\"href\") %>%\n  \n  # Prefix them all with the base URL\n  paste0(\"http://blog.rstudio.com\", .)\n\nhead(links)\n\n\n#> [1] \"http://blog.rstudio.com/2017/08/25/rstudio-conf-2018-early-bird-pricing/\"    \n#> [2] \"http://blog.rstudio.com/2017/08/22/rstudio-v1-1-preview-object-explorer/\"    \n#> [3] \"http://blog.rstudio.com/2017/08/18/google-cloud-platform/\"                   \n#> [4] \"http://blog.rstudio.com/2017/08/16/rstudio-preview-connections/\"             \n#> [5] \"http://blog.rstudio.com/2017/08/15/contributed-talks-diversity-scholarships/\"\n#> [6] \"http://blog.rstudio.com/2017/08/15/shiny-1-0-4/\""
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#html-from-each-blog-post",
    "href": "posts/2017-08-16-hadley-pleased/index.html#html-from-each-blog-post",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "HTML from each blog post",
    "text": "HTML from each blog post\nNow that we have every link, we’re ready to extract the HTML from each individual blog post. To make things more manageable, we start by creating a tibble, and then using the mutate + map combination to created a column of XML Nodesets (we will use this combination a lot). Each nodeset contains the HTML for that blog post (exactly like the HTML for the archive page).\n\nblog_data <- tibble(links)\n\nblog_data <- blog_data %>%\n  mutate(main = map(\n                    # Iterate through every link\n                    .x = links, \n                    \n                    # For each link, read the HTML for that page, and return the main section \n                    .f = ~read_html(.) %>%\n                            html_nodes(\"main\")\n                    )\n         )\n\nblog_data$main[1]\n\n\n#> [[1]]\n#> {xml_nodeset (1)}\n#> [1] <main><div class=\"article-meta\">\\n<h1><span class=\"title\">Newer to R? rstudio::conf 2018 is fo ..."
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#meta-information",
    "href": "posts/2017-08-16-hadley-pleased/index.html#meta-information",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Meta information",
    "text": "Meta information\nBefore extracting the blog post itself, lets grab the meta information about each post, specifically:\n\nAuthor\nTitle\nDate\nCategory\nTags\n\nIn the exploratory analysis, we will use author and title, but the other information might be useful for future analysis.\nLooking at the first blog post, the Author, Date, and Title are all HTML class names that we can feed into rvest to extract that information.\n\nIn the code below, an example of extracting the author information is shown. To select a HTML class (like “author”) as opposed to a tag (like “main”), we have to put a period in front of the class name. Once the html node we are interested in has been identified, we can extract the text for that node using html_text().\n\nblog_data$main[[1]] %>%\n  html_nodes(\".author\") %>%\n  html_text()\n\n\n#> [1] \"Roger Oberg\"\n\nTo scale up to grab the author for all posts, we use map_chr() since we want a character of the author’s name returned.\n\nmap_chr(.x = blog_data$main,\n        .f = ~html_nodes(.x, \".author\") %>%\n                html_text()) %>%\n  head(10)\n\n\n#>  [1] \"Roger Oberg\"        \"Kevin Ushey\"        \"Roger Oberg\"       \n#>  [4] \"Jonathan McPherson\" \"Hadley Wickham\"     \"Winston Chang\"     \n#>  [7] \"Gary Ritchie\"       \"Roger Oberg\"        \"Jeff Allen\"        \n#> [10] \"Javier Luraschi\"\n\nFinally, notice that if we switch \".author\" with \".title\" or \".date\" then we can grab that information as well. This kind of thinking means that we should create a function for extracting these pieces of information!\n\nextract_info <- function(html, class_name) {\n  map_chr(\n          # Given the list of main HTMLs\n          .x = html,\n          \n          # Extract the text we are interested in for each one \n          .f = ~html_nodes(.x, class_name) %>%\n                  html_text())\n}\n\n# Extract the data\nblog_data <- blog_data %>%\n  mutate(\n     author = extract_info(main, \".author\"),\n     title  = extract_info(main, \".title\"),\n     date   = extract_info(main, \".date\")\n    )\n\n\nselect(blog_data, author, date)\n\n\n#> # A tibble: 253 × 2\n#>    author             date      \n#>    <chr>              <chr>     \n#>  1 Roger Oberg        2017-08-25\n#>  2 Kevin Ushey        2017-08-22\n#>  3 Roger Oberg        2017-08-18\n#>  4 Jonathan McPherson 2017-08-16\n#>  5 Hadley Wickham     2017-08-15\n#>  6 Winston Chang      2017-08-15\n#>  7 Gary Ritchie       2017-08-11\n#>  8 Roger Oberg        2017-08-10\n#>  9 Jeff Allen         2017-08-03\n#> 10 Javier Luraschi    2017-07-31\n#> # … with 243 more rows\n\n\nselect(blog_data, title)\n\n\n#> # A tibble: 253 × 1\n#>    title                                                                        \n#>    <chr>                                                                        \n#>  1 Newer to R? rstudio::conf 2018 is for you! Early bird pricing ends August 31.\n#>  2 RStudio v1.1 Preview - Object Explorer                                       \n#>  3 RStudio Server Pro is ready for BigQuery on the Google Cloud Platform        \n#>  4 RStudio 1.1 Preview - Data Connections                                       \n#>  5 rstudio::conf(2018): Contributed talks, e-posters, and diversity scholarships\n#>  6 Shiny 1.0.4                                                                  \n#>  7 RStudio v1.1 Preview: Terminal                                               \n#>  8 Building tidy tools workshop                                                 \n#>  9 RStudio Connect v1.5.4 - Now Supporting Plumber!                             \n#> 10 sparklyr 0.6                                                                 \n#> # … with 243 more rows"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#categories-and-tags",
    "href": "posts/2017-08-16-hadley-pleased/index.html#categories-and-tags",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Categories and tags",
    "text": "Categories and tags\nThe other bits of meta data that might be interesting are the categories and tags that the post falls under. This is a little bit more involved, because both the categories and tags fall under the same class, \".terms\". To separate them, we need to look into the href to see if the information is either a tag or a category (href = “/categories/” VS href = “/tags/”).\n\nThe function below extracts either the categories or the tags, depending on the argument, by:\n\nExtracting the \".terms\" class, and then all of the links inside of it (a tags).\nChecking each link to see if the hyperlink reference contains “categories” or “tags” depending on the one that we are interested in. If it does, it returns the text corresponding to that link, otherwise it returns NAs which are then removed.\n\nThe final step results in two list columns containing character vectors of varying lengths corresponding to the categories and tags of each post.\n\nextract_tag_or_cat <- function(html, info_name) {\n  \n  # Extract the links under the terms class\n  cats_and_tags <- map(.x = html, \n                       .f = ~html_nodes(.x, \".terms\") %>%\n                              html_nodes(\"a\"))\n  \n  # For each link, if the href contains the word categories/tags \n  # return the text corresponding to that link\n  map(cats_and_tags, \n    ~if_else(condition = grepl(info_name, html_attr(.x, \"href\")), \n             true      = html_text(.x), \n             false     = NA_character_) %>%\n      .[!is.na(.)])\n}\n\n# Apply our new extraction function\nblog_data <- blog_data %>%\n  mutate(\n    categories = extract_tag_or_cat(main, \"categories\"),\n    tags       = extract_tag_or_cat(main, \"tags\")\n  )\n\n\nselect(blog_data, categories, tags)\n\n\n#> # A tibble: 253 × 2\n#>    categories tags     \n#>    <list>     <list>   \n#>  1 <chr [3]>  <chr [1]>\n#>  2 <chr [1]>  <chr [0]>\n#>  3 <chr [2]>  <chr [4]>\n#>  4 <chr [1]>  <chr [0]>\n#>  5 <chr [1]>  <chr [0]>\n#>  6 <chr [2]>  <chr [0]>\n#>  7 <chr [1]>  <chr [3]>\n#>  8 <chr [3]>  <chr [8]>\n#>  9 <chr [3]>  <chr [2]>\n#> 10 <chr [1]>  <chr [3]>\n#> # … with 243 more rows\n\n\nblog_data %>%\n  filter(title == \"Building tidy tools workshop\") %>%\n  pull(categories)\n\n\n#> [[1]]\n#> [1] \"Packages\"  \"tidyverse\" \"Training\"\n\n\nblog_data %>%\n  filter(title == \"Building tidy tools workshop\") %>%\n  pull(tags)\n\n\n#> [[1]]\n#> [1] \"Advanced R\"       \"data science\"     \"ggplot2\"          \"Hadley Wickham\"  \n#> [5] \"R\"                \"RStudio Workshop\" \"r training\"       \"tutorial\""
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#the-blog-post-itself",
    "href": "posts/2017-08-16-hadley-pleased/index.html#the-blog-post-itself",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "The blog post itself",
    "text": "The blog post itself\nFinally, to extract the blog post itself, we can notice that each piece of text in the post is inside of a paragraph tag (p). Being careful to avoid the \".terms\" class that contained the categories and tags, which also happens to be in a paragraph tag, we can extract the full blog posts. To ignore the \".terms\" class, use the :not() selector.\n\nblog_data <- blog_data %>%\n  mutate(\n    text = map_chr(main, ~html_nodes(.x, \"p:not(.terms)\") %>%\n                 html_text() %>%\n                 # The text is returned as a character vector. \n                 # Collapse them all into 1 string.\n                 paste0(collapse = \" \"))\n  )\n\n\n\n\n\nselect(blog_data, text)\n#> # A tibble: 253 × 1\n#>    text                                                                         \n#>    <chr>                                                                        \n#>  1 \"Immersion is among the most effective ways to learn any language. Immersing…\n#>  2 \"Today, we’re continuing our blog series on new features in RStudio 1.1. If …\n#>  3 \"RStudio is excited to announce the availability of RStudio Server Pro on th…\n#>  4 \"Today, we’re continuing our blog series on new features in RStudio 1.1. If …\n#>  5 \"rstudio::conf, the conference on all things R and RStudio, will take place …\n#>  6 \"Shiny 1.0.4 is now available on CRAN. To install it, run: For most Shiny us…\n#>  7 \"Today we’re excited to announce availability of our first Preview Release f…\n#>  8 \"Have you embraced the tidyverse? Do you now want to expand it to meet your …\n#>  9 \"We’re thrilled to announce support for hosting Plumber APIs in RStudio Conn…\n#> 10 \"We’re excited to announce a new release of the sparklyr package, available …\n#> # … with 243 more rows"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#who-writes-the-most-posts",
    "href": "posts/2017-08-16-hadley-pleased/index.html#who-writes-the-most-posts",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Who writes the most posts?",
    "text": "Who writes the most posts?\nNow that we have all of this data, what can we do with it? To start with, who writes the most posts?\n\nblog_data %>%\n  group_by(author) %>%\n  summarise(count = n()) %>%\n  mutate(author = reorder(author, count)) %>%\n  \n  # Create a bar graph of author counts\n  ggplot(mapping = aes(x = author, y = count)) + \n  geom_col() +\n  coord_flip() +\n  labs(title    = \"Who writes the most RStudio blog posts?\",\n       subtitle = \"By a huge margin, Hadley!\") +\n  # Shoutout to Bob Rudis for the always fantastic themes\n  hrbrthemes::theme_ipsum(grid = \"Y\")"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#tidytext",
    "href": "posts/2017-08-16-hadley-pleased/index.html#tidytext",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Tidytext",
    "text": "Tidytext\nI’ve never used tidytext before today, but to get our feet wet, let’s create a tokenized tidy version of our data. By using unnest_tokens() the data will be reshaped to a long format holding 1 word per row, for each blog post. This tidy format lends itself to all manner of analysis, and a number of them are outlined in Julia Silge and David Robinson’s Text Mining with R.\n\ntokenized_blog <- blog_data %>%\n  mutate(short_title = str_sub(title, end = 15)) %>%\n  select(title, short_title, author, date, text) %>%\n  unnest_tokens(output = word, input = text)\n\nselect(tokenized_blog, short_title, word)\n#> # A tibble: 85,761 × 2\n#>    short_title     word     \n#>    <chr>           <chr>    \n#>  1 Newer to R? rst immersion\n#>  2 Newer to R? rst is       \n#>  3 Newer to R? rst among    \n#>  4 Newer to R? rst the      \n#>  5 Newer to R? rst most     \n#>  6 Newer to R? rst effective\n#>  7 Newer to R? rst ways     \n#>  8 Newer to R? rst to       \n#>  9 Newer to R? rst learn    \n#> 10 Newer to R? rst any      \n#> # … with 85,751 more rows"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#remove-stop-words",
    "href": "posts/2017-08-16-hadley-pleased/index.html#remove-stop-words",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Remove stop words",
    "text": "Remove stop words\nA number of words like “a” or “the” are included in the blog that don’t really add value to a text analysis. These stop words can be removed using an anti_join() with the stop_words dataset that comes with tidytext. After removing stop words, the number of rows was cut in half!\n\ntokenized_blog <- tokenized_blog %>%\n  anti_join(stop_words, by = \"word\") %>%\n  arrange(desc(date))\n\nselect(tokenized_blog, short_title, word)\n#> # A tibble: 40,315 × 2\n#>    short_title     word     \n#>    <chr>           <chr>    \n#>  1 Newer to R? rst immersion\n#>  2 Newer to R? rst effective\n#>  3 Newer to R? rst learn    \n#>  4 Newer to R? rst language \n#>  5 Newer to R? rst immersing\n#>  6 Newer to R? rst advanced \n#>  7 Newer to R? rst users    \n#>  8 Newer to R? rst improve  \n#>  9 Newer to R? rst language \n#> 10 Newer to R? rst rare     \n#> # … with 40,305 more rows"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#top-15-words-overall",
    "href": "posts/2017-08-16-hadley-pleased/index.html#top-15-words-overall",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Top 15 words overall",
    "text": "Top 15 words overall\nOut of pure curiousity, what are the top 15 words for all of the blog posts?\n\ntokenized_blog %>%\n  count(word, sort = TRUE) %>%\n  slice(1:15) %>%\n  mutate(word = reorder(word, n)) %>%\n  \n  ggplot(aes(word, n)) +\n  geom_col() + \n  coord_flip() + \n  labs(title = \"Top 15 words overall\") +\n  hrbrthemes::theme_ipsum(grid = \"Y\")"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#is-hadley-more-pleased-than-everyone-else",
    "href": "posts/2017-08-16-hadley-pleased/index.html#is-hadley-more-pleased-than-everyone-else",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Is Hadley more “pleased” than everyone else?",
    "text": "Is Hadley more “pleased” than everyone else?\nAs mentioned at the beginning of the post, Hadley apparently uses the word “pleased” in his blog posts an above average number of times. Can we verify this statistically?\nOur null hypothesis is that the proportion of blog posts that use the word “pleased” written by Hadley is less than or equal to the proportion of those written by the rest of the RStudio team.\nMore simply, our null is that Hadley uses “pleased” less than or the same as the rest of the team.\nLet’s check visually to compare the two groups of posts.\n\npleased <- tokenized_blog %>%\n  \n  # Group by blog post\n  group_by(title) %>%\n  \n  # If the blog post contains \"pleased\" put yes, otherwise no\n  # Add a column checking if the author was Hadley\n  mutate(\n    contains_pleased = case_when(\n      \"pleased\" %in% word ~ \"Yes\",\n      TRUE                ~ \"No\"),\n    \n    is_hadley = case_when(\n      author == \"Hadley Wickham\" ~ \"Hadley\",\n      TRUE                       ~ \"Not Hadley\")\n    ) %>%\n  \n  # Remove all duplicates now\n  distinct(title, contains_pleased, is_hadley)\n\npleased %>%\n  ggplot(aes(x = contains_pleased)) +\n  geom_bar() +\n  facet_wrap(~is_hadley, scales = \"free_y\") +\n  labs(title    = \"Does this blog post contain 'pleased'?\", \n       subtitle = \"Nearly half of Hadley's do!\",\n       x        = \"Contains 'pleased'\",\n       y        = \"Count\") +\n  hrbrthemes::theme_ipsum(grid = \"Y\")"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#is-there-a-statistical-difference-here",
    "href": "posts/2017-08-16-hadley-pleased/index.html#is-there-a-statistical-difference-here",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Is there a statistical difference here?",
    "text": "Is there a statistical difference here?\nTo check if there is a statistical difference, we will use a test for difference in proportions contained in the R function, prop.test(). First, we need a continency table of the counts. Given the current form of our dataset, this isn’t too hard with the table() function from base R.\n\ncontingency_table <- pleased %>%\n  ungroup() %>%\n  select(is_hadley, contains_pleased) %>%\n  # Order the factor so Yes is before No for easy interpretation\n  mutate(contains_pleased = factor(contains_pleased, levels = c(\"Yes\", \"No\"))) %>%\n  table()\n\ncontingency_table\n#>             contains_pleased\n#> is_hadley    Yes  No\n#>   Hadley      43  45\n#>   Not Hadley  17 148\n\nFrom our null hypothesis, we want to perform a one sided test. The alternative to our null is that Hadley uses “pleased” more than the rest of the RStudio team. For this reason, we specify alternative = \"greater\".\n\ntest_prop <- contingency_table %>%\n  prop.test(alternative = \"greater\")\n\ntest_prop\n#> \n#>  2-sample test for equality of proportions with continuity correction\n#> \n#> data:  .\n#> X-squared = 45.063, df = 1, p-value = 9.541e-12\n#> alternative hypothesis: greater\n#> 95 percent confidence interval:\n#>  0.2809899 1.0000000\n#> sample estimates:\n#>    prop 1    prop 2 \n#> 0.4886364 0.1030303\n\nWe could also tidy this up with broom if we were inclined to.\n\nbroom::tidy(test_prop)\n#> # A tibble: 1 × 9\n#>   estimate1 estimate2 statistic  p.value parameter conf.low conf.high method    \n#>       <dbl>     <dbl>     <dbl>    <dbl>     <dbl>    <dbl>     <dbl> <chr>     \n#> 1     0.489     0.103      45.1 9.54e-12         1    0.281         1 2-sample …\n#> # … with 1 more variable: alternative <chr>"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#test-conclusion",
    "href": "posts/2017-08-16-hadley-pleased/index.html#test-conclusion",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Test conclusion",
    "text": "Test conclusion\n\n48.86% of Hadley’s posts contain “pleased”\n10.3% of the rest of the RStudio team’s posts contain “pleased”\nWith a p-value of 9.5414477^{-12}, we reject the null that Hadley uses “pleased” less than or the same as the rest of the team. The evidence supports the idea that he has a much higher preference for it!\n\nHadley uses “pleased” quite a bit!"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#conclusion",
    "href": "posts/2017-08-16-hadley-pleased/index.html#conclusion",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis post used a lot of different tools, but that’s the beauty of having over 12,000 R packages at our disposal. I think that this dataset could be used in a number of other ways, so be on the lookout for more posts!"
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "",
    "text": "After realizing how fast I can burn through my free 25 hours on shinyapps.io, I decided to repurpose my RStudio Server to also work with Shiny Server. Here’s my new setup:\nIn case I ever have to go through this madness again, or if anyone else wants to, I’ve compiled some step by step notes on the setup. It’s definitely worth it, though, so that you can have your own RStudio and Shiny servers!\n(I know that some others have already done posts like this, but I went into even more laborious detail on some of the basics.)\nIn this post, I will walk you through getting up and running with an RStudio server. In the next post, you’ll learn to get Shiny server working."
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-1-setup-an-aws-account",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-1-setup-an-aws-account",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Step 1: Setup an AWS Account",
    "text": "Step 1: Setup an AWS Account\nAmazon is nice enough to provide 1 year’s worth of access to their Free Tier for AWS. There are a huge number of options available, but the important one is that they provide a free 750 hours/month to deploy an EC2 instance. That’s just enough to keep 1 EC2 instance active 24/7, since 24 hours x 31 days = 744.\nIf you aren’t familiar with EC2, think of it as your own personal always-on Linux computer that you can connect to through SSH, and access through the web by using an IP address. One step further and you can access it through a custom domain name.\nCreate your free AWS account, and come back when you’ve finished. You should be able to click on the giant sign in button, and sign in to your console.\n\nIf all goes well, you’ll be at the console.\n\nWe won’t do anything else yet, just stay signed in."
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-2-setup-the-rstudio-amazon-machine-image",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-2-setup-the-rstudio-amazon-machine-image",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Step 2: Setup the RStudio Amazon Machine Image",
    "text": "Step 2: Setup the RStudio Amazon Machine Image\nIt’s worth it to get familiar with setting up your own EC2 server, but we won’t have to do that here. Luckily, Louis Aslett has created an Amazon Machine Image (AMI) to take care of all of the hard work for us. It’s basically some preconfigured settings that at the time of writing install the following:\n\nRStudio Server 0.99.903\nR 3.3.1\nShiny Server\nJulia 0.4.6\nPython 3.5.2\nGit\n\nYou can find the link to the image here. Click one of the links on the right to start the setup, I normally click the one closest to me regionally.\nThe Virginia link takes me here:\n\nYou can click through the settings, but to just get setup, click “Review and Launch.” It will let you review one last time, and will likely warn you about security, we will change all that later, just click “Launch.”\nImportant! Amazon will pop up a message box that talks about a key pair. This is how you will SSH into your server later on. This is really important, as you only get this screen one time, and can never come back to it. Setup a new key pair name (it can be anything), and click “Download Key Pair.”\n\nStore the .pem key pair file somewhere on your local computer. This should be a secure location, but somewhere you can remember the file path to. Then click Launch Instances.\nAt the top of the next screen, click Services, and then select EC2. This will take you to the EC2 Dashboard. You should see that you have “1 Running Instance.”\n\nClick on “1 Running Instance,” and you’ll see your server starting up. Below, it’s the one that says “running.”\n\nThere’s one last thing to do before we can access the server. We have to setup the security to allow HTTP (web browser) access. In the “Description” tab in the bottom half of the above image, scroll down until you see “Security Groups.” You’ll likely have something like “launch-wizard-1” there. Click on that.\nOn the next screen, click the “Inbound” tab down where “Description” is listed. As you can see, only the SSH option is available for accessing the instance. Let’s change that.\nClick:\n\nEDIT -> Add Rule -> Type set it to HTTP -> change the source from Custom to Anywhere -> Save\n\nNote that this is not a secure option, but it’ll get you going.\nFinally, to check that you’re up and running, go back to your instances tab (the same image as above). See the Public DNS (IPv4) box? Copy that, and paste it into your browser as a URL. It should take you to an authentication page for RStudio Server. Congrats! You’ve figured something out that took me hours.\nDefaults:\n\nUsername - rstudio\nPassword - rstudio\n\nClever, right?\n\nIf it worked, you should see this."
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-3-new-password-for-rstudio-server",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-3-new-password-for-rstudio-server",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Step 3: New password for RStudio Server",
    "text": "Step 3: New password for RStudio Server\nIt’s advised that you immediately change the password. There are two ways to do so. The first way is easy. In the Welcome.R file that is shown above, you’ll see a description for how to library(\"RStudioAMI\") and then run passwd(). You can do that, but eventually you’ll have to SSH into your server for something, so you may as well learn how now.\nHave you still got the AWS Console Instances page up? The one where you found the Public DNS (IPv4). Here it is again.\n\nAWS has made it pretty easy to connect through SSH. Click the “Connect” button. A window should pop up with some pretty detailed instructions. Do you have the path to your .pem file lying around? You’re going to need it!\n\nI run on a Mac, so I’ll be using Terminal. If you run on Windows, you’ll need to download PuTTY. Open up Terminal, and type in the following for step 3:\n\nchmod 400 path_to_file/file.pem\n\nNote that you actually need to locate your pem file, and pass Terminal the path. This command hides the file, and is necessary to connect.\nNext you’ll connect to your instance by typing:\n\nssh -i \"path_to_file/file.pem\" ubuntu@ec2-IPADDRESS.compute-1.amazonaws.com\n\nAgain, you’ll have to type in the correct path, but the IP address shown for you should be correct.\nWhen you connect for the first time, it might give you a prompt basically saying, “Are you sure?” Type yes. Hopefully you’ll see something like this:\n\n\n\n\n\nTo update the password for the rstudio user:\n\nsudo passwd rstudio\n\nThen follow the prompts. Type exit to disconnect from the server, and go back to your RStudio Server site. Try and login with the new password."
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-4-update-everything",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-4-update-everything",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Step 4: Update everything",
    "text": "Step 4: Update everything\nUnfortunately, the Amazon Images are only updated every few releases of RStudio Server. However, it’s not too hard to get the newest release installed straight from RStudio’s site.\nYou’ll need to first set the CRAN mirror on your Ubuntu server so that you can actually download the latest version of R. This part is a bit of a pain, requiring you to work with some text editors through Terminal, but bear with me.\nSign back into your Linux server through Terminal following the above instructions. When you’re done, type:\n\nls\n# rstudio-server-1.0.143-amd64.deb  shiny-server-1.5.3.838-amd64.deb\n\nAnd you should see a few .deb files, one for rstudio-server and one for shiny-server (mine are already upgraded). If you don’t, well, hopefully you can still try and follow along (Maybe cd ~ will get you there? Maybe go back to step 1?).\nNow, we need to navigate to the correct file and add the CRAN mirror to it. That is located at /etc/apt/sources.list for you pros. For the rest of us, follow along.\nFirst navigate up two levels:\n\ncd ../..\nls\n# bin   etc         initrd.img.old            lib         media  proc  sbin  sys  var\n# boot  home        jupyterhub_cookie_secret  lib64       mnt    root  snap  tmp  vmlinuz\n# dev   initrd.img  jupyterhub.sqlite         lost+found  opt    run   srv   usr  vmlinuz.old\n\nThen, we need to get into etc/apt:\n\ncd etc/apt\nls\n# apt.conf.d     sources.list    sources.list.save  trusted.gpg~\n# preferences.d  sources.list.d  trusted.gpg        trusted.gpg.d\n\nI don’t have a whole lot of experience with terminal editors, but I know enough to get by. I will use nano, which I believe comes on every Mac, to open up my sources.list file. sudo is likely needed to give admin privelages so you can save the file afterwards.\n\nsudo nano sources.list\n\nA file should open, scroll all the way down to the bottom, and on a new line paste:\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu/ xenial/\n\nThere are a number of different versions of this command here, but this specific one works because the Amazon Image you downloaded uses Xenial (tbh I don’t really know what that means, trial and error and a bit of common sense got it to work).\nNow you have to escape from nano, a first-timer’s nightmare. Follow this sequence of commands:\n\n^X # Control+X      This is used to \"Quit\"\nY  # Yes            This is used to save the file when it asks you\n# Then click Enter/Return on your keyboard to resave the file with the same name\n\nNow that that is taken care of, navigate back to:\n\ncd ~\n\nAnd you can update all of the linux apps, and then download the latest version of R using the two commands:\n\nsudo apt-get update\nsudo apt-get install r-base\n\nFinally, you’ll update to the latest version of RStudio Server. At the time of writing, this is 1.0.143, but it updates regularly, so go here and scroll down to find the latest update for 64bit Ubuntu. The commands generally look like:\n\nsudo apt-get install gdebi-core\nwget https://download2.rstudio.org/rstudio-server-1.0.143-amd64.deb\nsudo gdebi rstudio-server-1.0.143-amd64.deb\n\nAll done? Great! exit out of your Linux server, and reload your RStudio Server in the browser. When you login, you should be able to run version to see the latest version of R, and go to Help -> About RStudio to see the updated version of RStudio Server!"
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#last-words",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#last-words",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Last words",
    "text": "Last words\nThis was quite the struggle. There are a few other resources out there to help, but I still struggled through some pieces of this one. Hopefully it wasn’t near as bad for you! In the next post, I’ll show you how to update your Shiny Server and start hosting your own apps on there (with no 5 app limit like shinyapps.io)!\nHere are some additional resources that I found helpful when setting up my server:\n\nhttps://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/#user-libraries"
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html",
    "title": "Writing a paper with RStudio",
    "section": "",
    "text": "This semester I had to write a paper for my Financial Econometrics class. My topic was on analyzing the volatility of Bitcoin using GARCH modeling. I’m not particularly interested in Bitcoin, but with all the recent news around it, and with its highly volatile characteristics, I figured it would be a good candidate for analysis.\nI did the analysis in R, but I wanted to take it a step further. Could I write the entire paper in R and RStudio in a fairly professional format? Yes.\n\nI figured I would outline a few issues I had along the way, and talk about the experience for anyone that might do something similar."
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#github",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#github",
    "title": "Writing a paper with RStudio",
    "section": "Github",
    "text": "Github\nIf you want to go view the entire paper and analysis, it’s on Github. Check out the repo here. The PDF paper itself is buried here.\nIf you really want to follow the analysis steps, look in R/ to see the code that generates everything else. Do note that I had to keep the raw data zipped to get it on Github, so to run the data cleaning script, you will have to unzip the file in data/raw/. The paper is written in paper/."
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#tooling",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#tooling",
    "title": "Writing a paper with RStudio",
    "section": "Tooling",
    "text": "Tooling\nTo even begin thinking about this, I needed two R packages. One that allowed me to write the post in RMarkdown and render it into a journal style format, and one that created nice looking and customizable tables.\nFor the first, I initially looked into the rticles package from the RStudio team, which is amazing, but I wasn’t satisfied with the journal styles that they had as options. I then remembered that Dirk Eddelbuettel and James Balamuta had created pinp, or, Pinp Is Not PNAS, as an extension of rticles and in particular the PNAS journal format. This one fit the bill for me, as it provided an uncluttered journal layout with a bit of nice coloring as well.\nFor the second, there are plenty of options out there for creating LaTeX tables in R, xtable, stargazer, etc. However, I recently had found huxtable and was instantly drawn to its intuitive pipeable syntax for creating tables from data frames. Now that I have a good bit of experience with it, I can say for certain that it is incredibly flexible, and I doubt I’ll ever use another package for table creation.\nI was initially pretty worried about how well the two would play together. Sure enough, after rendering a table or two I got errors that (for a non LaTeX expert) seemed faily cryptic. However, it turned out to be a LaTeX package (not R package) dependency problem where huxtable needed packages that pinp didn’t use by default. That was easily fixed by adding the following to the top of the YAML header provided by pinp:\nheader-includes:\n   - \\usepackage{tabularx,colortbl,multirow,hhline,mathtools}\nI’ll also add in there that I used the rugarch package for all of my GARCH modeling, and it exceeded my expectations. Its author Alexios has created a fantastic S4 class system that makes trying different versions of GARCH models dead simple."
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#huxtable",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#huxtable",
    "title": "Writing a paper with RStudio",
    "section": "Huxtable",
    "text": "Huxtable\nI really wish huxtable had more publicity. It is a highly underused package for everything that it can do. For example, the following table was created using huxtable. Notice the math symbols in the first column, the fact that the standard errors are closer to the row above them due to smaller margins, and the bolding of certain cells.\n\nThe tibble used to create the core table looked like this:\n\n#> # A tibble: 14 × 4\n#>    metric                   `5-Min`    Daily `5-Min No Outlier`\n#>    <chr>                      <dbl>    <dbl>              <dbl>\n#>  1 MAPE                    0.0278   0.0372             0.0249  \n#>  2 RMSE                    0.00407  0.00285            0.00244 \n#>  3 MZ Intercept            0.00226  0.00167            0.00127 \n#>  4 MZ Intercept Std. Error 0.000271 0.000486           0.000314\n#>  5 MZ Slope                0.282    0.441              0.798   \n#>  6 MZ Slope Std. Error     0.0540   0.144              0.113   \n#>  7 MZ $R^2$                0.200    0.0792             0.316   \n#>  8 MAPE                    0.281    0.325              0.279   \n#>  9 RMSE                    0.00788  0.00711            0.00700 \n#> 10 MZ Intercept            0.00250  0.00232            0.00164 \n#> 11 MZ Intercept Std. Error 0.000756 0.00127            0.000963\n#> 12 MZ Slope                0.161    0.196              0.607   \n#> 13 MZ Slope Std. Error     0.151    0.376              0.347   \n#> 14 MZ $R^2$                0.0104   0.00249            0.0275\n\nAnd the R code to generate the table looked like:\n\nlibrary(huxtable)\n\ntriple_blank <- function() {\n  c(\"\", \"\", \"\")\n}\n\nht <- hux_data %>%\n  huxtable() %>%\n  \n  # Column names\n  add_colnames() %>%\n  \n  # Proxy rows\n  insert_row(\"Proxy: RV\",    triple_blank(), after = 1) %>%\n  insert_row(\"Proxy: $r^2$\", triple_blank(), after = 9) %>%\n  \n  # Number rounding\n  set_number_format(everywhere, everywhere, \"%5.4f\") %>%\n  \n  # Bold\n  set_bold(matrix(c(2,10)), 1, TRUE) %>%\n  set_bold(1, everywhere, TRUE) %>%\n  \n  # Alignment\n  set_align(everywhere, everywhere, 'center') %>%\n  set_align(everywhere, 1, 'right') %>%\n  \n  # Padding\n  set_all_padding(value = 10) %>%\n  set_top_padding(c(6, 8, 14, 16), everywhere, 0) %>%\n  set_bottom_padding(c(5, 7, 13, 15), everywhere, 0) %>%\n  set_left_padding(everywhere, 1, -40) %>%\n\n  # Borders\n  set_bottom_border(matrix(c(1, 9)), everywhere, value = .3) %>%\n  set_right_border(everywhere, 1, .3) %>%\n  \n  # Escape latex\n  set_escape_contents(everywhere, 1, FALSE) %>%\n  set_escape_contents(1, everywhere, FALSE) %>%\n  \n  set_caption(\"Out of sample performance of GARCH(1,1) Normal models. MAPE for 5-min is lower than for daily. Across the board, using RV as a proxy over $r^2$ gives more accurate results. Removing the 1 extreme forecast from the 5-min method results in a much higher MZ $R^2$, and a MZ slope much closer to 1.\") \n\nht[1,1] <- \"\"\n\nht\n\nYou even get to view your table in the console without knitting the document to check that you put lines / bolding / even coloring in the right place. This saves more time than you might think!\n\n\n\n\n\nI find the pipeable syntax very intuitive, and the family of set_*() functions allow for endless combinations. I didn’t even venture into the world of conditional cell formatting, but I hear that that is pretty powerful too."
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#adding-images",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#adding-images",
    "title": "Writing a paper with RStudio",
    "section": "Adding images",
    "text": "Adding images\nI had created a script in my analysis that generated a number of graphics that I wanted to include in the paper. The pinp vignette is an excellent source of examples for these common use cases, and the following allowed me to embed my images in the paper and add a caption with minimal effort:\n\\begin{figure*}\n  \\begin{center}\n    \\includegraphics[width=1.00\\textwidth, height=8.5in]{../../visualizations/returns} \n  \\end{center}\n  \\caption{Descriptive plots of 5-minute and daily returns}\\label{fig}\n\\end{figure*}\nNotice the relative path ../../visualizations/returns where I backtrack up two levels from the location of the RMarkdown paper document and then into my visualizations folder. My file structure looked a bit like this:\nfin-econ-project/\n\n  - fin-econ-project.Rproj\n  \n  - visualizations/\n    - returns.png\n    \n  - paper/\n    - forecasting-volatility/\n        - forecasting-volatility.Rmd\nIdeally, I would have set my directory to be the RStudio Project directory so I could have just done visualizations/returns, but pinp and rticles both dump a large number of files into whatever directory you render it from, and I didn’t want that cluttering things up. Perhaps specifying the location of that file dump can be a separate feature?"
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#adding-equations",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#adding-equations",
    "title": "Writing a paper with RStudio",
    "section": "Adding equations",
    "text": "Adding equations\nAdding mathematical equations can be done in two main ways.\nAs usual, you can add inline math with the use of a dollar sign, then the equation, then end with a dollar sign, it ends up looking like this: \\(x + y = z\\). Make sure that you don’t leave a space between the dollar signs and the equation, otherwise it doesn’t render and you end up with: $ x + y = z $.\nLarger chunks of equations that need their own lines can be specified using two dollar signs on each side:\n$$ \nr_{t_i}         = \\epsilon_{t_i} \\\\\n\\epsilon_{t_i}  = \\sigma_{t_i}  z_{t_i} \\\\\n\\sigma_{t_i}    = \\alpha \\epsilon_{t_{i-1}}  + \\beta \\sigma_{t_{i-1}}\n$$\n\\[\nr_{t_i}         = \\epsilon_{t_i} \\\\\n\\epsilon_{t_i}  = \\sigma_{t_i}  z_{t_i} \\\\\n\\sigma_{t_i}    = \\alpha \\epsilon_{t_{i-1}}  + \\beta \\sigma_{t_{i-1}}\n\\]\nUnfortunately, this doesn’t align the equations at the equal sign, and I think that that looks pretty nice. To do this, you can add &= instead of just = along with adding \\begin{aligned} and \\end{aligned} before and after the equation.\n$$\n\\begin{aligned}\n  r_{t_i}         &= \\epsilon_{t_i} \\\\\n  \\epsilon_{t_i}  &= \\sigma_{t_i}  z_{t_i} \\\\\n  \\sigma_{t_i}    &= \\alpha \\epsilon_{t_{i-1}}  + \\beta \\sigma_{t_{i-1}}\n\\end{aligned}\n$$\nto get:\n\\[\n\\begin{aligned}\n  r_{t_i}         &= \\epsilon_{t_i} \\\\\n  \\epsilon_{t_i}  &= \\sigma_{t_i}  z_{t_i} \\\\\n  \\sigma_{t_i}    &= \\alpha \\epsilon_{t_{i-1}}  + \\beta \\sigma_{t_{i-1}}\n\\end{aligned}\n\\]\nNice!"
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#overall",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#overall",
    "title": "Writing a paper with RStudio",
    "section": "Overall",
    "text": "Overall\nFor the most part, I enjoyed writing the paper straight from RStudio. Once I figured out a few of the pain points with directory locations, images, equations, and dependencies, the process was pretty smooth. The only other comment I have is that the pinp Knit process is a bit slow. I doubt this has too much to do with the implementation, but more with the underlying rendering engines. I wish there was some way to have Live Rendering like with blogdown so that I could just keep a rendered version of the paper up and have it reload every time I save. That would be the dream!"
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#update",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#update",
    "title": "Writing a paper with RStudio",
    "section": "Update",
    "text": "Update\nThanks to Dirk, the dream has come true."
  },
  {
    "objectID": "posts/2018-03-06-copula-resources/index.html",
    "href": "posts/2018-03-06-copula-resources/index.html",
    "title": "Copula Resources",
    "section": "",
    "text": "NC State University lecture notes - A consise introduction into copulas. This is a good place to start but not a good place to find examples.\nCopulas for Finance: A Reading Guide and Some Applications - A much more intense survey of copulas. Math heavy but full of examples. Page 20 includes a few simulation techniques, but overcomplicates the simple ones (Gaussian copula) in order to stay general.\nWiki: Copulas - The mathematical definition section, along with Sklar’s theorem and the section of Gaussian Copulas makes this worthwhile to look at.\nStackExchange answer - This was the missing link for me. A great description of the “point of a copula.” The full explanation in the last paragraph of the answer made things click for me.\nDataScience+ - A good walkthrough of using the copula R package. Includes an example of using a t-copula with normal marginals.\ncopula - The copula R package. A one stop shop for your copula needs if you use R and don’t want to (or have to) implement it all yourself.\nMatlab example - Obviously the code is all in matlab, but they do a nice job of explaining how to simulate from copulas, without the theory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Davis Vaughan",
    "section": "",
    "text": "Mar 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Davis, a Software Engineer at RStudio working on creating user friendly R packages with the rest of the tidyverse team."
  },
  {
    "objectID": "about.html#what-else",
    "href": "about.html#what-else",
    "title": "About",
    "section": "What else?",
    "text": "What else?\nI’ve developed a few R packages:\n\nfurrr\nslider\nclock\nalmanac\nhardhat\nworkflows\nprobably\n\nAnd I work on many more, including:\n\ntidyr\nvctrs\nyardstick"
  }
]