[
  {
    "objectID": "posts/2019-08-13-persistant-r-objects-in-c/index.html",
    "href": "posts/2019-08-13-persistant-r-objects-in-c/index.html",
    "title": "Persistent R Objects in C",
    "section": "",
    "text": "This is another entry in my series of R + C based posts (you can see a full list here). This article focuses on a somewhat esoteric skill: constructing a global R object at the C level in a persistent way. By “persistent”, I mean that this object will only be created once (at package load time), and will be reusable throughout the life of the R session. You’ll be able to call it from other C files, and can even return the object to the R side. The other “trick” that will be used is a way to run arbitrary C code on R package load, using .onLoad() + .Call(). This is actually much more generic than what we will use it for in this article, so it is worth paying attention to in case you have other uses for it. Along the way, I’ll also use C header files to share C functions/objects between files, and discuss a bit about how I set up my R packages that use C code.\nMost of these ideas are not my own. They are adaptations of ideas used by Lionel Henry in vctrs and rlang.\nWhy are persistent R objects callable from C useful? I can think of two reasons.\n\nThe first is performance. You might have a simple R object (for instance, an integer vector holding 1) that generally takes a small amount of time to create, but is generated and destroyed thousands of time across your C code base. To save a little bit of time, you might want to make this a persistent, unchangeable, global variable.\nThe other is just for readability. Rather than having to deal with PROTECT()ing and UNPROTECT()ing common variables like int_one in the partial example below:\n\n\nSEXP int_one = PROTECT(Rf_ScalarInteger(1));\n\n// Create an R list of length 1, put `int_one` in it\nSEXP result = PROTECT(Rf_allocVector(VECSXP, 1));\nSET_VECTOR_ELT(result, 0, int_one);\n\nUNPROTECT(2); // unprotect `int_one` and `result`\nreturn result;\n\nYou can instead declare int_one as a global variable with a more permanent meaningful name, like shared_int_one, and use it without worrying about protection:\n\nSEXP result = PROTECT(Rf_allocVector(VECSXP, 1));\n\n// can use `shared_int_one` without creating a new one\nSET_VECTOR_ELT(result, 0, shared_int_one);\n\nUNPROTECT(1); // only have to care about `result` protection\nreturn result;\n\nWhen you have a large C based R package, these kinds of things really pay off in terms of increasing readability and cohesiveness of your package, especially if the global variable takes a few lines of C code to create each time. Additionally, if naming conventions for these kinds of variables are used consistently, you’ll immediately be able to recognize what shared_empty_dbl is without having to look it up in the code base. This makes reading over C code a more pleasant experience.\nThe rest of this post will focus on creating a package that constructs some of these global variables. Specifically, we will look at creating a shared empty integer and a shared character vector, and then we will see how to return them back to the R side. One thing to keep in mind is that these kinds of things take a lot of setup on the C side for the first object, but adding subsequent objects is much simpler.\nIf you haven’t read Now You C Me, and you aren’t too familiar with working on an R package with C code in it, you might want to go check out that post before continuing. It will teach you the basics of working with an R package containing C code.\nThe final product is an R package called cshared. It contains one R function, get_shared_objects(). I’ll discuss the bits and pieces of the package throughout the post, but that will be the ultimate reference for the end result."
  },
  {
    "objectID": "posts/2019-08-13-persistant-r-objects-in-c/index.html#setup",
    "href": "posts/2019-08-13-persistant-r-objects-in-c/index.html#setup",
    "title": "Persistent R Objects in C",
    "section": "Setup",
    "text": "Setup\nFirst, some setup. We’ll leverage usethis and devtools to get our new package up and running. I’m assuming you are working in RStudio for this. The Now You C Me post describes these steps in much greater detail.\n\n# Create a new R package, cshared\nusethis::create_package(\"~/path/to/location/for/the/package/cshared\")\n\n# Use roxygen2\nusethis::use_roxygen_md()\n\n# As prompted by use_roxygen_md()\ndevtools::document()\n\n# Set up `cshared-package.R`, which also gives usethis a place to add extra\n# roxygen namespace tags, which is used by `use_c()` later on.\nusethis::use_package_doc()\n\n# Create a `src/shared.c` file, and add the all important registration info\n# to `cshared-package.R`\nusethis::use_c(\"shared\") \n\n# Initialize the C DLL, otherwise document() will complain\ndevtools::load_all(\".\")\n\n# As prompted by use_c()\ndevtools::document()"
  },
  {
    "objectID": "posts/2019-08-13-persistant-r-objects-in-c/index.html#header-files",
    "href": "posts/2019-08-13-persistant-r-objects-in-c/index.html#header-files",
    "title": "Persistent R Objects in C",
    "section": "Header Files",
    "text": "Header Files\nAt this point you should be in an R package, and if you’ve opened shared.c you should see this staring at you:\n\n#define R_NO_REMAP\n#include <R.h>\n#include <Rinternals.h>\n\nI actually like to move these defines / includes into a package API header file that I can #include in all of my .c files, so personally I’m going to create a cshared.h file next, and move this over there. There’s not a shortcut for this, so in RStudio do File -> New File -> C++ File then save it as cshared.h in the src/ folder. Copy those three lines to that file, and remove them from shared.c, replacing them with the following single include statement, which will have the same effect:\n\n#include \"cshared.h\"\n\nTo prevent cshared.h from accidentally being included twice in the same file, we should also add some header include guards:\n\n#ifndef CSHARED_H\n#define CSHARED_H\n\n#define R_NO_REMAP\n#include <R.h>\n#include <Rinternals.h>\n\n#endif"
  },
  {
    "objectID": "posts/2019-08-13-persistant-r-objects-in-c/index.html#c---r",
    "href": "posts/2019-08-13-persistant-r-objects-in-c/index.html#c---r",
    "title": "Persistent R Objects in C",
    "section": "C -> R",
    "text": "C -> R\nOkay, now we have the basic structure set up, so let’s wire up a C function to be callable from the R side. For now, it will create a list containing an empty integer vector and a character vector holding \"tidyverse\", and return it to the R side. Later it will return the same list but holding the shared versions of these objects. Add the following function to shared.c:\n\n#include \"cshared.h\"\n\nSEXP cshared_get_shared_objects() {\n  // An empty integer vector\n  SEXP empty_int = PROTECT(Rf_allocVector(INTSXP, 0));\n\n  // Character vector of size 1, containing \"hello world\"\n  SEXP tidyverse = PROTECT(Rf_allocVector(STRSXP, 1));\n  SET_STRING_ELT(tidyverse, 0, Rf_mkChar(\"tidyverse\"));\n\n  // Initialize the output list, then insert our objects into it\n  SEXP out = PROTECT(Rf_allocVector(VECSXP, 2));\n  SET_VECTOR_ELT(out, 0, empty_int);\n  SET_VECTOR_ELT(out, 1, tidyverse);\n\n  // Must unprotect 3 PROTECT() calls before exiting!\n  UNPROTECT(3);\n  return out;\n}\n\nTo call this from R, we need an init.c file that registers the C routine to the R side. We’ve done something like this in the other blog post, so create init.c and fill it with:\n\n#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\n/* .Call calls */\nextern SEXP cshared_get_shared_objects();\n\nstatic const R_CallMethodDef CallEntries[] = {\n  {\"cshared_get_shared_objects\", (DL_FUNC) &cshared_get_shared_objects, 0},\n  {NULL, NULL, 0}\n};\n\nvoid R_init_cshared(DllInfo *dll) {\n  R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n  R_useDynamicSymbols(dll, FALSE);\n}\n\nOver on the R side, we now need an R function that calls this cshared_get_shared_objects routine. Call usethis::use_r(\"shared\") and fill the resulting R file with:\n\n#' Get the shared objects\n#'\n#' @examples\n#'\n#' get_shared_objects()\n#'\n#' @export\nget_shared_objects <- function() {\n  .Call(cshared_get_shared_objects)\n}\n\nLastly, run devtools::load_all() and devtools::document() to recompile the package and ensure that the shiny new get_shared_objects() is exported.\nYou should now be able to call:\n\nget_shared_objects()\n#> [[1]]\n#> integer(0)\n#> \n#> [[2]]\n#> [1] \"tidyverse\""
  },
  {
    "objectID": "posts/2019-08-13-persistant-r-objects-in-c/index.html#lets-share",
    "href": "posts/2019-08-13-persistant-r-objects-in-c/index.html#lets-share",
    "title": "Persistent R Objects in C",
    "section": "Let’s Share",
    "text": "Let’s Share\nThe next step is to replace our empty_int and tidyverse C variables with shared global variables that were created at package load time. This will clean up our code a bit, and make cshared_get_shared_object() a bit easier to read. But accomplishing this requires some thought! What we want is a way to initialize some C SEXP objects when the R package is loaded. Generally, when we want to perform any action when a package is loaded we use the .onLoad() hook (see ?.onLoad for more). To make it actually initialize our C variables, we will use .Call() from inside .onLoad() to call a C function that does the initialization.\nThe general outline we are going to follow is:\n\nCreate a C global variable, initialized to NULL.\nCreate a C initialization function where we modify that global variable and set it to its actual value.\nRegister this initialization function as a routine callable from R like we did with cshared_get_shared_objects().\nCall it from .onLoad().\n\nWe will start with empty_int, and then add tidyverse. I find that it is useful to store these global variables in a utils.c file, with a companion utils.h file that holds the definitions, allowing you to share them with other .c files. So, to start, create utils.h and place the following in it:\n\n#ifndef CSHARED_UTILS_H\n#define CSHARED_UTILS_H\n\n#include \"cshared.h\"\n\nSEXP cshared_shared_empty_int;\n\n#endif\n\nAll this holds is the “definition” of the global object cshared_shared_empty_int. By “definition” I just mean that we don’t actually initialize the thing here, we just say “hey, there is this thing called ‘cshared_shared_empty_int’, it is going to be a SEXP, and somewhere else it is going to be initialized, but if you #include \"utils.h\" you can use this thing”.\nNow create utils.c, where we will actually initialize the object:\n\n#include \"cshared.h\"\n#include \"utils.h\"\n\nSEXP cshared_shared_empty_int = NULL;\n\nSEXP cshared_init_utils() {\n  cshared_shared_empty_int = Rf_allocVector(INTSXP, 0);\n  R_PreserveObject(cshared_shared_empty_int);\n  MARK_NOT_MUTABLE(cshared_shared_empty_int);\n  \n  Rprintf(\"Initialized!\");\n  \n  return R_NilValue;\n}\n\nHere, SEXP cshared_shared_empty_int = NULL; declares it as a global variable, but just sets it to NULL. We can’t set it directly to an empty integer vector because that isn’t a “compile time value”, it is a “run time value”, meaning it can’t be known before the program starts.\ncshared_init_utils() is the initialization function that we are eventually going to call from R in .onLoad(). It does the following:\n\nUpdates cshared_shared_empty_int to actually hold an empty integer vector.\nCalls R_PreserveObject() on it to ensure it isn’t garbage collected.\nCalls MARK_NOT_MUTABLE() on it to ensure it can’t be overwritten accidentally throughout the life of the R session.\n\nI’ve also added a print statement to prove that every time the package is loaded, this code is run.\nNow we have to register it to the R side, so modify init.c to export cshared_init_utils(). That looks like:\n\n#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\n/* .Call calls */\nextern SEXP cshared_get_shared_objects();\nextern void cshared_init_utils();\n\nstatic const R_CallMethodDef CallEntries[] = {\n  {\"cshared_get_shared_objects\", (DL_FUNC) &cshared_get_shared_objects, 0},\n  {\"cshared_init_utils\", (DL_FUNC) &cshared_init_utils, 0},\n  {NULL, NULL, 0}\n};\n\nvoid R_init_cshared(DllInfo *dll) {\n  R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n  R_useDynamicSymbols(dll, FALSE);\n}\n\nIf we devtools::load_all() now, we should have access to the cshared_init_utils routine object. This is what we need to .Call() from .onLoad(). I generally put my .onLoad() in zzz.R, as it is an auxiliary function. It should be pretty simple:\n\n.onLoad <- function(libname, pkgname) {\n  .Call(cshared_init_utils)\n}\n\nIf we devtools::load_all() again, this will trigger .onLoad(), and you should see…\n\ndevtools::load_all()\n#> Loading cshared\n#> Initialized!\n\nGreat! So now we know that code is being run. At this point, go back and remove the Rprintf() line from cshared_init_utils().\nHead back to shared.c. At the top, just under #include \"cshared.h\", add #include \"utils.h\" which will give you access to cshared_shared_empty_int. Now update cshared_get_shared_objects() to use it. The function is becoming a bit easier to read!\n\n#include \"cshared.h\"\n#include \"utils.h\" // To access `cshared_shared_empty_int`\n\nSEXP cshared_get_shared_objects() {\n  // Character vector of size 1, containing \"hello world\"\n  SEXP tidyverse = PROTECT(Rf_allocVector(STRSXP, 1));\n  SET_STRING_ELT(tidyverse, 0, Rf_mkChar(\"tidyverse\"));\n\n  SEXP out = PROTECT(Rf_allocVector(VECSXP, 2));\n  SET_VECTOR_ELT(out, 0, cshared_shared_empty_int); // <- using it here!\n  SET_VECTOR_ELT(out, 1, tidyverse);\n\n  UNPROTECT(2);\n  return out;\n}\n\nAgain, run devtools::load_all() and call get_shared_objects(). It should work as before, but this time it is returning a list holding the shared integer vector along with the tidyverse string!"
  },
  {
    "objectID": "posts/2019-08-13-persistant-r-objects-in-c/index.html#the-tidyverse-string",
    "href": "posts/2019-08-13-persistant-r-objects-in-c/index.html#the-tidyverse-string",
    "title": "Persistent R Objects in C",
    "section": "The tidyverse string",
    "text": "The tidyverse string\nThe final step is to make the tidyverse string global and shared. Now that we have the infrastructure set up, this is much more straightforward. Update utils.h with a strings_tidyverse variable:\n\n#ifndef CSHARED_UTILS_H\n#define CSHARED_UTILS_H\n\n#include \"cshared.h\"\n\nSEXP cshared_shared_empty_int;\n\nSEXP strings_tidyverse;\n\n#endif\n\nUpdate utils.c with:\n\n#include \"cshared.h\"\n#include \"utils.h\"\n\nSEXP cshared_shared_empty_int = NULL;\n\n// This is new\nSEXP strings_tidyverse = NULL;\n\nSEXP cshared_init_utils() {\n  cshared_shared_empty_int = Rf_allocVector(INTSXP, 0);\n  R_PreserveObject(cshared_shared_empty_int);\n  MARK_NOT_MUTABLE(cshared_shared_empty_int);\n\n  // This is new\n  strings_tidyverse = Rf_allocVector(STRSXP, 1);\n  R_PreserveObject(strings_tidyverse);\n  SET_STRING_ELT(strings_tidyverse, 0, Rf_mkChar(\"tidyverse\"));\n  MARK_NOT_MUTABLE(strings_tidyverse);\n\n  return R_NilValue;\n}\n\nThis does much of the same as what we did with cshared_shared_empty_int. It creates a character vector of size 1 to overwrite the NULL global variable, preserves it, sets the first element value to \"tidyverse\", then marks it as immutable.\nFinally we can go back to shared.c and use strings_tidyverse.\n\n#include \"cshared.h\"\n#include \"utils.h\" // To access `cshared_shared_empty_int` and `strings_tidyverse`\n\nSEXP cshared_get_shared_objects() {\n  SEXP out = PROTECT(Rf_allocVector(VECSXP, 2));\n  SET_VECTOR_ELT(out, 0, cshared_shared_empty_int);\n  SET_VECTOR_ELT(out, 1, strings_tidyverse);\n\n  UNPROTECT(1);\n  return out;\n}\n\nOne thing that I hope is clear is how much more focused cshared_get_shared_objects() is. It’s much easier to see what the purpose of the function is when you don’t have to worry about creating these common shared objects. Additionally, you only have to UNPROTECT() 1 value, out, which makes things slightly easier to keep track of. I also appreciate the fact that we can give our global objects evocative names like strings_tidyverse. If I had another string object I wanted to make into a global variable, I could call it strings_dplyr. When I come across other C code that uses this variable, I immediately know what its value is because of this consistent naming convention."
  },
  {
    "objectID": "posts/2019-08-13-persistant-r-objects-in-c/index.html#conclusion",
    "href": "posts/2019-08-13-persistant-r-objects-in-c/index.html#conclusion",
    "title": "Persistent R Objects in C",
    "section": "Conclusion",
    "text": "Conclusion\nThese global variables are a neat trick for making code clearer, more internally consistent, and occasionally a bit faster. Additionally, being able to call arbitrary C code on R package load is a useful tool in more ways than just global variable initialization (which we didn’t get to explore in this post). In a later post, I hope to show how to use this trick to initialize a variable holding a call object that let’s you efficiently call an R function from C."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html",
    "title": "Debugging an R Package with C++",
    "section": "",
    "text": "This post is dedicated to teaching you how to debug an R package that has C++ code in it. I had no clue how to do this in a formal way, and honestly this post is for future me. This post is long, because this stuff is finicky, and I wanted to document things as explictly as possible.\nThe method of debugging you will learn here has advantages and drawbacks:\n\n\n✅ It is great because you can jump right to places where R would generally crash and shut down, and instead have a chance to figure out why things broke.\n\n\n✅ You can print out any R variables at the C++ level.\n\n\n✅ You can step forward through your code, one line at a time, just like how you do from RStudio.\n\n\n✅ You can even step up into the function that called the one you are currently in, which is very valuable if you have a guess as to where something went wrong, and then want to backtrack to where the wrong result came from.\n\n\n✅ You can run arbitrary C++ code interactively while inside a function to help you interrogate objects.\n\n\n❌ It is painful to get going, and requires a decent amount of time investment.\n\n\n❌ Not every C++ expression works as you might expect when running it interactively. For example, you can’t easily use std::cout.\n\n\nBecause this method is a pain to get going, my recommendation would be to start with printing out objects using some combination of:\n\n\nRcpp::Rcout << obj << std::endl for Rcpp objects.\n\nRf_PrintValue() for SEXP objects.\n\nR_inspect() for other details about SEXP objects (like attributes).\n\nIf you’ve tried that, and still can’t seem to figure it out, it might be time for a true debugger."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#os",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#os",
    "title": "Debugging an R Package with C++",
    "section": "OS",
    "text": "OS\nI am using a MacOS running Mojave. I am also using R 3.5.1. I will be generally be using the Terminal to run R, rather than run it from RStudio.\nFor Windows users, you can probably use the Windows Command Prompt, but you might have to tweak your PATH variable, or explicitly specify the path to R to get it to run. See this Stack Overflow post for some potentially helpful info. (I’ll pray for you)."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#gdb-and-lldb",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#gdb-and-lldb",
    "title": "Debugging an R Package with C++",
    "section": "gdb and lldb",
    "text": "gdb and lldb\nThere are two main C++ debuggers out there, as far as I know. gdb works alongside the g++ compiler, and lldb works with the clang++ compiler. The one that you will use will depend on what compiler you use to compile the C/C++ code in your package with. If you aren’t sure what you are using, I’ll show you an easy way to find out which one you have later on. I compile with clang++, so I’ll be showing lldb.\nThere are a number of commands that will be useful. Don’t worry about understanding them all now, this will serve as a nice reference for you later on:\n\n\nrun (or process launch): Run R and drop me at the console.\n\nnext (or n): Run the next line of C++ code.\n\nstep (or s): Step into a function.\n\nup: Step up into the previous function call.\n\ndown: Step down into the next function call.\n\nfinish: Run the rest of the code.\n\nframe variable: Print the value of all of the variables in the current frame.\n\nexit: Exists lldb.\n\nbreakpoint set --name <function_name>: Set a breakpoint that will be triggered whenever that function is called.\n\nprocess continue: Jump back into an already running R process you started with run.\n\nIt’s also useful to know that when you are working at the R console in lldb, you can press Ctrl + C to exit the R process (without killing it) and jump back to the lldb console. Pressing CTRL + Z at any point will kill lldb and the attached R process.\nIf you use gdb, there is a nice command map from gdb <-> lldb that will be useful to you as you follow along.\nThe general workflow is going to look like:\n\nStart the debugger with R attached\nSet a breakpoint\n\nrun to start R\nRun some setup R code to activate breakpoints\nTrigger the error\nDebug!"
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#telling-prompts-apart",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#telling-prompts-apart",
    "title": "Debugging an R Package with C++",
    "section": "Telling prompts apart",
    "text": "Telling prompts apart\nIn my code blocks, I’ll use the following conventions to tell apart the terminal, lldb, and R consoles:\n\nTerminal: (term) <code>\n\nlldb: (lldb) <code>\n\nR: (R) <code>"
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#example-package",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#example-package",
    "title": "Debugging an R Package with C++",
    "section": "Example package",
    "text": "Example package\nEventually for these examples we will be using a mini package I created called debugit. It lives on github here. Hop into RStudio and install it with devtools:\n\ndevtools::install_github(\"DavisVaughan/debugit\")\n\nDownloading GitHub repo DavisVaughan/debugit@master\n✔  checking for file ‘<stuff>’ ...\n─  preparing ‘debugit’:\n✔  checking DESCRIPTION meta-information ...\n─  cleaning src\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘debugit_0.0.0.9000.tar.gz’\n   \n* installing *source* package ‘debugit’ ...\n** libs\nclang++  <stuff> -fPIC  -Wall -g -O2  -c RcppExports.cpp -o RcppExports.o\nclang++  <stuff> -fPIC  -Wall -g -O2  -c buggy.cpp -o buggy.o\nclang++  <stuff> -fPIC  -Wall -g -O2  -c stack.cpp -o stack.o\nclang++ -dynamiclib <stuff> -o debugit.so RcppExports.o buggy.o stack.o\ninstalling to /Library/Frameworks/R.framework/Versions/3.5/Resources/library/debugit/libs\n** R\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded\n* DONE (debugit)\nAdding ‘debugit_0.0.0.9000.tgz’ to the cache\nBecause I don’t hate you, I trimmed some of the output. Do you see the section starting with * installing *source* package ‘debugit’ ...? There are two important things I want you to notice here:\n\n\nclang++ is at the front of those 4 lines. That’s our compiler! So we should be using lldb.\nThese two “flags” -g -O2. These are important flags that can make you either very happy or very miserable as you try and debug. -g tells the compiler to “compile with debug information”. -O2 is the level of “optimization” that the compiler should use. O0 is the lowest, and O3 is the highest. With a lower level of optimization, more information is left lying around to help us debug. Higher levels of optimization can sometimes result in faster code. O2 is what R defaults to when setting flags for your code. Unfortunately, this will come back to haunt us: insert epic foreshadowing omen.\n\nNow that we know what debugger we should be using, let’s play around with attaching that debugger to R. We will come back to the package after we are more comfortable with lldb."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#starting-r-with-a-debugger",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#starting-r-with-a-debugger",
    "title": "Debugging an R Package with C++",
    "section": "Starting R with a debugger",
    "text": "Starting R with a debugger\nTo run R with a debugger, you’ll need to run it from the command line. Open up Terminal. First, just type R:\n(term) R\nR version 3.5.1 (2018-07-02) -- \"Feather Spray\"\nCopyright (C) 2018 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin15.6.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n[master]> \nYou can ignore the fact that my R prompt says [master]>. I’ve customized it using Gabor Csardi’s prompt package! I promise it is the normal R prompt.\nThis started R from Terminal, and let’s you work interactively from the R console. Let’s close it back up. Type q() then n and press Enter. You should end up back at the terminal prompt.\n(R) q()\nSave workspace image? [y/n/c]: n\nTo start R with a debugger, run:\n(term) R -d lldb\n(lldb) target create \"/Library/Frameworks/R.framework/Resources/bin/exec/R\"\nCurrent executable set to '/Library/Frameworks/R.framework/Resources/bin/exec/R' (x86_64).\n(lldb) \nWell that’s different! It looks like it dropped us into the lldb prompt, and is using R as the “executable”. This means that when we call run, it will run that executable, starting R. Let’s do that. Type run at the lldb prompt and hit enter.\n(lldb) run\nProcess 52472 launched: '/Library/Frameworks/R.framework/Resources/bin/exec/R' (x86_64)\n\nR version 3.5.1 (2018-07-02) -- \"Feather Spray\"\nCopyright (C) 2018 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin15.6.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n[master]> \nIf you get an error here rather than getting R to start, you might need clang4. Skip down to the clang4 Required section and then come back.\nSo this looks like what happened when we called R, but also has an extra line at the top about “Process launched”. This is now an R process that our debugger is “attached” to. Run the following R code at the R console:\n(R) x <- 1 + 1\nNow, rather than running q() to quit, let’s exit the process without quitting and jump back into our debugger. Press CTRL + C and you should see:\nProcess 53060 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP\n    frame #0: 0x00007fff5af7ae82 libsystem_kernel.dylib`__select + 10\nlibsystem_kernel.dylib`__select:\n->  0x7fff5af7ae82 <+10>: jae    0x7fff5af7ae8c            ; <+20>\n    0x7fff5af7ae84 <+12>: movq   %rax, %rdi\n    0x7fff5af7ae87 <+15>: jmp    0x7fff5af73e31            ; cerror\n    0x7fff5af7ae8c <+20>: retq   \nTarget 0: (R) stopped.\n(lldb) \nYou may or may not get all that unintelligible garbage about “libsystem_kernel.dylib__select:”. It doesn’t seem to hurt me thought so let’s continue. The main thing is that we got a Target 0: (R) stopped. and we are now back at the (lldb) prompt.\nIf you try and call run now you get this because you already have an R process running:\n(lldb) run\n\nThere is a running process, kill it and restart?: [Y/n]\nSo let’s jump back into that R process with process continue and the lldb prompt:\n(lldb) process continue\n\nProcess 53060 resuming\nI get this message…and then it kind of hangs. I don’t see the R console prompt. For some reason, you have to help it along. Press Enter if nothing shows up.\nProcess 53060 resuming\n\n[master]> \nThat’s better. Here we can run R commands again in that same process we started in. To prove that it is the same process, print x.\n(R) x\n\n[1] 2\nNow that you have a bit of the basics down, let’s shut down and start over. Press CTRL + C and then run:\n(lldb) exit\nYou should be back at the Terminal prompt."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#clang4",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#clang4",
    "title": "Debugging an R Package with C++",
    "section": "clang4",
    "text": "clang4\nOnly read this section if you couldn’t get the debugger to start R. Otherwise skip on to the next section.\nWhen trying to run the debugger with R -d lldb and then a call to run, at least on:\n\nR 3.5.1\nMacOS Mojave\nCompiling with clang\n\nI immediately hit something like:\n(lldb) run \n\nProcess 74239 launched: \n'/Library/Frameworks/R.framework/Resources/bin/exec/R' (x86_64) \ndyld: Library not loaded: /usr/local/clang4/lib/libomp.dylib \n  Referenced from: /Library/Frameworks/R.framework/Resources/bin/exec/R \n  Reason: image not found \nProcess 74239 stopped \n* thread #1, stop reason = signal SIGABRT \n    frame #0: 0x000000010002c9ee dyld`__abort_with_payload + 10 \ndyld`__abort_with_payload: \n->  0x10002c9ee <+10>: jae    0x10002c9f8               ; <+20> \n    0x10002c9f0 <+12>: movq   %rax, %rdi \n    0x10002c9f3 <+15>: jmp    0x10002c300               ; cerror_nocancel \n    0x10002c9f8 <+20>: retq \nI believe this was a bug that was fixed in December 2018. You can read about that here. It’s trying to tell you it can’t find /usr/local/clang4, even though clang6 is the recommended clang build nowadays. If you hit this, ensure that you don’t have clang4 by opening a Terminal window and running:\n(term) cd /usr/local\n(term) ls\nIf you see clang6 there but not clang4, you need to get clang4 to continue. Luckily the research group at AT&T has you covered. Go to this page to see the libraries they provide. One is clang4 (it’s in alphabetical order). At the bottom, they tell you how to install it. If the version you see for clang 4.00 is the same as the one in the code below, you can open up a Terminal window and run this, otherwise, tweak it a bit as needed:\n(term) curl -O http://r.research.att.com/libs/clang-4.0.0-darwin15.6-Release.tar.gz\n(term) sudo tar fvxz clang-4.0.0-darwin15.6-Release.tar.gz -C /\nI think I got an error of some kind from this, but it didn’t seem to affect anything in the end and ran fine. Check /usr/local again and look for clang4.\nNote that it specifies darwin15, and their key specifies that this means you need MacOS El Capitan or higher for this to work."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#the-package",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#the-package",
    "title": "Debugging an R Package with C++",
    "section": "The package",
    "text": "The package\nNow that we know how to use the debugger, let’s look at this package. Here is some real R code, run in RStudio and not at the command prompt:\n\nlibrary(debugit)\nlibrary(rlang)\n\n# What are the names of the functions in the package?\nnames(pkg_env(\"debugit\"))\n#> [1] \"add_one\"   \"buggy_fun\"\n\nThere are two functions here. add_one() takes a numeric input and supposedly adds 1 to it.\n\n# Or maybe not...\nadd_one(5)\n#> [1] 105\n\nbuggy_fun() is supposed to create an integer vector holding 0 and return it to you. Instead, it crashes R so you might not want to run it right away."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#debugging-buggy_fun---round-1",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#debugging-buggy_fun---round-1",
    "title": "Debugging an R Package with C++",
    "section": "Debugging buggy_fun() - Round 1",
    "text": "Debugging buggy_fun() - Round 1\nSo at this point, you’ve installed the package, and can use R with a debugger. Now it’s time to learn how to debug a crashing R session. Let’s demonstrate the problem. Start R from the command line:\n(term) R\nNow run:\n(R) debugit::buggy_fun()\n *** caught segfault ***\naddress 0x7f84fe68fd40, cause 'memory not mapped'\n\nTraceback:\n 1: buggy_fun_impl()\n 2: debugit::buggy_fun()\n\nPossible actions:\n1: abort (with core dump, if enabled)\n2: normal R exit\n3: exit R without saving workspace\n4: exit R saving workspace\nR crashes! We get a \"memory not mapped\" reason for the crash, and a traceback telling us that we called debugit::buggy_fun() and then the error happened in a function called buggy_fun_impl(). This is the C++ function that is causing the issues (technically this is the R function that Rcpp exposed the C++ function of the same name as, but either way you think about it is fine). It looks like this:\n\nbool buggy_fun_impl() {\n\n  NumericVector x(1);\n\n  int n = INT_MAX;\n\n  x[n] = 0;\n\n  return true;\n}\n\nIt creates x, an Rcpp numeric vector with length 1 (by default filled with the value 0). But then tries to assign 0 to a memory location at INT_MAX (a really big number). Since x doesn’t “own” that memory, we crash. But say we don’t know all that…\nHow do we debug this? Well, we at least know we should be looking into buggy_fun_impl(), so lets start there. What we need to do is set a breakpoint. This is a spot in the C++ code that we tell the debugger to stop at, so we can have a look around before everything implodes. You can do that in a few ways with lldb.\n# breakpoint on a specific line\nbreakpoint set --file <file.cpp> --line <line-number>\n\n# breakpoint on a object/function name\nbreakpoint set --name <function_name>\n\n# breakpoint for any errors that are thrown\nbreakpoint set -E c++\nThe last one is super useful when you have no idea where the error is happening, but usually you have a guess. Let’s try setting it on the name buggy_fun_impl. Back in Terminal…\n(term) R -d lldb\n\n(lldb) breakpoint set --name buggy_fun_impl\n\nBreakpoint 1: no locations (pending).\nWARNING:  Unable to resolve breakpoint to any actual locations.\nSo we set the breakpoint, but it didn’t actually find anything named \"buggy_fun_impl\", so it set the breakpoint to pending. This shouldn’t be too surprising, we haven’t started an R process yet (we haven’t run run), and more importantly we need to load the package that holds the buggy functions. We can confirm that the breakpoint exists with breakpoint list:\n(lldb) breakpoint list\n\nCurrent breakpoints:\n1: name = 'buggy_fun_impl', locations = 0 (pending)\nLet’s start our R session and library the package.\n(lldb) run\n\n(R) library(debugit)\n\n1 location added to breakpoint 1\nImmediately as we loaded the package the breakpoint was set! Great, now we just trigger the bug.\n(R) buggy_fun()\n\nProcess 57494 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001087c45e0 debugit.so`buggy_fun_impl()\ndebugit.so`buggy_fun_impl:\n->  0x1087c45e0 <+0>: pushq  %rbp\n    0x1087c45e1 <+1>: movq   %rsp, %rbp\n    0x1087c45e4 <+4>: pushq  %r15\n    0x1087c45e6 <+6>: pushq  %r14\nTarget 0: (R) stopped.\nUh? Okay well it didn’t crash. And it seems to be pointing us in the right direction:\n\n\nstop reason = breakpoint 1.1 says that it stopped because it hit the breakpoint we requested\n\ndebugit.so`buggy_fun_impl() is telling us it stopped at the function we are interested in\n\nBut I promised you line by line debugging power! What is this garbage? Here’s the thing. I don’t know why, but I can’t seem to effectively debug packages that I installed using install.packages() or install_github(). The information is just not there. Instead, you need to have the package locally on your computer (like you are a developer working on it), and you need to use devtools::load_all() rather than library() to load it.\nWhile this may seem frustrating, this is the probable state that you will be in when you are debugging. You’ll be the maintainer of the package, so you will have it locally and will be used to the load_all() workflow.\nTo get out of this, press CTRL + Z to kill lldb."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#debugging-buggy_fun---round-2",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#debugging-buggy_fun---round-2",
    "title": "Debugging an R Package with C++",
    "section": "Debugging buggy_fun() - Round 2",
    "text": "Debugging buggy_fun() - Round 2\nBy whatever means necessary, get the files for the debugit package locally on your computer. I think the easiest way is:\n\n# you may have to set `protocol = \"https\"` as well depending on how you have\n# git set up\nusethis::create_from_github(\n  \"DavisVaughan/debugit\", \n  destdir = \"~/path/to/destination\"\n)\n\nYou can also do a standard Fork + Clone github workflow. Or you can download the zip file if you are desparate. Here is the link to the zip.\nI’m going to assume you now have it locally. Jump back in Terminal, and change to the directory where you placed the package. It is important that you start R from here!\n(term) cd ~/path/to/debugit\nYou know you are in the right place if you see this:\n(term) ls\n\nDESCRIPTION LICENSE     LICENSE.md  NAMESPACE   R       debugit.Rproj   man     src\nStart the debugger, set a breakpoint, and jump back into R.\n(term) R -d lldb\n\n(lldb) breakpoint set --name buggy_fun_impl\n(lldb) run\nNow, run a devtools::load_all(). Because you are in the right working directory, it will automatically find the debugit package and install it. I see:\n(R) devtools::load_all()\n\nLoading debugit\nRe-compiling debugit\n─  installing *source* package ‘debugit’ ...\n   ** libs\n   clang++  <stuff> -fPIC  -Wall -g -O2  -c RcppExports.cpp -o RcppExports.o\n   clang++  <stuff> -fPIC  -Wall -g -O2  -c buggy.cpp -o buggy.o\n   clang++  <stuff> -fPIC  -Wall -g -O2  -c stack.cpp -o stack.o\n   clang++ -dynamiclib <stuff> -o debugit.so RcppExports.o buggy.o stack.o\n   installing to /private/var/folders/41/qx_9ygp112nfysdfgxcssgwc0000gn/T/Rtmpr8QYTT/devtools_install_e1c251df2215/debugit/libs\n─  DONE (debugit)\n1 location added to breakpoint 1\nIt recompiled the package, and then the breakpoint was set! Now trigger the bug.\n(R) buggy_fun()\n\nProcess 57912 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001085ecbe4 debugit.so`buggy_fun_impl() at buggy.cpp:7\n   4    // [[Rcpp::export()]]\n   5    bool buggy_fun_impl() {\n   6    \n-> 7      NumericVector x(1);\n   8    \n   9      int n = INT_MAX;\n   10   \nTarget 0: (R) stopped.\n(lldb)\nWoah! Now it stopped right where we wanted it to. Just inside the buggy_fun_impl() function. What can we do with this?\nType next and hit enter to run the current line, this moves us to line 9:\n(lldb) next\n\nProcess 57912 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = step over\n    frame #0: 0x00000001085ecc03 debugit.so`buggy_fun_impl() at buggy.cpp:9\n   6    \n   7      NumericVector x(1);\n   8    \n-> 9      int n = INT_MAX;\n   10   \n   11     x[n] = 0;\n   12   \nTarget 0: (R) stopped.\nView the available variables with frame variable. We see x, which is a NumericVector with a more complicated structure, and n which is an int. Looks like INT_MAX = 2147483647.\n(lldb) frame variable\n\n(Rcpp::NumericVector) x = {\n  Rcpp::PreserveStorage<Rcpp::Vector<14, Rcpp::PreserveStorage> > = (data = 0x00000001095c2b08)\n  cache = {\n    start = 0x00000001095c2b38\n  }\n}\n(int) n = 2147483647\nWe can even run arbitrary C++ code with expr\n(lldb) expr 1 + 1\n\n(int) $2 = 2\nIf you want to store the result, use the special syntax of $var_name rather than just var_name.\n(lldb) expr int $var = 1 + 1\n(lldb) expr $var\n\n(int) $var = 2\nHere’s a neat trick, what if I want to print out the value of x? Normally I’d use Rcpp::Rcout << x << std::endl, but that doesn’t work. We have to call a function from the R API, Rf_PrintValue(), on the underlying SEXP that x stores. Normally I’d get at that with SEXP(x), but that doesn’t work either. We really have to be creative. If you look at what printed out for x earlier, you’ll see a data member. That’s the SEXP, and we can call Rf_PrintValue() on that.\n(lldb) expr Rf_PrintValue(x.data)\n\n[1] 0\nNice! Now let’s continue until we hit the bug:\n(lldb) next\n\nProcess 57912 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x5095c2b30)\n    frame #0: 0x00000001085ecc2b debugit.so`buggy_fun_impl() at buggy.cpp:11\n   8    \n   9      int n = INT_MAX;\n   10   \n-> 11     x[n] = 0;\n   12   \n   13     return true;\n   14   }\nTarget 0: (R) stopped.\nAh, looks like that did it. See the stop reason = EXC_BAD_ACCESS? That’s our error saying we are “badly accessing” a location in memory. Importantly, we now know exactly where the problem is. And we have the power to print x and n and see that we are assigning to a location much larger than the size of x. So, with that, we can fix our problem. Press CTRL + Z to exit."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#break-on-any-errors",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#break-on-any-errors",
    "title": "Debugging an R Package with C++",
    "section": "Break on any errors",
    "text": "Break on any errors\nJust for kicks and giggles, lets try setting the breakpoint a different way. This way says to break any time we hit an error.\n(term) R -d lldb\n\n(lldb) breakpoint set -E c++\n(lldb) run\n\n(R) devtools::load_all()\n(R) buggy_fun()\n\nProcess 58089 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x5012cc530)\n    frame #0: 0x000000010a327c2b debugit.so`buggy_fun_impl() at buggy.cpp:11\n   8    \n   9      int n = INT_MAX;\n   10   \n-> 11     x[n] = 0;\n   12   \n   13     return true;\n   14   }\nTarget 0: (R) stopped.\nThis immediately takes us to the problem line, where we can now look around like before using expr and frame variable."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#debugging-add_one---round-1",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#debugging-add_one---round-1",
    "title": "Debugging an R Package with C++",
    "section": "Debugging add_one() - Round 1",
    "text": "Debugging add_one() - Round 1\nNow let’s try a different problem. add_one() doesn’t error, but clearly gives the wrong results. We expect the result to be 6.\n\ndebugit::add_one(5)\n#> [1] 105\n\nNow, generally I’d try and use some print statements to figure out WTF is happening here. That’s the quick way to do this and would probably work fine.\nBut let’s say you have no idea what is happening, but you think something is going on in the underlying add_one_impl() C++ function that powers add_one(). That looks like this:\n\nNumericVector add_one_impl(NumericVector x) {\n\n  NumericVector y = get_one();\n\n  NumericVector result = x + y;\n\n  return result;\n}\n\nLet’s use the same tactic as before to set a breakpoint on add_one_impl.\n(term) R -d lldb\n\n(lldb) breakpoint set --name add_one_impl\n(lldb) run\n\n(R) devtools::load_all()\n(R) add_one(5)\n\ndebugit.so was compiled with optimization - stepping may behave oddly; variables may not be available.\nProcess 58425 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001089d2693 debugit.so`add_one_impl(Rcpp::Vector<14, Rcpp::PreserveStorage>) [inlined] get_one() at stack.cpp:6 [opt]\n   3    \n   4    NumericVector get_one() {\n   5    \n-> 6      NumericVector one(1, 1.0);\n   7    \n   8      // Not 1!\n   9      one[0] = 100;\nTarget 0: (R) stopped.\nAgh, what? That’s not right, somehow we ended up in the get_one() function instead. But wait, what is that first line at the top:\ndebugit.so was compiled with optimization - stepping may behave oddly; variables may not be available.\nAh. Remember that bit at the beginning where I mentioned the “flags”? It is coming back to haunt us. R compiled this code with O2, but that stripped out some of the debugging info, so our debugger stopped in the wrong place. We need to recompile with O0. But how do we do that? We have to set O0 as one of our CXXFLAGS in a Makevars file. That sounds ridiculous but it isn’t too bad thanks to usethis.\nOpen RStudio. Run:\n\nusethis::edit_r_makevars()\n\nThis should open a file located at ~/.R/Makevars. Be careful here! This gets run whenever you install any packages with code that needs to be compiled. Add the following lines:\nCXXFLAGS = -g -O0\nSave and make sure you add a new blank line after the CXXFLAGS line. Now close out of RStudio again."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#debugging-add_one---round-2",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#debugging-add_one---round-2",
    "title": "Debugging an R Package with C++",
    "section": "Debugging add_one() - Round 2",
    "text": "Debugging add_one() - Round 2\nLet’s try this again:\n(term) R -d lldb\n\n(lldb) breakpoint set --name add_one_impl\n(lldb) run\nAt this point, if we run devtools::load_all() it actually won’t do anything, because we already compiled the code once and none of the code actually changed. We really need to force it to compile again by clearing out the old compiled code. You can do that with:\n(R) devtools::clean_dll()\nIt will look like nothing happens, but if you run a devtools::load_all() it should compile:\n(R) devtools::load_all()\n\nLoading debugit\nRe-compiling debugit\n─  installing *source* package ‘debugit’ ...\n   ** libs\n   clang++  <stuff> -g -O0 -c RcppExports.cpp -o RcppExports.o\n   clang++  <stuff> -g -O0 -c buggy.cpp -o buggy.o\n   clang++  <stuff> -g -O0 -c stack.cpp -o stack.o\n   clang++  <stuff> -o debugit.so RcppExports.o buggy.o stack.o\n   installing to <stuff>\n─  DONE (debugit)\n1 location added to breakpoint 1\nLook! Do you see the -g -O0 you set? If so, you should be good to go.\n(R) add_one(5)\n\nProcess 58576 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x000000010b865ebf debugit.so`add_one_impl(x=Rcpp::NumericVector @ 0x00007ffeefbfd198) at stack.cpp:17\n   14   // [[Rcpp::export()]]\n   15   NumericVector add_one_impl(NumericVector x) {\n   16   \n-> 17     NumericVector y = get_one();\n   18   \n   19     NumericVector result = x + y;\n   20   \nTarget 0: (R) stopped.\nWoop! We are now exactly where we wanted, and we don’t get any of those annoying warnings about out package being compiled with optimization. Run next to have the get_one() line run.\n(lldb) next\n\nProcess 58576 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = step over\n    frame #0: 0x000000010b865ed7 debugit.so`add_one_impl(x=Rcpp::NumericVector @ 0x00007ffeefbfd198) at stack.cpp:19\n   16   \n   17     NumericVector y = get_one();\n   18   \n-> 19     NumericVector result = x + y;\n   20   \n   21     return result;\n   22   }\nTarget 0: (R) stopped.\nWhat does y look like?\n(lldb) expr Rf_PrintValue(y.data)\n\n[1] 100\nThat seems wrong. This should just be 1. What is happening in get_one()? Let’s run finish to run the rest of the lines and try again:\n(lldb) finish\nRun process continue to dump us back into the R session so we can try again:\n(lldb) process continue\n\nProcess 58576 resuming\n[1] 105\n[master]>\nOh look, there’s the result of that call we debugged. Let’s go back into the debugger by calling it again.\n(R) add_one(5)\n\nProcess 58576 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x000000010b865ebf debugit.so`add_one_impl(x=Rcpp::NumericVector @ 0x00007ffeefbfd198) at stack.cpp:17\n   14   // [[Rcpp::export()]]\n   15   NumericVector add_one_impl(NumericVector x) {\n   16   \n-> 17     NumericVector y = get_one();\n   18   \n   19     NumericVector result = x + y;\n   20   \nTarget 0: (R) stopped.\nNow since we know get_one() seems to be the issue, we can step into the function with step:\n(lldb) step\n\nProcess 58576 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = step in\n    frame #0: 0x000000010b865dcb debugit.so`get_one() at stack.cpp:6\n   3    \n   4    NumericVector get_one() {\n   5    \n-> 6      NumericVector one(1, 1.0);\n   7    \n   8      // Not 1!\n   9      one[0] = 100;\nTarget 0: (R) stopped.\nOkay, we are inside get_one(). Let’s run this line creating one and take a look at it.\n(lldb) next\n\nProcess 58576 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = step over\n    frame #0: 0x000000010b865dff debugit.so`get_one() at stack.cpp:9\n   6      NumericVector one(1, 1.0);\n   7    \n   8      // Not 1!\n-> 9      one[0] = 100;\n   10   \n   11     return one;\n   12   }\nTarget 0: (R) stopped.\nThings look okay now…\n(lldb) expr Rf_PrintValue(one.data)\n\n[1] 1\nBut then you run the next line…\n(lldb) next\n\nProcess 58576 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = step over\n    frame #0: 0x000000010b865e21 debugit.so`get_one() at stack.cpp:11\n   8      // Not 1!\n   9      one[0] = 100;\n   10   \n-> 11     return one;\n   12   }\n   13   \n   14   // [[Rcpp::export()]]\nTarget 0: (R) stopped.\nAnd as I am sure you can guess by now, you see that one now holds 100 because of the assignment we did there on line 9!\n(lldb) expr Rf_PrintValue(one.data)\n\n[1] 100\nNow we know where the problem is, so we can head back into our local copy of the package, fix the issue, and try again. At this point, CTRL + Z to quit.\nDon’t forget to go comment out or delete that line in the ~/.R/Makevars file!"
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#wrapping-up",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#wrapping-up",
    "title": "Debugging an R Package with C++",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis has been a very long winded post. But hopefully it can serve as a reference that anyone can look back on and use to understand how to debug compiled code in an R package. I think the main points are:\n\nUse the workflow:\n\nR -d lldb\nSet a breakpoint\n\nrun to start R\n\ndevtools::load_all() to activate breakpoint\nTrigger bug\nDebug!\n\n\nRemember to use -g -O0 when compiling.\nRemember to use Rf_PrintValue() on SEXP objects to get a pretty view of what the R object actually looks like, and Rf_PrintValue(x.data) to print Rcpp objects."
  },
  {
    "objectID": "posts/2019-04-05-debug-r-package-with-cpp/index.html#resources",
    "href": "posts/2019-04-05-debug-r-package-with-cpp/index.html#resources",
    "title": "Debugging an R Package with C++",
    "section": "Resources",
    "text": "Resources\nHere are some extra resources I found really useful as I was figuring all this out:\n\nThe R Packages section on debugging compiled code.\nSection 4.4.2 Inspecting R objects when debugging from R Core’s Writing R Extensions (bookdown-ified by Colin Fay) is quite useful. It is where I learned about Rf_PrintValue(). See also R_inspect(), and Rf_PrintValue(x->attrib) to view attributes of a SEXP.\nKevin Ushey has a great blog post with some more pointers on using lldb with Rcpp functions created on the fly."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html",
    "href": "posts/2019-03-02-now-you-c-me/index.html",
    "title": "Now You C Me",
    "section": "",
    "text": "This post is designed to help you get up and running with an R package that uses C code, teaching the absolute minimum required to get going. Over the past few days, I learned a lot about working with packages that call C. It’s not quite as pleasant as working with C++ and Rcpp, but as you start to figure it out, it really feels like a superpower. Superpower or not, as Jim Hester says, if you have the choice it is much easier to use Rcpp, and my advice would also be to start there. I created this guide because I was contributing to a project that used C, and was frustrated by the lack of resources for beginners.\nThat said, it’s a steep learning curve. Yes, Writing R Extensions has much of what you need to know, but I’m not big on the whole RTFM idea, and I think posts that actually get you up and running so you can start exploring on your own are valuable."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#package",
    "href": "posts/2019-03-02-now-you-c-me/index.html#package",
    "title": "Now You C Me",
    "section": "Package",
    "text": "Package\nSo let’s go. I’m going to create a C function that takes one numeric argument of length 1, and adds 1 to it. Then, I’ll show how to call it from R so users of the package can actually interface with it. Exciting.\nWe need the structure for a package, so let’s use some usethis helpers to get started. We’ll call the package addr, and you should replace path with the path to the location you want the package created at.\n\nusethis::create_package(path = \"~/path/to/addr\")\n\nIf you work with RStudio, this should open a new RStudio instance with the addr package opened up. Switch over to that."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#optional-git",
    "href": "posts/2019-03-02-now-you-c-me/index.html#optional-git",
    "title": "Now You C Me",
    "section": "Optional git",
    "text": "Optional git\nIf you are so inclined and have everything hooked up (you’ve used git before on your computer), you can set up git and GitHub for this repo. I’ll do it so you can see the final product, along with all of the commits along the way. You can find my end result here.\n\n# This will restart RStudio\nusethis::use_git()\n\n# Then call this to use github\n# for this package\nusethis::use_github()"
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#roxygen2",
    "href": "posts/2019-03-02-now-you-c-me/index.html#roxygen2",
    "title": "Now You C Me",
    "section": "roxygen2",
    "text": "roxygen2\nSome of the usethis functions we are going to call require roxygen2 to be used for creating documentation (and it makes our lives a heck of a lot easier). Let’s set that up next. The easiest way is to just call:\n\ndevtools::document()\n\nAlternatively, on a Mac I can call CMD+Shift+D. This should just add some information about roxygen2 to the DESCRIPTION file.\nThe other thing we will want to do is create a “package doc” .R file. Other usethis functions called later will use this file to store useful information automatically. Basically, this is just a .R file named addr-package.R.\n\nusethis::use_package_doc()\n\nThe contents of this look like:\n\n#' @keywords internal\n\"_PACKAGE\"\n\n# The following block is used by usethis to automatically manage\n# roxygen namespace tags. Modify with care!\n## usethis namespace: start\n## usethis namespace: end\nNULL\n\nThe usethis information is going to be inserted between the usethis namespace: start and usethis namespace: end lines as we go along."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#can-you-c",
    "href": "posts/2019-03-02-now-you-c-me/index.html#can-you-c",
    "title": "Now You C Me",
    "section": "Can you C?",
    "text": "Can you C?\nLet’s work on our first C function. Assuming you are in RStudio and in the current addr project, let’s create our first C file. There’s a usethis helper for that too.\n\nusethis::use_c(\"add\")\n\nWow. That did a lot more than just create a C file. I get something like this:\n\n#> ✔ Creating 'src/'\n#> ✔ Adding '*.o', '*.so', '*.dll' to 'src/.gitignore'\n#> ✔ Adding '@useDynLib addr, .registration = TRUE' to 'R/addr-package.R'\n#> ● Run `devtools::document()` to update 'NAMESPACE'\n#> ✔ Writing 'src/add.c'\n#> ● Modify 'src/add.c'\n\nSo what happened here?\n\nA new directory was created, src/. This is where all of your C files go.\nMultiple types of files were added to a .gitignore in src/. This is very helpful, as these 3 types of files, .o, .so and .dll are ones that would be created by our package when the C code is compiled, or “built” if you are familiar with R packages. In the same way that we wouldn’t commit a built R package to github, we don’t commit these files either.\nSome information was added to addr-package.R, and we get a suggestion to document().\nA new file, src/add.c was added, and then it opened in RStudio. Great! We will work on this file in a moment.\n\nWhat’s this information that got added to addr-package.R? If we open up addr-package.R right now, we will see:\n\n## usethis namespace: start\n#' @useDynLib addr, .registration = TRUE\n## usethis namespace: end\nNULL\n\nThe second line here is a roxygen comment added by use_c(), and it is an important one! Basically, it is the way we eventually tell R that we should be looking for any C routines (functions) that we have “registered” (i.e. exposed to the R side), so we can actually call them from R.\nReally, this information needs to be in the package NAMESPACE file, which contains the information on what functions are imported to and exported from your package, along with information about external code like this. That’s why a suggestion to document popped up, as roxygen2 will take care of that for you:\n\ndevtools::document()\n\nIf you got an error while documenting that looked like getDLLRegisteredRoutines.DLLInfo(), don’t be alarmed. We just don’t have any C code for it to load.\nCheck out the NAMESPACE file and you should see:\n\nuseDynLib(addr, .registration = TRUE)"
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#addr_add_one",
    "href": "posts/2019-03-02-now-you-c-me/index.html#addr_add_one",
    "title": "Now You C Me",
    "section": "addr_add_one()",
    "text": "addr_add_one()\nNow we are ready to work on our C function. Open up that add.c file if it isn’t open already. You should see this at the top:\n\n#define R_NO_REMAP\n#include <R.h>\n#include <Rinternals.h>\n\nThe second two #include lines give us access to the C-level R API. If you aren’t familiar with includes or header files, for now think of them as calling library() on a package to get access to its functions for your own use.\nThe first line, #define R_NO_REMAP, purposefully comes before the other two, and prevents a “re-mapping” of the API functions from a standardized name of Rf_<fn> to just <fn>. I think it’s pretty good practice to prevent this remapping, as it makes it clear to us what functions are from the R API (and it works really well for finding new functions with auto-complete!).\nOKAY, now let’s write some C code. Go ahead and add these lines to the file:\n\n#define R_NO_REMAP\n#include <R.h>\n#include <Rinternals.h>\n\nSEXP addr_add_one(SEXP a) {\n\n  SEXP out = PROTECT(Rf_allocVector(REALSXP, 1));\n\n  REAL(out)[0] = Rf_asReal(a) + 1;\n\n  UNPROTECT(1);\n\n  return out;\n}\n\n…okay. This looks foreign. So many questions, I know, I had them too. I’ll tackle as many as I can think of. First, a broad overview of what this code does:\n\nTake in a single argument, a.\nCreate an object that we will assign the result to, out. This is a numeric vector of length 1. We also protect that object from the garbage collector.\nAdd 1 to a and assign it to out.\nUnprotect our result, out, as we are about to return it.\nReturn the result.\n\nWhat is a SEXP?\nGood question. It’s called an S-Expression, don’t ask me how to pronounce the shorthand notation, and I think it originated in the functional language, Lisp.\nJust think of a SEXP as a container that is able to represent any kind of R object, but at the C level. This means that an R list can be a SEXP, so can a matrix, so can a single logical value.\nWhy are there so many explicit types?\nMeaning, why is the argument SEXP a and not just a? Why is the return value, out, created as SEXP out = not just out =?\nC is a statically typed language, unlike R. Whereas in R we have the flexibility to create variables of any type without specifying the type in advance, in C we have to specify the type of every single variable ahead of time. This comes with the benefit of speed, and is one of the reasons C is so much faster than R.\nAllocation and protection\nIn C, creating new R objects is a bit more complicated than what you’d do at the R level. We know that we are going to take in a numeric vector of length 1, a, and we want to add 1 to it and return that result, which is also a numeric vector of length 1. So we need a place to put that result.\nTo do so, we have to create a new numeric vector of length 1 at the C level. The easiest way to do this is using a function from the R API, Rf_allocVector(). It takes in two arguments, a SEXPTYPE (the type of the SEXP to make) and a R_xlen_t (the length of the SEXP) and returns a SEXP of the type and length requested.\nThere are 7 SEXPTYPEs you can use, and you can find information about them at Hadley’s R Internals documentation repo. We used the type for a “real” (numeric) vector, REALSXP. We also specified the length to be 1.\nGreat, so we used Rf_allocVector(REALSXP, 1) to create a numeric vector of length 1. All good? Not quite. We also have to protect that numeric vector. From what? Well, if we don’t protect it by wrapping it in PROTECT() immediately, then when R’s garbage collector runs (which I think of as happening randomly), then that object gets cleaned up and removed 😢. If not managed correctly, this can cause great heartache.\nAs if it wasn’t difficult enough, we also have to manage unprotecting all of these objects. We do that here by calling UNPROTECT(1) at the end of the function, right before we return our result. UNPROTECT() takes an integer value of the number of things to unprotect, so if you had created 3 new objects, you could call UNPROTECT(3) instead. We don’t have to protect and unprotect the arguments to the function, as R knows to protect these automatically.\nAutocomplete ❤️\nYou might be wondering, “How did he know about Rf_allocVector()?” Or, “How do I find more of these neat C level API functions?” Great question! Luckily, RStudio’s autocomplete has your back. By typing Rf_ and pressing tab, we get the following pop up that we can scroll through.\n\nUsing this, we can find new functions to research google and learn about.\nSimilarly, how did I know what type Rf_allocVector() took and returned? Well…\n\nThe popup we get from RStudio tells us not only the name of the C function, but also the types of the arguments and return value!\nIn a moment, we will use Rf_asReal().\n\nAddition\nOkay, so about the actual addition line…\n\nREAL(out)[0] = Rf_asReal(a) + 1;\n\nThat seems like a lot of work to add 1.\nRf_asReal() takes a SEXP, and returns a single double value corresponding to the first element in the SEXP. A double is the C type that is somewhat equivalent to an R numeric. So this converts our numeric vector of length 1, a, into an object that can be manipulated at the C level with C operations like addition.\nNext, we add 1 to that double that Rf_asReal() gives us. After that, we have to put it somewhere. If you thought we could just do out = Rf_asReal(a) + 1, well, haven’t you learned by now that it isn’t ever that simple?\nout is a SEXP, and we can’t assign a double straight to a SEXP. What we actually need is a way to access the double underlying the out SEXP we created. That is what REAL(out) gets us. Technically, it gives us a double* (a double pointer) that points to the actual double array that out abstracts away into an R numeric vector. Jargon aside, it gives us something that we can kind of treat like an R vector, where we can index into it with [] and assign values to those slots.\nOne more note is that C is 0-index based, while R is 1-index based, so rather than doing REAL(out)[1] to access the first position, we really do REAL(out)[0].\nAlright, so REAL(out) gave us access to the double*, and REAL(out)[0] gave us access to the actual double at the 1st position in the vector. This is a double, so we can assign our result to this.\nReturn\nFinally, after calling UNPROTECT(1) as described above, we return out with return out. Unlike R, you actually have to specify the return in C (it’s generally optional in R).\nNote that in the function signature, we specified SEXP addr_add_one(...). The SEXP at the beginning there was our way of telling C that we are going to be returning a SEXP object."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#registration",
    "href": "posts/2019-03-02-now-you-c-me/index.html#registration",
    "title": "Now You C Me",
    "section": "Registration",
    "text": "Registration\nUpdate) I have since learned that this registration section can be generated automatically! I highly recommend still reading this post in order to understand what the registration piece does, but check out the Automatic Registration section at the end of the post to learn how to do it automatically. Thanks, Hadley & Jim and Metin.\nGreat, so we have a C function that can add 1. Can we call this from R yet? Almost, but still no. If you run a devtools::load_all(), then you should see the code compile. For me that looks like this, but I’ve manually trimmed it so you aren’t overloaded with output:\n\n#> Loading addr\n#> Re-compiling addr\n#> ─  installing *source* package ‘addr’ ...\n#>    ** libs\n#>    clang <trimmed>  -c add.c -o add.o\n#>    clang -dynamiclib <trimmed> -o addr.so add.o\n#>    installing to /private/var/<trimmed>/addr/libs\n#> ─  DONE (addr)\n\nThis is telling us that the:\n\n\nadd.c was compiled into an “object” file, add.o.\nThat one object file was used to make an addr.so “shared object”. You can maybe think of this kind of like a “built” R package.\n\nNevertheless, if you try and do addr:: after running load_all(), you will be disappointed, and you won’t see addr_add_one anywhere! This is because we still have to register the routine, aka expose these functions to the R side. This is related to that @useDynLib roxygen2 tag we added at the beginning. That tells R to look for the C functions, but we still have to actually expose them too. Let’s do that.\nCreate a new file, init.c. This is the standard name for the file where this “routine registration” happens:\n\nusethis::use_c(\"init\")\n\nAdd the following to what is already there:\n\n#define R_NO_REMAP\n#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\n/* .Call calls */\nextern SEXP addr_add_one(SEXP);\n\nstatic const R_CallMethodDef CallEntries[] = {\n  {\"addr_add_one\", (DL_FUNC) &addr_add_one, 1},\n  {NULL, NULL, 0}\n};\n\nvoid R_init_addr(DllInfo *dll) {\n  R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n  R_useDynamicSymbols(dll, FALSE);\n}\n\nThis is the how C level functions are registered as something that can be called from the R side. Ugly, right? I won’t go into much detail here (I don’t know everything about it myself anyways), and you’ll generally only ever change 3 things. First, take a look at the section starting with “.Call calls”.\n\n/* .Call calls */\nextern SEXP addr_add_one(SEXP);\n\nHere, we have to list all of our C level functions that we want to expose to the R side, along with their full function signature (the argument types and return type), and we have to prefix it with extern. Our function takes 1 SEXP, a, as an argument, so we only have 1 SEXP specified here. If your function has 2 arguments, you would do extern SEXP my_fun(SEXP, SEXP). Generally the only things you will pass back and forth between R and C will be SEXP objects. If you have another function to export, you just add another extern call below this one.\nNext, we have to construct an “array of call method definitions”, named CallEntries.\n\nstatic const R_CallMethodDef CallEntries[] = {\n  {\"addr_add_one\", (DL_FUNC) &addr_add_one, 1},\n  {NULL, NULL, 0}\n};\n\nIn this, you’ll specify the address of the function you specified above (that’s what &addr_add_one does), and from what I understand you convert it into a dynamically loadable function. Basically, you’ll add one line per function you are exporting, and it is of the form:\n\n{\"<function-name>\", (DL_FUNC) &<function-name>, num_args}\n\nIf you have a second function to export, add another line after the first one, but before the {NULL, NULL, 0}.\nLastly, there is a function that R will automatically call for you that actually does the registration of these functions. R looks for a C function called R_init_<pkg>() to call to register these routines. So the only thing you’d ever change here is to change <pkg> to your current package name.\n\nvoid R_init_addr(DllInfo *dll) {\n  R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n  R_useDynamicSymbols(dll, FALSE);\n}\n\nNote how we pass the CallEntries into R_registerRoutines(). This seems to be passing along all of the information required about how to create the entry points into the C code that we will call from the R side."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#call-it",
    "href": "posts/2019-03-02-now-you-c-me/index.html#call-it",
    "title": "Now You C Me",
    "section": ".Call() it!",
    "text": ".Call() it!\nTry running devtools::load_all() one more time. You should now have access to addr::addr_add_one. It’s not a function, so don’t try and call it with addr_add_one(). Let’s print it out.\n\naddr::addr_add_one\n#> $name\n#> [1] \"addr_add_one\"\n#> \n#> $address\n#> <pointer: 0x7fea6b4da160>\n#> attr(,\"class\")\n#> [1] \"RegisteredNativeSymbol\"\n#> \n#> $dll\n#> DLL name: addr\n#> Filename: /Users/davis/Desktop/r/projects/data-insights-package/addr/src/addr.so\n#> Dynamic lookup: FALSE\n#> \n#> $numParameters\n#> [1] 1\n#> \n#> attr(,\"class\")\n#> [1] \"CallRoutine\"      \"NativeSymbolInfo\"\n\nSo this is really just a list of class \"CallRoutine\" holding information about where to find the actual C function we need.\nSince you can’t call this like a function, how do you use it? The magic is with the function .Call(), which serves as the function that let’s us call this addr_add_one entry point along with any arguments that we might need to pass through. Try the following:\n\n.Call(addr_add_one, 2)\n#> [1] 3\n\nWoah! So that just called our C function to add 1 to 2, so we get 3."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#now-what",
    "href": "posts/2019-03-02-now-you-c-me/index.html#now-what",
    "title": "Now You C Me",
    "section": "Now what?",
    "text": "Now what?\nWell, we need a better way to expose this to our users. The best way is to create a function that wraps this that we can export and document. Also, we did no error checking at the C level, so if we pass any bad or unexpected inputs in to this .Call(), it could actually crash R completely. Not just error, crash. This isn’t the worst thing in the world, and it’s a pretty normal part of the development process of connecting R and C (I do it all the time), but it isn’t fun for the user. To fix that, we will also add some error checking to our function.\nCreate a file named add.R.\n\nusethis::use_r(\"add\")\n\nNow add the following:\n\n#' Add 1 to a single numeric\n#'\n#' `add_one()` adds 1 to a single numeric value.\n#'\n#' @param a A single numeric value.\n#'\n#' @examples\n#'\n#' add_one(2)\n#'\n#' @export\nadd_one <- function(a) {\n\n  ok <- is.numeric(a) & length(a) == 1L\n\n  if (!ok) {\n    stop(\"`a` must be a single numeric value.\", call. = FALSE)\n  }\n\n  .Call(addr_add_one, a)\n}\n\nAt this point, call load_all() again and you should have access to add_one().\n\nadd_one(2)\n#> [1] 3\n\n\nadd_one(c(1, 2))\n#> Error: `a` must be a single numeric value.\n\nAwesome! Since we have added the @export tag, to actually export this function we just need to call:\n\ndevtools::document()\n\nwhich will create a .Rd help page for the file, and will add export(add_one) to the NAMESPACE file."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#cran-check",
    "href": "posts/2019-03-02-now-you-c-me/index.html#cran-check",
    "title": "Now You C Me",
    "section": "CRAN Check!",
    "text": "CRAN Check!\nThe only thing left is to see if it passes a cran check! Plot twist, it won’t quite yet. We need to add a license first.\n\nusethis::use_mit_license(name = \"Davis Vaughan\")\n\nOkay, now try it:\n\ndevtools::check()\n\n#> 0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\n😎"
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#resources",
    "href": "posts/2019-03-02-now-you-c-me/index.html#resources",
    "title": "Now You C Me",
    "section": "Resources",
    "text": "Resources\nIf you want to learn more about R’s C interface, there are a few resources out there for you!\n\nHadley’s R Internals documentation\nAdvanced R’s old section on C\nThe massive but thorough Writing R Extensions. I would focus on section 5.9 on Handling R objects in C.\nThe full addr package is on GitHub. The commit history attempts to follow this post."
  },
  {
    "objectID": "posts/2019-03-02-now-you-c-me/index.html#automatic-registration",
    "href": "posts/2019-03-02-now-you-c-me/index.html#automatic-registration",
    "title": "Now You C Me",
    "section": "Automatic Registration",
    "text": "Automatic Registration\nAs mentioned in the Registration section, you don’t actually have to create the init.c file “by hand,” which is great because it’s the thing I forget to do most. I’ve left this section until the end, rather than replacing the current Registration section, because I think the order of how you link things up makes more sense when done the manual way (expose to R with the init.c file, then .Call() it). So if you saw that note in Registration and instantly skipped to here, I’d advice going back and reading the rest of that section and the rest of the blog post first. As you get more familiar with working in C, you can use the methods described here.\nTo work with automatic registration, follow the blog post like usual, but skip the Registration and .Call it! sections and go straight to the Now What? section. Generate the add.R file that looks like this:\n\n#' Add 1 to a single numeric\n#'\n#' `add_one()` adds 1 to a single numeric value.\n#'\n#' @param a A single numeric value.\n#'\n#' @examples\n#'\n#' add_one(2)\n#'\n#' @export\nadd_one <- function(a) {\n\n  ok <- is.numeric(a) & length(a) == 1L\n\n  if (!ok) {\n    stop(\"`a` must be a single numeric value.\", call. = FALSE)\n  }\n\n  .Call(addr_add_one, a)\n}\n\nAt this point you should have an add.R file, but no init.c file. Try running:\n\ndevtools::load_all()\n\nYou should have access to add_one(), but if you call it, you get…\n\nadd_one(1)\n#> Error in add_one(1) : object 'addr_add_one' not found\n\nThis makes sense, because we have no init.c file, so the C function was not exposed to the R side.\nAt this point, we can generate the init.c file automatically using pkgbuild::compile_dll(). The key is to run it with register_routines = TRUE, which will take care of automatically setting up init.c. You may also have to run it with force = TRUE. If you have previously compiled all of the C code already and nothing has changed, it won’t try and do it again (this is generally a good thing!), and the function will exit early. But we want to trigger the recompilation to make it generate the init.c file for us, so we should force it.\n\npkgbuild::compile_dll(force = TRUE, register_routines = TRUE)\n#> Re-compiling addr\n#> ─  installing *source* package ‘addr’ ...\n#>    ** libs\n#>    clang <trimmed>  -c init.c -o init.o\n#>    clang <trimmed> -o addr.so add.o init.o\n#>    installing to /private/var/<trimmed>/addr/libs\n#> ─  DONE (addr)\n\nThis output looks similar to what was generated with devtools::load_all(), and that’s because this function is called from it. But this time, you can see that it compiled an init.c file as well, one that it created for us! Let’s take a look at init.c:\n\n#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\n\n/* Section generated by pkgbuild, do not edit */\n/* .Call calls */\nextern SEXP addr_add_one(SEXP);\n\nstatic const R_CallMethodDef CallEntries[] = {\n    {\"addr_add_one\", (DL_FUNC) &addr_add_one, 1},\n    {NULL, NULL, 0}\n};\n/* End section generated by pkgbuild */\n\nvoid R_init_addr(DllInfo *dll)\n{\n    R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n    R_useDynamicSymbols(dll, FALSE);\n}\n\nAwesome, so this entire file was created automatically, and looks essentially the same as the one that we made manually. Now that we have all the pieces, call devtools::load_all() one more time, which will sync everything up. Then you should be able to do:\n\nadd_one(1)\n#> [1] 2\n\nYou’ll also want to call devtools::document() again as well to add export(add_one) to the NAMESPACE file if you haven’t already.\nThe reason we have to create add_one() first, before calling pkgbuild::compile_dll(), is because of the way the information is found to generate the init.c file. It looks into your .R files, and scans for any calls to .Call(). The information there, along with the name of your package, is enough to completely generate the init.c file! Internally, compile_dll() calls tools::package_native_routine_registration_skeleton() (yes, this is a mouthful), which is what generates the skeleton for init.c, using the information it located. compile_dll() performs a few extra steps on top of that to clean up.\nIf you add another C based function to your package, just call compile_dll(register_routines = TRUE) again, and it will update the information in the init.c file."
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "",
    "text": "School has started up, and I’m in a class called Financial Computing. I thought it might be interesting to share some of my assignments and explain what I learn along the way. Most of the posts won’t be describing the Stochastic Calculus involved in each assignment, but will instead focus on the details of the implementation in R. I don’t claim to have the best way for any of these assignments, but perhaps you can learn something!"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#what-are-we-doing-today",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#what-are-we-doing-today",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "What are we doing today?",
    "text": "What are we doing today?\nIn this post, I’ll work through simulating paths of a stock that follows the log normal distribution used in the Black Scholes model. Importantly, I’ll explain my thought process as I tried to optimize the implementation from loops to vectors.\nAs an added bonus, at the very bottom is some extra content on a basic function that I have created to replicate the concept of broadcasting from Python. Someone could (should?) probably create an entire package out of this idea."
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#the-model",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#the-model",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "The Model",
    "text": "The Model\nUnder the Black Scholes model, the stock price, \\(S_t\\), at time t follows a Geometric Brownian Motion, which means that it satisfies the Stochastic Differential Equation:\n\\[ dS_t = r S_t dt + \\sigma S_t dW_t \\]\nWhere:\n\n\n\\(r =\\) Drift - The average log return per time unit\n\n\\(\\sigma =\\) Volatility - How much variance is in that drift\n\n\\(W_t =\\) Brownian Motion - Random noise from a normal distribution with mean 0 and variance t\n\n\nInterestingly, we actually have the solution to this equation (one of the few we have analytical solutions for):\n\\[ S_t = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t + \\sigma W_t} \\]\nMore generally, this can be written as a formula providing us with the recursive equation:\n\\[ S_{t_i} = S_{t_{i-1}} \\times e^{(r - \\frac{1}{2} \\sigma^2) ({t_i - t_{i-1}}) + \\sigma (W_{t_i} - W_{t_{i-1}}) } \\]\nIf you want to know how to get the solution, this is a pretty good explanation, but be prepared to learn about Ito’s Lemma."
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#really-dude",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#really-dude",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "Really, dude?",
    "text": "Really, dude?\nOkay, that’s a lot without any explanation, and I get that. But the point of this post is more to explain how to simulate paths of \\(S_t\\). So how do we do that?\n\nWe will start from time 0 with an initial stock price, then we will generate the next stock price from that using the recursive formula, and so on.\nThe only random piece is the brownian motion increment (dW), which we will generate at each time point using draws from a normal distribution.\nThe time increment will be a constant value (dt) to keep things simple.\n\nI was given some starting parameters:\n\n# Set a seed too so I can reproduce the exact numbers\nset.seed(123)\n\n# Parameters\nr       <- 0.028\nsigma   <- 0.255\ntime_T  <- 0.5\ndt      <- 1/12\nt_total <- time_T / dt\ns_0     <- 100\nN       <- 10000\n\nWhere time_T and dt mean each simulation path will go from time 0 to time_T by increments of dt. Dividing time_T / dt gets us the total number of time steps required. N is the number of paths to simulate."
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#first-attempt",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#first-attempt",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "First attempt",
    "text": "First attempt\nWe all know loops are to be avoided when you can in R, and that you should instead vectorize the operations. At first, I thought this wasn’t going to be possible, as this is a recursive type of formula where the next value relies on the previous one. With that in mind, I created the following implementation.\nFirst off, set up a matrix to fill with the 10000 simulations (one per row), each one having 6 time steps (7 columns total including the initial stock price).\n\n# Create a 10000x7 matrix of NA's to fill in\n# Each row is a simulation\ns <- matrix(NA_real_, nrow = N, ncol = t_total+1)\n\n# The first column is just the initial price\ns[,1] <- s_0\n\nhead(s)\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#> [1,]  100   NA   NA   NA   NA   NA   NA\n#> [2,]  100   NA   NA   NA   NA   NA   NA\n#> [3,]  100   NA   NA   NA   NA   NA   NA\n#> [4,]  100   NA   NA   NA   NA   NA   NA\n#> [5,]  100   NA   NA   NA   NA   NA   NA\n#> [6,]  100   NA   NA   NA   NA   NA   NA\n\nSo far so good, now let’s create a matrix for dW, our brownian motion increments. A very important fact is that these are all independent of each other, so the generation of them is straightforward. Each increment:\n\\[ W_{t_i} - W_{t_{i-1}} \\]\ncan be drawn from a normal distribution with mean 0 and variance \\(t_i - t_{i-1}\\) (which is what I have defined as dt because it is a constant).\n\n# ~N(0, dt)\n# To fill in 10000 simulations, and move forward 6 steps, we need a 10000x6 matrix\ndW <- rnorm(N * t_total, mean = 0, sd = sqrt(dt))\ndW <- matrix(dW, N, t_total)\nhead(dW)\n#>             [,1]        [,2]        [,3]        [,4]        [,5]        [,6]\n#> [1,] -0.16179538  0.68436942 -0.24141807 -0.05588936  0.13929365  0.07493710\n#> [2,] -0.06644652 -0.04815446 -0.06367394  0.07452070  0.20824004  0.26486253\n#> [3,]  0.44996033  0.26759071 -0.60723241 -0.15539745 -0.14658701 -0.20851535\n#> [4,]  0.02035402 -0.16401128 -0.48145457 -0.34036612 -0.01867981 -0.23333150\n#> [5,]  0.03732215  0.06497791 -0.31695458  0.25999452  0.37589000 -0.04080481\n#> [6,]  0.49509662  0.32677618 -0.48082343 -0.00469084 -0.06311503  0.65154366\n\nBased on this setup, I thought I would need a loop. The algorithm would step through the 10000 simulations all at once, but would have to loop through the 6 time steps one at a time, because each time step depended on the previous one. So, following the formula (below again for reference) I did this:\n\\[ S_{t_i} = S_{t_{i-1}} \\times e^{(r - \\frac{1}{2} \\sigma^2) ({t_i - t_{i-1}}) + \\sigma (W_{t_i} - W_{t_{i-1}}) } \\]\n\nfor(i in 1:(t_total)) {\n  s[,i+1] <- s[,i] * exp((r - 1/2 * sigma^2) * dt + sigma * dW[,i])\n}\n\nhead(s)\n#>      [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n#> [1,]  100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n#> [2,]  100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n#> [3,]  100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n#> [4,]  100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n#> [5,]  100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n#> [6,]  100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\nAnd that does work! But can we avoid the loop? YES WE CAN!"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#math",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#math",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "MATH",
    "text": "MATH\nTo avoid the loop, we are going to manipulate a simple case, and then apply it generally. One key element for this vectorization is that brownian motion increments are independent. Let’s think about what actually happens from time 0->1 and from 1->2.\n\\[ S_1 = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0) + \\sigma (W_1 - W_0)} \\] \\[ S_2 = S_1 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_1) + \\sigma (W_2 - W_1)} \\]\nIf we plug the equation for S_1 into the equation for S_2…\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0) + \\sigma (W_1 - W_0)}) \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_1) + \\sigma (W_2 - W_1)} \\]\nAnd then combine exponents…\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_1 - t_0 + t_2 - t_1) + \\sigma (W_1 - W_0 + W_2 - W_1)}) \\]\nNotice that some of the t and W terms cancel:\n\\[ S_2 = (S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) (t_2 - t_0) + \\sigma (W_2 - W_0)}) \\]\nAnd by definition t_0 and W_0 are 0:\n\\[ S_2 = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t_2 + \\sigma W_2} \\]\nThis is actually the form that was proposed as the solution to the geometric brownian motion stochastic differential equation:\n\\[ S_t = S_0 \\times e^{(r - \\frac{1}{2} \\sigma^2) t + \\sigma W_t} \\]\nIt looks like we can actually generate S_2 without needing to know S_1 at all. Notice that the exponent now contains t_2 and W_2. t_2 is known beforehand, but W_2 seems like it would rely on W_1 in a way that has to be recursively calculated. Actually, if we think of W_2 as a sum of brownian motion increments (I told you this would help):\n\\[ W_2 = (W_2 - W_1) + (W_1 - W_0) = dW_2 + dW_1 \\]\nthen W_2 is just the cumulative sum of the increments, and, by definition, each increment is independent of the previous increment so we can generate them all before hand (we already did this when we created the dW matrix).\n\n# Rowwise cumulative sum of dW generates W1, W2, W3, ... for each simulation\nW  <- plyr::aaply(dW, 1, cumsum)\nhead(W)\n#>    \n#> X1            1          2          3           4          5          6\n#>   1 -0.16179538  0.5225740  0.2811560  0.22526661  0.3645603  0.4394974\n#>   2 -0.06644652 -0.1146010 -0.1782749 -0.10375422  0.1044858  0.3693483\n#>   3  0.44996033  0.7175510  0.1103186 -0.04507883 -0.1916658 -0.4001812\n#>   4  0.02035402 -0.1436573 -0.6251118 -0.96547795 -0.9841578 -1.2174893\n#>   5  0.03732215  0.1023001 -0.2146545  0.04534000  0.4212300  0.3804252\n#>   6  0.49509662  0.8218728  0.3410494  0.33635853  0.2732435  0.9247872\n\nUnlike the recursive formula from before where dt was used, the time that we are currently at, t, is used instead so we will need that as well.\n\ntime_steps <- matrix(seq(from = dt, to = time_T, by = dt), \n                     nrow = N, ncol = t_total, byrow = TRUE)\nhead(time_steps)\n#>            [,1]      [,2] [,3]      [,4]      [,5] [,6]\n#> [1,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [2,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [3,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [4,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [5,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n#> [6,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n\nNow it’s a vectorized one-liner to calculate the stock price at each time!\n\n# Stock price simulation\ns_t <- s_0 * exp((r - 1/2 * sigma^2) * time_steps + sigma * W)\n\n# Add the original stock price onto the front\ns_t <- cbind(s_0, s_t)\n\n# Add 0 as the column name for initial value (it's important I promise)\ncolnames(s_t)[1] <- \"0\"\n\nhead(s_t)\n#>     0         1         2         3         4         5         6\n#> 1 100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n#> 2 100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n#> 3 100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n#> 4 100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n#> 5 100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n#> 6 100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\nJust as a sanity check, this should have produced the same results as the for loop\n\n# ignore the dimname attributes\nall.equal(s, s_t, check.attributes = FALSE)\n#> [1] TRUE"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#now-what",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#now-what",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "Now what?",
    "text": "Now what?\nThere are a number of interesting things we could do with these results. One is to calculate the fair price of a European Option on this stock. I think I’ll save that for the next post.\nSomething else we might do is visualize the distribution of \\(S_T\\), the stock price at the terminal (final) time. Because the stock price is modeled as an exponential of a normal random variable (W_t), the stock price itself has a log-normal distribution. For practicality, this means that it is right tailed and can’t drop below 0 (good properties of a stock).\n\nlibrary(tidyverse)\n\n# let's just take a moment and admire the fact that I can put LaTeX in ggplots\nlibrary(latex2exp) \n\ntibble::tibble(s_T = s_t[,6]) %>%\n  ggplot(mapping = aes(x=s_T)) +\n  geom_histogram(bins = 500) + \n  labs(x = TeX('S_T'), y = NULL, title = TeX('Log-normal Distribution of S_T') )\n\n\n\n\nWe could also look at the 6-step path of 100 of our simulations.\n\nas_tibble(s_t) %>%\n  rownames_to_column(var = \"iteration\") %>%\n  gather(time_step, stock_price, -iteration) %>%\n  mutate(time_step = as.numeric(time_step),\n         iteration = as.factor(iteration)) %>%\n  filter(iteration %in% 1:100) %>%\n  \n  ggplot(aes(x = time_step, y = stock_price, group = iteration)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Stock Price\", title = \"100 Simulated Paths\")"
  },
  {
    "objectID": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#conclusion-extra-content",
    "href": "posts/2017-08-26-numerical-methods-for-finance-part-1/index.html#conclusion-extra-content",
    "title": "Financial Numerical Methods - Part 1: Vectorized Stock Price Simulation",
    "section": "Conclusion + Extra Content",
    "text": "Conclusion + Extra Content\nIf I haven’t bored you to tears yet, allow me to thank you for sticking around this long. I think these posts are useful because they force me to try and understand a concept a bit more than if I was just reading it from a book.\nAs you may have noted in the post above, I had to create a large matrix time_steps for R to perform the matrix addition I wanted correctly. I thought it would have been simple. Ideally I could create a 1x6 matrix of times, and add it to a 10000x6 matrix of the brownian motions and have the times matrix broadcasted to each row of the brownian motion matrix, adding element by element to each row. This works in Python and Matlab, but R has a mind of it’s own.\n\n# First try with a vector\nx1 <- c(1,2)\nx1\n#> [1] 1 2\n\nx2 <- matrix(c(1,2,3,4), nrow = 2, ncol = 2, byrow = TRUE)\nx2\n#>      [,1] [,2]\n#> [1,]    1    2\n#> [2,]    3    4\n\n# I want to add c(1,2) to each row of x2\nx1 + x2\n#>      [,1] [,2]\n#> [1,]    2    3\n#> [2,]    5    6\n\nThat’s not right, it’s adding 1 to the first column of x2, then 2 to the second column of x2. To get what I want I could butcher it like this:\n\nt(x1+t(x2))\n#>      [,1] [,2]\n#> [1,]    2    4\n#> [2,]    4    6\n\nIf x1 was a matrix instead of a vector, then it gives a non-conformable array error.\n\nx1 <- matrix(x1, ncol = 2)\nx1\n#>      [,1] [,2]\n#> [1,]    1    2\n\nx1 + x2\n#> Error in x1 + x2: non-conformable arrays\n\nSo R is super strict here. That’s fine and all, but with other languages able to do this, and with it being such a natural way of thinking about this type of addition, I decided to roll my own function that allows me to add matrices together that meet certain conditions by broadcasting one of them over the other.\n\n# Broadcast addition\n# One of the special % % operators\n`%+%` <- function(e1, e2) {\n  \n  stopifnot(is.matrix(e1))\n  stopifnot(is.matrix(e2))\n  \n  # e1 - e2 & 1 has more rows & equal cols\n  if(nrow(e1) >= nrow(e2) & ncol(e1) == ncol(e2)) {\n    case <- \"1\"\n    \n    # e1 - e2 & 2 has more rows & equal cols\n  } else if(nrow(e1) < nrow(e2) & ncol(e1) == ncol(e2)) {\n    case <- \"2\"\n    \n    # e1 - e2 & 1 has more cols & equal rows\n  } else if(ncol(e1) >= ncol(e2) & nrow(e1) == nrow(e2)) {\n    case <- \"3\"\n    \n    # e1 - e2 & 2 has more cols & equal rows\n  } else if(ncol(e1) < ncol(e2) & nrow(e1) == nrow(e2)) {\n    case <- \"4\"\n    \n    # Fail\n  } else {\n    stop(\"Incorrect dims\")\n  }\n  \n  switch(case,\n         \"1\" = t(apply(e1, 1, function(x) {x + e2})),\n         \"2\" = t(apply(e2, 1, function(x) {x + e1})),\n         \"3\" = t(apply(e1, 2, function(x) {x + e2})),\n         \"4\" = t(apply(e2, 2, function(x) {x + e1})))\n}\n\nLet’s see what this thing can do!\n\nx1 %+% x2\n#>      [,1] [,2]\n#> [1,]    2    4\n#> [2,]    4    6\n\nNice! That’s what I want. One thing to note is that order of operations don’t work quite as you’d expect because of the precedence of the special %+% operator in relation to + and *, so you have to be really explicit.\n\n# This tries to do addition first\nx1 * 2 %+% x2\n#> Error in 2 %+% x2: is.matrix(e1) is not TRUE\n\n# Explicit parenthesis\n(x1 * 2) %+% x2\n#>      [,1] [,2]\n#> [1,]    3    6\n#> [2,]    5    8\n\nArmed with the ability to broadcast addition, let’s redo the last step of the stock price simulation.\n\n# Instead of a massive matrix, just create a 1x6\ntime_steps <- matrix(seq(from = dt, to = time_T, by = dt), nrow = 1, ncol = t_total)\ntime_steps\n#>            [,1]      [,2] [,3]      [,4]      [,5] [,6]\n#> [1,] 0.08333333 0.1666667 0.25 0.3333333 0.4166667  0.5\n\n# Remember that W is 10000x6\nhead(W)\n#>    \n#> X1            1          2          3           4          5          6\n#>   1 -0.16179538  0.5225740  0.2811560  0.22526661  0.3645603  0.4394974\n#>   2 -0.06644652 -0.1146010 -0.1782749 -0.10375422  0.1044858  0.3693483\n#>   3  0.44996033  0.7175510  0.1103186 -0.04507883 -0.1916658 -0.4001812\n#>   4  0.02035402 -0.1436573 -0.6251118 -0.96547795 -0.9841578 -1.2174893\n#>   5  0.03732215  0.1023001 -0.2146545  0.04534000  0.4212300  0.3804252\n#>   6  0.49509662  0.8218728  0.3410494  0.33635853  0.2732435  0.9247872\n\n# Add using broadcasted addition, making sure to be careful about parenthesis!\ns_t <- s_0 * exp(((r - 1/2 * sigma^2) * time_steps) %+% (sigma * W))\n\ns_t <- cbind(s_0, s_t, deparse.level = 0)\nhead(s_t)\n#>   [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n#> 1  100  95.92209 114.16839 107.31161 105.75330 109.53595 111.60722\n#> 2  100  98.28292  97.04695  95.44803  97.24258 102.50728 109.62854\n#> 3  100 112.11600 119.98823 102.73710  98.70849  95.05115  90.09528\n#> 4  100 100.48258  96.33055  85.16909  78.05933  77.65918  73.14576\n#> 5  100 100.91830 102.56581  94.56668 101.01083 111.13033 109.93864\n#> 6  100 113.41388 123.22299 108.96314 108.79196 107.01479 126.30943\n\nSo much better! I have used this a few times in the past month or so. Credit to Alex Hayes for teaching me a bit about why broadcasting is awesome. I created the base for %+% in response to his comments here."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html",
    "href": "posts/2017-05-10-aws-rds-r/index.html",
    "title": "Amazon RDS + R",
    "section": "",
    "text": "The code and advice in this post is still valid, but I have shut down the AWS database that this post pulled from, so it no longer runs. Additionally, the output is no longer shown."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#intro",
    "href": "posts/2017-05-10-aws-rds-r/index.html#intro",
    "title": "Amazon RDS + R",
    "section": "Intro",
    "text": "Intro\nWelcome to my first post! To start things off at Data Insights, I’m going to show you how to connect to an AWS RDS instance from R.\nFor those of you who don’t know, RDS is an easy way to create a database in the cloud. In this post, I won’t be showing you how to setup an RDS instance, but I will show you how to connect to it if you have one running.\nLet’s get started."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#step-1-the-one-where-you-got-connected",
    "href": "posts/2017-05-10-aws-rds-r/index.html#step-1-the-one-where-you-got-connected",
    "title": "Amazon RDS + R",
    "section": "Step 1: The one where you got connected",
    "text": "Step 1: The one where you got connected\nYou’ll need a few packages to get started.\n\n\nDBI and RMySQL are used to connect to the database, although RMySQL is usually called without explicitely loading it (that’s just the standard)\n\ntidyquant is just there to help us download some data to put in and get out of our database\n\ndplyr will be used to show off an alternate way to query from the database. Note that you should get the most recent github version of dplyr, along with the database specific pieces from dbplyr.\n\n\nlibrary(DBI)\n# library(RMySQL)\n\nlibrary(tidyquant)\n\n# devtools::install_github(\"tidyverse/dplyr\")\n# devtools::install_github(\"tidyverse/dbplyr\")\nlibrary(dplyr)\n# library(dbplyr)\n\nGetting connected isn’t too hard once you know what you’re looking for.\n\ncn <- dbConnect(drv      = RMySQL::MySQL(), \n                username = \"user1\", \n                password = \"testpassword\", \n                host     = \"davisdbinstance.crarljboc8to.us-west-2.rds.amazonaws.com\", \n                port     = 3306, \n                dbname   = \"firstdb\")\n\nLet’s go through the arguments to dbConnect(), the function from DBI that we used to connect.\n\ndrv - The driver I used is from the RMySQL package, an implementation of the general interface provided by DBI. I’ll leave it to the experts to explain all of this.\nusername / password - You’ll have to have created a user and password on AWS first, but then you can use them here.\nhost The host name is the Endpoint of your RDS server, without the port on the end. I’ve attached a screenshot to show where to find this. Basically, on the RDS Dashboard Instances page, hit the drop down arrow beside “MySQL” to show the Endpoint.\n\n\n\nport - The rest of the Endpoint shows the port that you’ll need to access your RDS instance through. That goes here.\ndbname - Finally, you’ll need the DB Name you used when setting up the instance. This can be found by clicking Instance Actions -> See Details, and then under Configuration Details you’ll find DB Name."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#step-2-the-one-where-you-take-it-for-a-test-spin",
    "href": "posts/2017-05-10-aws-rds-r/index.html#step-2-the-one-where-you-take-it-for-a-test-spin",
    "title": "Amazon RDS + R",
    "section": "Step 2: The one where you take it for a test spin",
    "text": "Step 2: The one where you take it for a test spin\nWell, alright…that was…cool? How do we know it’s working? Let’s get some data to load into the database. We will use some Apple stock data retrieved through tidyquant.\n\naapl <- tq_get(\"AAPL\")\n\nslice(aapl, 1:10)\n\n\n\n\nTo write the tibble (data frame) to the database, we will use another function called dbWriteTable(). It’s pretty straightforward. “name” is the name of the table you are creating, and “value” is the data frame you want to write.\n\ndbWriteTable(cn, name = \"apple\", value = aapl)\n\nNow the fun part! Let’s use a SQL query to pull it back down with dbGetQuery(). This function is a combination of dbSendQuery(), which returns a result set for your query, and dbFetch() which returns the rows from that result set.\n\napple_from_db <- dbGetQuery(cn, \"SELECT * FROM apple;\")\n\n# This effectively is the same as\n# dbReadTable(cn, \"apple\")\n\nslice(apple_from_db, 1:10)\n\nThere are a huge number of functions from DBI that you can use to communicate with databases. Maybe I will cover more in a separate post, but for now, let’s move on to dplyr."
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#step-3-the-one-with-the-pliers",
    "href": "posts/2017-05-10-aws-rds-r/index.html#step-3-the-one-with-the-pliers",
    "title": "Amazon RDS + R",
    "section": "Step 3: The one with the pliers",
    "text": "Step 3: The one with the pliers\nBefore dplyr 0.6.0 was announced, you’d have to disconnect, and then reconnect through a dplyr specific function, src_mysql(). That would look something like the code below. Since then, however, you can now use the DBI connection with dplyr!\n\n# There is no need for this code anymore!\ndbDisconnect(cn)\n\ncn <- src_mysql(user     = \"user1\",\n                password = \"testpassword\",\n                host     = \"davisdbinstance.crarljboc8to.us-west-2.rds.amazonaws.com\",\n                port     = 3306,\n                dbname   = \"firstdb\")\n\nSelect the apple table from the database. This does not actually pull the data into memory. It just makes a connection!\n\n# With dplyr 0.6.0 we can just use the DBI connection!\napple_table <- tbl(cn, \"apple\")\n\n# By default the first 1000 rows are displayed\napple_table\n\nThe best part is that we can use almost any dplyr command with this! It queries the database, and does not do the manipulation in R. All of the familiar syntax of dplyr, but with databases. Let’s use filter() to get all of the rows after January 1, 2009.\n\nfilter(apple_table, date > \"2009-01-01\")\n\nTo do any serious manipulation outside of dplyr, you’ll likely have pull the data into memory to be able to use it with other R functions. Here, I’ll use the dplyr equivalent to dbWriteTable() to add the stock prices for IBM to the database.\n\nibm <- tq_get(\"IBM\")\n\ncopy_to(cn, df = ibm, temporary = FALSE)\n\nTo actually retrieve the data to memory, first make the connection using tbl() like before, and then use collect() to create the in memory tibble. Unfortunately, dates are stored as characters in the table, and collect() won’t try to fix that, so I’ll also take advantage of the readr package’s type_convert() function to do the thinking for me.\nOnce we have the data in memory, we can calculate the daily return with tidyquant and tq_mutate().\n\n# Connection\nibm_table <- tbl(cn, \"ibm\")\n\n# Collect to tibble\nreal_tibble <- collect(ibm_table) %>%\n  readr::type_convert()\n\n# Daily return\nreal_tibble <- real_tibble %>% \n  tq_mutate(select     = adjusted, \n            mutate_fun = periodReturn, \n            period     = \"daily\")\n\nreal_tibble\n\nAlways disconnect when you’re finished!\n\ndbDisconnect(cn)"
  },
  {
    "objectID": "posts/2017-05-10-aws-rds-r/index.html#last-words",
    "href": "posts/2017-05-10-aws-rds-r/index.html#last-words",
    "title": "Amazon RDS + R",
    "section": "Last words",
    "text": "Last words\nHopefully I’ve been able to show you the power of DBI + dplyr with Amazon RDS. This integration has come a long way, and is just one of the huge advancements that the RStudio team has been working on in collaboration with other R users in the community.\nUntil next time!"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html",
    "href": "posts/2017-08-16-hadley-pleased/index.html",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "",
    "text": "Awhile back, I saw a conversation on twitter about how Hadley uses the word “pleased” very often when introducing a new blog post (I couldn’t seem to find this tweet anymore. Can anyone help?). Out of curiousity, and to flex my R web scraping muscles a bit, I’ve decided to analyze the 240+ blog posts that RStudio has put out since 2011. This post will do a few things:\n\nScrape the RStudio blog archive page to construct URL links to each blog post\nScrape the blog post text and metadata from each post\nUse a bit of tidytext for some exploratory analysis\nPerform a statistical test to compare Hadley’s use of “pleased” to the other blog post authors\n\nSpoiler alert: Hadley uses “pleased” ALOT."
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#required-packages",
    "href": "posts/2017-08-16-hadley-pleased/index.html#required-packages",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Required packages",
    "text": "Required packages\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rvest)\nlibrary(xml2)"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#extract-the-html-from-the-rstudio-blog-archive",
    "href": "posts/2017-08-16-hadley-pleased/index.html#extract-the-html-from-the-rstudio-blog-archive",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Extract the HTML from the RStudio blog archive",
    "text": "Extract the HTML from the RStudio blog archive\nTo be able to extract the text from each blog post, we first need to have a link to that blog post. Luckily, RStudio keeps an up to date archive page that we can scrape. Using xml2, we can get the HTML off that page.\n\narchive_page <- \"https://blog.rstudio.com/archives/\"\n\narchive_html <- read_html(archive_page)\n\n# Doesn't seem very useful...yet\narchive_html\n\n\n#> {xml_document}\n#> <html lang=\"en-us\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\\n<meta charset=\"u ...\n#> [2] <body>\\n    <nav class=\"menu\"><svg version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xli ...\n\nNow we use a bit of rvest magic combined with the HTML inspector in Chrome to figure out which elements contain the info we need (I also highly recommend SelectorGadget for this kind of work). Looking at the image below, you can see that all of the links are contained within the main tag as a tags (links).\n\nThe code below extracts all of the links, and then adds the prefix containing the base URL of the site.\n\nlinks <- archive_html %>%\n  \n  # Only the \"main\" body of the archive\n  html_nodes(\"main\") %>%\n  \n  # Grab any node that is a link\n  html_nodes(\"a\") %>%\n  \n  # Extract the hyperlink reference from those link tags\n  # The hyperlink is an attribute as opposed to a node\n  html_attr(\"href\") %>%\n  \n  # Prefix them all with the base URL\n  paste0(\"http://blog.rstudio.com\", .)\n\nhead(links)\n\n\n#> [1] \"http://blog.rstudio.com/2017/08/25/rstudio-conf-2018-early-bird-pricing/\"    \n#> [2] \"http://blog.rstudio.com/2017/08/22/rstudio-v1-1-preview-object-explorer/\"    \n#> [3] \"http://blog.rstudio.com/2017/08/18/google-cloud-platform/\"                   \n#> [4] \"http://blog.rstudio.com/2017/08/16/rstudio-preview-connections/\"             \n#> [5] \"http://blog.rstudio.com/2017/08/15/contributed-talks-diversity-scholarships/\"\n#> [6] \"http://blog.rstudio.com/2017/08/15/shiny-1-0-4/\""
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#html-from-each-blog-post",
    "href": "posts/2017-08-16-hadley-pleased/index.html#html-from-each-blog-post",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "HTML from each blog post",
    "text": "HTML from each blog post\nNow that we have every link, we’re ready to extract the HTML from each individual blog post. To make things more manageable, we start by creating a tibble, and then using the mutate + map combination to created a column of XML Nodesets (we will use this combination a lot). Each nodeset contains the HTML for that blog post (exactly like the HTML for the archive page).\n\nblog_data <- tibble(links)\n\nblog_data <- blog_data %>%\n  mutate(main = map(\n                    # Iterate through every link\n                    .x = links, \n                    \n                    # For each link, read the HTML for that page, and return the main section \n                    .f = ~read_html(.) %>%\n                            html_nodes(\"main\")\n                    )\n         )\n\nblog_data$main[1]\n\n\n#> [[1]]\n#> {xml_nodeset (1)}\n#> [1] <main><div class=\"article-meta\">\\n<h1><span class=\"title\">Newer to R? rstudio::conf 2018 is fo ..."
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#meta-information",
    "href": "posts/2017-08-16-hadley-pleased/index.html#meta-information",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Meta information",
    "text": "Meta information\nBefore extracting the blog post itself, lets grab the meta information about each post, specifically:\n\nAuthor\nTitle\nDate\nCategory\nTags\n\nIn the exploratory analysis, we will use author and title, but the other information might be useful for future analysis.\nLooking at the first blog post, the Author, Date, and Title are all HTML class names that we can feed into rvest to extract that information.\n\nIn the code below, an example of extracting the author information is shown. To select a HTML class (like “author”) as opposed to a tag (like “main”), we have to put a period in front of the class name. Once the html node we are interested in has been identified, we can extract the text for that node using html_text().\n\nblog_data$main[[1]] %>%\n  html_nodes(\".author\") %>%\n  html_text()\n\n\n#> [1] \"Roger Oberg\"\n\nTo scale up to grab the author for all posts, we use map_chr() since we want a character of the author’s name returned.\n\nmap_chr(.x = blog_data$main,\n        .f = ~html_nodes(.x, \".author\") %>%\n                html_text()) %>%\n  head(10)\n\n\n#>  [1] \"Roger Oberg\"        \"Kevin Ushey\"        \"Roger Oberg\"       \n#>  [4] \"Jonathan McPherson\" \"Hadley Wickham\"     \"Winston Chang\"     \n#>  [7] \"Gary Ritchie\"       \"Roger Oberg\"        \"Jeff Allen\"        \n#> [10] \"Javier Luraschi\"\n\nFinally, notice that if we switch \".author\" with \".title\" or \".date\" then we can grab that information as well. This kind of thinking means that we should create a function for extracting these pieces of information!\n\nextract_info <- function(html, class_name) {\n  map_chr(\n          # Given the list of main HTMLs\n          .x = html,\n          \n          # Extract the text we are interested in for each one \n          .f = ~html_nodes(.x, class_name) %>%\n                  html_text())\n}\n\n# Extract the data\nblog_data <- blog_data %>%\n  mutate(\n     author = extract_info(main, \".author\"),\n     title  = extract_info(main, \".title\"),\n     date   = extract_info(main, \".date\")\n    )\n\n\nselect(blog_data, author, date)\n\n\n#> # A tibble: 253 × 2\n#>    author             date      \n#>    <chr>              <chr>     \n#>  1 Roger Oberg        2017-08-25\n#>  2 Kevin Ushey        2017-08-22\n#>  3 Roger Oberg        2017-08-18\n#>  4 Jonathan McPherson 2017-08-16\n#>  5 Hadley Wickham     2017-08-15\n#>  6 Winston Chang      2017-08-15\n#>  7 Gary Ritchie       2017-08-11\n#>  8 Roger Oberg        2017-08-10\n#>  9 Jeff Allen         2017-08-03\n#> 10 Javier Luraschi    2017-07-31\n#> # … with 243 more rows\n\n\nselect(blog_data, title)\n\n\n#> # A tibble: 253 × 1\n#>    title                                                                        \n#>    <chr>                                                                        \n#>  1 Newer to R? rstudio::conf 2018 is for you! Early bird pricing ends August 31.\n#>  2 RStudio v1.1 Preview - Object Explorer                                       \n#>  3 RStudio Server Pro is ready for BigQuery on the Google Cloud Platform        \n#>  4 RStudio 1.1 Preview - Data Connections                                       \n#>  5 rstudio::conf(2018): Contributed talks, e-posters, and diversity scholarships\n#>  6 Shiny 1.0.4                                                                  \n#>  7 RStudio v1.1 Preview: Terminal                                               \n#>  8 Building tidy tools workshop                                                 \n#>  9 RStudio Connect v1.5.4 - Now Supporting Plumber!                             \n#> 10 sparklyr 0.6                                                                 \n#> # … with 243 more rows"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#categories-and-tags",
    "href": "posts/2017-08-16-hadley-pleased/index.html#categories-and-tags",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Categories and tags",
    "text": "Categories and tags\nThe other bits of meta data that might be interesting are the categories and tags that the post falls under. This is a little bit more involved, because both the categories and tags fall under the same class, \".terms\". To separate them, we need to look into the href to see if the information is either a tag or a category (href = “/categories/” VS href = “/tags/”).\n\nThe function below extracts either the categories or the tags, depending on the argument, by:\n\nExtracting the \".terms\" class, and then all of the links inside of it (a tags).\nChecking each link to see if the hyperlink reference contains “categories” or “tags” depending on the one that we are interested in. If it does, it returns the text corresponding to that link, otherwise it returns NAs which are then removed.\n\nThe final step results in two list columns containing character vectors of varying lengths corresponding to the categories and tags of each post.\n\nextract_tag_or_cat <- function(html, info_name) {\n  \n  # Extract the links under the terms class\n  cats_and_tags <- map(.x = html, \n                       .f = ~html_nodes(.x, \".terms\") %>%\n                              html_nodes(\"a\"))\n  \n  # For each link, if the href contains the word categories/tags \n  # return the text corresponding to that link\n  map(cats_and_tags, \n    ~if_else(condition = grepl(info_name, html_attr(.x, \"href\")), \n             true      = html_text(.x), \n             false     = NA_character_) %>%\n      .[!is.na(.)])\n}\n\n# Apply our new extraction function\nblog_data <- blog_data %>%\n  mutate(\n    categories = extract_tag_or_cat(main, \"categories\"),\n    tags       = extract_tag_or_cat(main, \"tags\")\n  )\n\n\nselect(blog_data, categories, tags)\n\n\n#> # A tibble: 253 × 2\n#>    categories tags     \n#>    <list>     <list>   \n#>  1 <chr [3]>  <chr [1]>\n#>  2 <chr [1]>  <chr [0]>\n#>  3 <chr [2]>  <chr [4]>\n#>  4 <chr [1]>  <chr [0]>\n#>  5 <chr [1]>  <chr [0]>\n#>  6 <chr [2]>  <chr [0]>\n#>  7 <chr [1]>  <chr [3]>\n#>  8 <chr [3]>  <chr [8]>\n#>  9 <chr [3]>  <chr [2]>\n#> 10 <chr [1]>  <chr [3]>\n#> # … with 243 more rows\n\n\nblog_data %>%\n  filter(title == \"Building tidy tools workshop\") %>%\n  pull(categories)\n\n\n#> [[1]]\n#> [1] \"Packages\"  \"tidyverse\" \"Training\"\n\n\nblog_data %>%\n  filter(title == \"Building tidy tools workshop\") %>%\n  pull(tags)\n\n\n#> [[1]]\n#> [1] \"Advanced R\"       \"data science\"     \"ggplot2\"          \"Hadley Wickham\"  \n#> [5] \"R\"                \"RStudio Workshop\" \"r training\"       \"tutorial\""
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#the-blog-post-itself",
    "href": "posts/2017-08-16-hadley-pleased/index.html#the-blog-post-itself",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "The blog post itself",
    "text": "The blog post itself\nFinally, to extract the blog post itself, we can notice that each piece of text in the post is inside of a paragraph tag (p). Being careful to avoid the \".terms\" class that contained the categories and tags, which also happens to be in a paragraph tag, we can extract the full blog posts. To ignore the \".terms\" class, use the :not() selector.\n\nblog_data <- blog_data %>%\n  mutate(\n    text = map_chr(main, ~html_nodes(.x, \"p:not(.terms)\") %>%\n                 html_text() %>%\n                 # The text is returned as a character vector. \n                 # Collapse them all into 1 string.\n                 paste0(collapse = \" \"))\n  )\n\n\n\n\n\nselect(blog_data, text)\n#> # A tibble: 253 × 1\n#>    text                                                                         \n#>    <chr>                                                                        \n#>  1 \"Immersion is among the most effective ways to learn any language. Immersing…\n#>  2 \"Today, we’re continuing our blog series on new features in RStudio 1.1. If …\n#>  3 \"RStudio is excited to announce the availability of RStudio Server Pro on th…\n#>  4 \"Today, we’re continuing our blog series on new features in RStudio 1.1. If …\n#>  5 \"rstudio::conf, the conference on all things R and RStudio, will take place …\n#>  6 \"Shiny 1.0.4 is now available on CRAN. To install it, run: For most Shiny us…\n#>  7 \"Today we’re excited to announce availability of our first Preview Release f…\n#>  8 \"Have you embraced the tidyverse? Do you now want to expand it to meet your …\n#>  9 \"We’re thrilled to announce support for hosting Plumber APIs in RStudio Conn…\n#> 10 \"We’re excited to announce a new release of the sparklyr package, available …\n#> # … with 243 more rows"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#who-writes-the-most-posts",
    "href": "posts/2017-08-16-hadley-pleased/index.html#who-writes-the-most-posts",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Who writes the most posts?",
    "text": "Who writes the most posts?\nNow that we have all of this data, what can we do with it? To start with, who writes the most posts?\n\nblog_data %>%\n  group_by(author) %>%\n  summarise(count = n()) %>%\n  mutate(author = reorder(author, count)) %>%\n  \n  # Create a bar graph of author counts\n  ggplot(mapping = aes(x = author, y = count)) + \n  geom_col() +\n  coord_flip() +\n  labs(title    = \"Who writes the most RStudio blog posts?\",\n       subtitle = \"By a huge margin, Hadley!\") +\n  # Shoutout to Bob Rudis for the always fantastic themes\n  hrbrthemes::theme_ipsum(grid = \"Y\")"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#tidytext",
    "href": "posts/2017-08-16-hadley-pleased/index.html#tidytext",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Tidytext",
    "text": "Tidytext\nI’ve never used tidytext before today, but to get our feet wet, let’s create a tokenized tidy version of our data. By using unnest_tokens() the data will be reshaped to a long format holding 1 word per row, for each blog post. This tidy format lends itself to all manner of analysis, and a number of them are outlined in Julia Silge and David Robinson’s Text Mining with R.\n\ntokenized_blog <- blog_data %>%\n  mutate(short_title = str_sub(title, end = 15)) %>%\n  select(title, short_title, author, date, text) %>%\n  unnest_tokens(output = word, input = text)\n\nselect(tokenized_blog, short_title, word)\n#> # A tibble: 85,761 × 2\n#>    short_title     word     \n#>    <chr>           <chr>    \n#>  1 Newer to R? rst immersion\n#>  2 Newer to R? rst is       \n#>  3 Newer to R? rst among    \n#>  4 Newer to R? rst the      \n#>  5 Newer to R? rst most     \n#>  6 Newer to R? rst effective\n#>  7 Newer to R? rst ways     \n#>  8 Newer to R? rst to       \n#>  9 Newer to R? rst learn    \n#> 10 Newer to R? rst any      \n#> # … with 85,751 more rows"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#remove-stop-words",
    "href": "posts/2017-08-16-hadley-pleased/index.html#remove-stop-words",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Remove stop words",
    "text": "Remove stop words\nA number of words like “a” or “the” are included in the blog that don’t really add value to a text analysis. These stop words can be removed using an anti_join() with the stop_words dataset that comes with tidytext. After removing stop words, the number of rows was cut in half!\n\ntokenized_blog <- tokenized_blog %>%\n  anti_join(stop_words, by = \"word\") %>%\n  arrange(desc(date))\n\nselect(tokenized_blog, short_title, word)\n#> # A tibble: 40,315 × 2\n#>    short_title     word     \n#>    <chr>           <chr>    \n#>  1 Newer to R? rst immersion\n#>  2 Newer to R? rst effective\n#>  3 Newer to R? rst learn    \n#>  4 Newer to R? rst language \n#>  5 Newer to R? rst immersing\n#>  6 Newer to R? rst advanced \n#>  7 Newer to R? rst users    \n#>  8 Newer to R? rst improve  \n#>  9 Newer to R? rst language \n#> 10 Newer to R? rst rare     \n#> # … with 40,305 more rows"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#top-15-words-overall",
    "href": "posts/2017-08-16-hadley-pleased/index.html#top-15-words-overall",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Top 15 words overall",
    "text": "Top 15 words overall\nOut of pure curiousity, what are the top 15 words for all of the blog posts?\n\ntokenized_blog %>%\n  count(word, sort = TRUE) %>%\n  slice(1:15) %>%\n  mutate(word = reorder(word, n)) %>%\n  \n  ggplot(aes(word, n)) +\n  geom_col() + \n  coord_flip() + \n  labs(title = \"Top 15 words overall\") +\n  hrbrthemes::theme_ipsum(grid = \"Y\")"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#is-hadley-more-pleased-than-everyone-else",
    "href": "posts/2017-08-16-hadley-pleased/index.html#is-hadley-more-pleased-than-everyone-else",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Is Hadley more “pleased” than everyone else?",
    "text": "Is Hadley more “pleased” than everyone else?\nAs mentioned at the beginning of the post, Hadley apparently uses the word “pleased” in his blog posts an above average number of times. Can we verify this statistically?\nOur null hypothesis is that the proportion of blog posts that use the word “pleased” written by Hadley is less than or equal to the proportion of those written by the rest of the RStudio team.\nMore simply, our null is that Hadley uses “pleased” less than or the same as the rest of the team.\nLet’s check visually to compare the two groups of posts.\n\npleased <- tokenized_blog %>%\n  \n  # Group by blog post\n  group_by(title) %>%\n  \n  # If the blog post contains \"pleased\" put yes, otherwise no\n  # Add a column checking if the author was Hadley\n  mutate(\n    contains_pleased = case_when(\n      \"pleased\" %in% word ~ \"Yes\",\n      TRUE                ~ \"No\"),\n    \n    is_hadley = case_when(\n      author == \"Hadley Wickham\" ~ \"Hadley\",\n      TRUE                       ~ \"Not Hadley\")\n    ) %>%\n  \n  # Remove all duplicates now\n  distinct(title, contains_pleased, is_hadley)\n\npleased %>%\n  ggplot(aes(x = contains_pleased)) +\n  geom_bar() +\n  facet_wrap(~is_hadley, scales = \"free_y\") +\n  labs(title    = \"Does this blog post contain 'pleased'?\", \n       subtitle = \"Nearly half of Hadley's do!\",\n       x        = \"Contains 'pleased'\",\n       y        = \"Count\") +\n  hrbrthemes::theme_ipsum(grid = \"Y\")"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#is-there-a-statistical-difference-here",
    "href": "posts/2017-08-16-hadley-pleased/index.html#is-there-a-statistical-difference-here",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Is there a statistical difference here?",
    "text": "Is there a statistical difference here?\nTo check if there is a statistical difference, we will use a test for difference in proportions contained in the R function, prop.test(). First, we need a continency table of the counts. Given the current form of our dataset, this isn’t too hard with the table() function from base R.\n\ncontingency_table <- pleased %>%\n  ungroup() %>%\n  select(is_hadley, contains_pleased) %>%\n  # Order the factor so Yes is before No for easy interpretation\n  mutate(contains_pleased = factor(contains_pleased, levels = c(\"Yes\", \"No\"))) %>%\n  table()\n\ncontingency_table\n#>             contains_pleased\n#> is_hadley    Yes  No\n#>   Hadley      43  45\n#>   Not Hadley  17 148\n\nFrom our null hypothesis, we want to perform a one sided test. The alternative to our null is that Hadley uses “pleased” more than the rest of the RStudio team. For this reason, we specify alternative = \"greater\".\n\ntest_prop <- contingency_table %>%\n  prop.test(alternative = \"greater\")\n\ntest_prop\n#> \n#>  2-sample test for equality of proportions with continuity correction\n#> \n#> data:  .\n#> X-squared = 45.063, df = 1, p-value = 9.541e-12\n#> alternative hypothesis: greater\n#> 95 percent confidence interval:\n#>  0.2809899 1.0000000\n#> sample estimates:\n#>    prop 1    prop 2 \n#> 0.4886364 0.1030303\n\nWe could also tidy this up with broom if we were inclined to.\n\nbroom::tidy(test_prop)\n#> # A tibble: 1 × 9\n#>   estimate1 estimate2 statistic  p.value parameter conf.low conf.high method    \n#>       <dbl>     <dbl>     <dbl>    <dbl>     <dbl>    <dbl>     <dbl> <chr>     \n#> 1     0.489     0.103      45.1 9.54e-12         1    0.281         1 2-sample …\n#> # … with 1 more variable: alternative <chr>"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#test-conclusion",
    "href": "posts/2017-08-16-hadley-pleased/index.html#test-conclusion",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Test conclusion",
    "text": "Test conclusion\n\n48.86% of Hadley’s posts contain “pleased”\n10.3% of the rest of the RStudio team’s posts contain “pleased”\nWith a p-value of 9.5414477^{-12}, we reject the null that Hadley uses “pleased” less than or the same as the rest of the team. The evidence supports the idea that he has a much higher preference for it!\n\nHadley uses “pleased” quite a bit!"
  },
  {
    "objectID": "posts/2017-08-16-hadley-pleased/index.html#conclusion",
    "href": "posts/2017-08-16-hadley-pleased/index.html#conclusion",
    "title": "Which RStudio blog posts “pleased” Hadley? A tidytext + web scraping analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis post used a lot of different tools, but that’s the beauty of having over 12,000 R packages at our disposal. I think that this dataset could be used in a number of other ways, so be on the lookout for more posts!"
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "",
    "text": "After realizing how fast I can burn through my free 25 hours on shinyapps.io, I decided to repurpose my RStudio Server to also work with Shiny Server. Here’s my new setup:\nIn case I ever have to go through this madness again, or if anyone else wants to, I’ve compiled some step by step notes on the setup. It’s definitely worth it, though, so that you can have your own RStudio and Shiny servers!\n(I know that some others have already done posts like this, but I went into even more laborious detail on some of the basics.)\nIn this post, I will walk you through getting up and running with an RStudio server. In the next post, you’ll learn to get Shiny server working."
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-1-setup-an-aws-account",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-1-setup-an-aws-account",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Step 1: Setup an AWS Account",
    "text": "Step 1: Setup an AWS Account\nAmazon is nice enough to provide 1 year’s worth of access to their Free Tier for AWS. There are a huge number of options available, but the important one is that they provide a free 750 hours/month to deploy an EC2 instance. That’s just enough to keep 1 EC2 instance active 24/7, since 24 hours x 31 days = 744.\nIf you aren’t familiar with EC2, think of it as your own personal always-on Linux computer that you can connect to through SSH, and access through the web by using an IP address. One step further and you can access it through a custom domain name.\nCreate your free AWS account, and come back when you’ve finished. You should be able to click on the giant sign in button, and sign in to your console.\n\nIf all goes well, you’ll be at the console.\n\nWe won’t do anything else yet, just stay signed in."
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-2-setup-the-rstudio-amazon-machine-image",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-2-setup-the-rstudio-amazon-machine-image",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Step 2: Setup the RStudio Amazon Machine Image",
    "text": "Step 2: Setup the RStudio Amazon Machine Image\nIt’s worth it to get familiar with setting up your own EC2 server, but we won’t have to do that here. Luckily, Louis Aslett has created an Amazon Machine Image (AMI) to take care of all of the hard work for us. It’s basically some preconfigured settings that at the time of writing install the following:\n\nRStudio Server 0.99.903\nR 3.3.1\nShiny Server\nJulia 0.4.6\nPython 3.5.2\nGit\n\nYou can find the link to the image here. Click one of the links on the right to start the setup, I normally click the one closest to me regionally.\nThe Virginia link takes me here:\n\nYou can click through the settings, but to just get setup, click “Review and Launch.” It will let you review one last time, and will likely warn you about security, we will change all that later, just click “Launch.”\nImportant! Amazon will pop up a message box that talks about a key pair. This is how you will SSH into your server later on. This is really important, as you only get this screen one time, and can never come back to it. Setup a new key pair name (it can be anything), and click “Download Key Pair.”\n\nStore the .pem key pair file somewhere on your local computer. This should be a secure location, but somewhere you can remember the file path to. Then click Launch Instances.\nAt the top of the next screen, click Services, and then select EC2. This will take you to the EC2 Dashboard. You should see that you have “1 Running Instance.”\n\nClick on “1 Running Instance,” and you’ll see your server starting up. Below, it’s the one that says “running.”\n\nThere’s one last thing to do before we can access the server. We have to setup the security to allow HTTP (web browser) access. In the “Description” tab in the bottom half of the above image, scroll down until you see “Security Groups.” You’ll likely have something like “launch-wizard-1” there. Click on that.\nOn the next screen, click the “Inbound” tab down where “Description” is listed. As you can see, only the SSH option is available for accessing the instance. Let’s change that.\nClick:\n\nEDIT -> Add Rule -> Type set it to HTTP -> change the source from Custom to Anywhere -> Save\n\nNote that this is not a secure option, but it’ll get you going.\nFinally, to check that you’re up and running, go back to your instances tab (the same image as above). See the Public DNS (IPv4) box? Copy that, and paste it into your browser as a URL. It should take you to an authentication page for RStudio Server. Congrats! You’ve figured something out that took me hours.\nDefaults:\n\nUsername - rstudio\nPassword - rstudio\n\nClever, right?\n\nIf it worked, you should see this."
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-3-new-password-for-rstudio-server",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-3-new-password-for-rstudio-server",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Step 3: New password for RStudio Server",
    "text": "Step 3: New password for RStudio Server\nIt’s advised that you immediately change the password. There are two ways to do so. The first way is easy. In the Welcome.R file that is shown above, you’ll see a description for how to library(\"RStudioAMI\") and then run passwd(). You can do that, but eventually you’ll have to SSH into your server for something, so you may as well learn how now.\nHave you still got the AWS Console Instances page up? The one where you found the Public DNS (IPv4). Here it is again.\n\nAWS has made it pretty easy to connect through SSH. Click the “Connect” button. A window should pop up with some pretty detailed instructions. Do you have the path to your .pem file lying around? You’re going to need it!\n\nI run on a Mac, so I’ll be using Terminal. If you run on Windows, you’ll need to download PuTTY. Open up Terminal, and type in the following for step 3:\n\nchmod 400 path_to_file/file.pem\n\nNote that you actually need to locate your pem file, and pass Terminal the path. This command hides the file, and is necessary to connect.\nNext you’ll connect to your instance by typing:\n\nssh -i \"path_to_file/file.pem\" ubuntu@ec2-IPADDRESS.compute-1.amazonaws.com\n\nAgain, you’ll have to type in the correct path, but the IP address shown for you should be correct.\nWhen you connect for the first time, it might give you a prompt basically saying, “Are you sure?” Type yes. Hopefully you’ll see something like this:\n\n\n\n\n\nTo update the password for the rstudio user:\n\nsudo passwd rstudio\n\nThen follow the prompts. Type exit to disconnect from the server, and go back to your RStudio Server site. Try and login with the new password."
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-4-update-everything",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#step-4-update-everything",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Step 4: Update everything",
    "text": "Step 4: Update everything\nUnfortunately, the Amazon Images are only updated every few releases of RStudio Server. However, it’s not too hard to get the newest release installed straight from RStudio’s site.\nYou’ll need to first set the CRAN mirror on your Ubuntu server so that you can actually download the latest version of R. This part is a bit of a pain, requiring you to work with some text editors through Terminal, but bear with me.\nSign back into your Linux server through Terminal following the above instructions. When you’re done, type:\n\nls\n# rstudio-server-1.0.143-amd64.deb  shiny-server-1.5.3.838-amd64.deb\n\nAnd you should see a few .deb files, one for rstudio-server and one for shiny-server (mine are already upgraded). If you don’t, well, hopefully you can still try and follow along (Maybe cd ~ will get you there? Maybe go back to step 1?).\nNow, we need to navigate to the correct file and add the CRAN mirror to it. That is located at /etc/apt/sources.list for you pros. For the rest of us, follow along.\nFirst navigate up two levels:\n\ncd ../..\nls\n# bin   etc         initrd.img.old            lib         media  proc  sbin  sys  var\n# boot  home        jupyterhub_cookie_secret  lib64       mnt    root  snap  tmp  vmlinuz\n# dev   initrd.img  jupyterhub.sqlite         lost+found  opt    run   srv   usr  vmlinuz.old\n\nThen, we need to get into etc/apt:\n\ncd etc/apt\nls\n# apt.conf.d     sources.list    sources.list.save  trusted.gpg~\n# preferences.d  sources.list.d  trusted.gpg        trusted.gpg.d\n\nI don’t have a whole lot of experience with terminal editors, but I know enough to get by. I will use nano, which I believe comes on every Mac, to open up my sources.list file. sudo is likely needed to give admin privelages so you can save the file afterwards.\n\nsudo nano sources.list\n\nA file should open, scroll all the way down to the bottom, and on a new line paste:\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu/ xenial/\n\nThere are a number of different versions of this command here, but this specific one works because the Amazon Image you downloaded uses Xenial (tbh I don’t really know what that means, trial and error and a bit of common sense got it to work).\nNow you have to escape from nano, a first-timer’s nightmare. Follow this sequence of commands:\n\n^X # Control+X      This is used to \"Quit\"\nY  # Yes            This is used to save the file when it asks you\n# Then click Enter/Return on your keyboard to resave the file with the same name\n\nNow that that is taken care of, navigate back to:\n\ncd ~\n\nAnd you can update all of the linux apps, and then download the latest version of R using the two commands:\n\nsudo apt-get update\nsudo apt-get install r-base\n\nFinally, you’ll update to the latest version of RStudio Server. At the time of writing, this is 1.0.143, but it updates regularly, so go here and scroll down to find the latest update for 64bit Ubuntu. The commands generally look like:\n\nsudo apt-get install gdebi-core\nwget https://download2.rstudio.org/rstudio-server-1.0.143-amd64.deb\nsudo gdebi rstudio-server-1.0.143-amd64.deb\n\nAll done? Great! exit out of your Linux server, and reload your RStudio Server in the browser. When you login, you should be able to run version to see the latest version of R, and go to Help -> About RStudio to see the updated version of RStudio Server!"
  },
  {
    "objectID": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#last-words",
    "href": "posts/2017-05-15-rstudio-shiny-server-and-aws/index.html#last-words",
    "title": "RStudio and Shiny Servers with AWS - Part 1",
    "section": "Last words",
    "text": "Last words\nThis was quite the struggle. There are a few other resources out there to help, but I still struggled through some pieces of this one. Hopefully it wasn’t near as bad for you! In the next post, I’ll show you how to update your Shiny Server and start hosting your own apps on there (with no 5 app limit like shinyapps.io)!\nHere are some additional resources that I found helpful when setting up my server:\n\nhttps://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/#user-libraries"
  },
  {
    "objectID": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html",
    "href": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html",
    "title": "Data Frames as Vectors of Rows",
    "section": "",
    "text": "Recently, I have been working a lot on {vctrs}. This package is an attempt to analyze the atomic types in R, such as integer, character and double, alongside the recursive types of list and data.frame, to extract a set of common principles. From this analysis, a growing toolkit of functions for working with vector types has developed around two themes of size and prototype. vctrs is a fun package to work on, and even more fun to build on top of.\nIf you’ve never heard of vctrs before, there’s a reason for that. For the most part, it’s a developer focused package, and honestly if you never knew this package existed, but still used the higher level packages that were built on top of it, then we’ve done our job. A few examples of packages that rely heavily on vctrs right now are:"
  },
  {
    "objectID": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#c",
    "href": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#c",
    "title": "Data Frames as Vectors of Rows",
    "section": "c()",
    "text": "c()\nAs you gain more experience working with R, you eventually start to learn about how certain data structures are implemented. A data frame, for example, is really a list where each element of the list is a single column. In other words, a data frame is a vector of columns.\nOne way to see this is by the fact that length() returns the number of columns.\n\ndf <- tibble(\n  x = 1:4, \n  y = c(\"a\", \"b\", \"a\", \"a\"), \n  z = c(\"x\", \"x\", \"y\", \"x\")\n)\n\ndf\n#> # A tibble: 4 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     1 a     x    \n#> 2     2 b     x    \n#> 3     3 a     y    \n#> 4     4 a     x\n\n\nlength(df)\n#> [1] 3\n\nYou can also check that a data frame is a list by calling is.list().\n\nis.list(df)\n#> [1] TRUE\n\nThis underlying assumption that a data frame is a vector of columns is deeply rooted into a number of R’s core functions, and is often used as a fallback when behavior is otherwise ill-defined. Consider, as an example, calling c(df, df) to “combine” the data frame with itself.\n\nc(df, df)\n#> $x\n#> [1] 1 2 3 4\n#> \n#> $y\n#> [1] \"a\" \"b\" \"a\" \"a\"\n#> \n#> $z\n#> [1] \"x\" \"x\" \"y\" \"x\"\n#> \n#> $x\n#> [1] 1 2 3 4\n#> \n#> $y\n#> [1] \"a\" \"b\" \"a\" \"a\"\n#> \n#> $z\n#> [1] \"x\" \"x\" \"y\" \"x\"\n\nOur data frames have combined to become a list! In a way, this behavior is consistent with the principle that a data frame is a list of columns. It follows the invariant (read: “unbreakable principle”) of:\n\n\nlength(c(x, y)) == length(x) + length(y)\n\n\n\nlength(df) + length(df)\n#> [1] 6\n\nlength(c(df, df)) == length(df) + length(df)\n#> [1] TRUE\n\nIs there any other type of output that makes sense? If we think of a data frame as a vector of columns, then no, because it makes sense to end up with something of length 6 after combining. However, I’d argue that if we flip our understanding of data frames from a vector of columns to a vector of rows, then another solution comes forward which offers a different result that I have begun to find pretty attractive."
  },
  {
    "objectID": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#vec_size",
    "href": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#vec_size",
    "title": "Data Frames as Vectors of Rows",
    "section": "vec_size()",
    "text": "vec_size()\nMuch of this particular section is adapted from the vctrs vignette on size.\nTo start the process of thinking about a “vector of rows”, look again to df. A single “row” would be:\n\ndf[1,]\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     1 a     x\n\nWith that in mind, df would be considered a vector of 4 rows. It would be nice to have a function that returned this information as a building block to work off of (as shown before, length() won’t do). We could try and use nrow(), which gives us what we want, but it returns NULL when given an individual column.\n\nnrow(df)\n#> [1] 4\n\nnrow(df$x)\n#> NULL\n\nWhat I’m looking for is a function that returns a value that is equivalent for the data frame itself and for any column of the data frame. Put another way, I’m really after the “number of observations”. One other option is to use NROW().\n\nNROW(df)\n#> [1] 4\n\nNROW(df$x)\n#> [1] 4\n\nThis looks good, but what happens if you give it some “non-vector” input? A practical way to think about that is something that isn’t allowed to exist as a column of a data frame, for example, a function, or an lm object.\n\ndata.frame(x = mean)\n#> Error in as.data.frame.default(x[[i]], optional = TRUE): cannot coerce class '\"function\"' to a data.frame\n\nNROW(mean)\n#> [1] 1\n\nlm_cars <- lm(mpg ~ cyl, data = mtcars)\n\ndata.frame(x = lm_cars)\n#> Error in as.data.frame.default(x[[i]], optional = TRUE, stringsAsFactors = stringsAsFactors): cannot coerce class '\"lm\"' to a data.frame\n\n# Treats it as a list\nNROW(lm_cars)\n#> [1] 12\n\nThese objects are considered scalar types rather than vector types. They are “scalar” in the sense that you only ever consider them one at a time. Even though lm_cars is technically implemented as a list, we look at it is as a single linear model object.\nCompare that to a double vector like c(1, 2, 3) which is made up of 3 observations.\nFor our purposes, it’s valuable to keep scalar and vector types distinct, so it would be nice if an error was thrown for scalars to indicate that they don’t really have this “number of observations” property that we are after.\nMotivated by this, the concept of size was created in vctrs to capture the invariants that were desired. In particular:\n\nIt is the length of 1d vectors.\nIt is the number of rows of data frames, matrices, and arrays.\nIt throws error for non vectors.\n\nThe vctrs function, vec_size(), is the resulting implementation of this concept.\n\nvec_size(df)\n#> [1] 4\n\nvec_size(df$x)\n#> [1] 4\n\nvec_size(mean)\n#> Error in `vec_size()`:\n#> ! `x` must be a vector, not a function."
  },
  {
    "objectID": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#vec_c",
    "href": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#vec_c",
    "title": "Data Frames as Vectors of Rows",
    "section": "vec_c()",
    "text": "vec_c()\nArmed with the concept of size, let’s take another look at c(df, df). If the length invariant for c() looks like:\n\n\nlength(c(x, y)) == length(x) + length(y)\n\n\nthen imagine what would happen if length() was swapped with vec_size():\n\n\nvec_size(c(x, y)) =?= vec_size(x) + vec_size(y)\n\n\nDoes this invariant hold? For 1d vectors, it does because vec_size() and length() are essentially the same. But for data frames, we’ve seen that vec_size() and length() are different, and c() was built with length() in mind, so it might not. In fact, looking at our original example of c(df, df) proves that it doesn’t hold:\n\nvec_size(df) + vec_size(df)\n#> [1] 8\n\nvec_size(c(df, df))\n#> [1] 6\n\nWe’d like this invariant to hold, but c() isn’t the right tool for the job. Instead, an alternative, vec_c(), was built that builds off of vec_size() and this invariant. The invariant that does hold actually looks like:\n\n\nvec_size(vec_c(x, y)) == vec_size(x) + vec_size(y)\n\n\nSo what does vec_c() do? For 1d vectors it acts like c(), as you might expect.\n\nvec_c(1:2, 3)\n#> [1] 1 2 3\n\nBut what about with vec_c(df, df)? Based on the fact that vec_size(df) + vec_size(df) = 8, at the very least we know it should return something with a size of 8. To accomplish this, rather than coercing to a list and combining the columns together, vec_c() instead leaves them as data frames and combines the rows.\n\nvec_c(df, df)\n#> # A tibble: 8 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     1 a     x    \n#> 2     2 b     x    \n#> 3     3 a     y    \n#> 4     4 a     x    \n#> 5     1 a     x    \n#> 6     2 b     x    \n#> 7     3 a     y    \n#> 8     4 a     x\n\nIf you view a data frame as a vector of rows, this makes complete sense. We start with a vector of 4 rows, and add another vector of 4 rows, so we should end up with a vector of 8 rows, i.e. a data frame with size 8.\nWhat happens if we combine df with a data frame containing one row but an entirely new column? Again, we “know” from our size invariant that we should get something back with a size of 5.\n\ndf_w <- tibble(w = 1)\n\nvec_c(df, df_w)\n#> # A tibble: 5 × 4\n#>       x y     z         w\n#>   <int> <chr> <chr> <dbl>\n#> 1     1 a     x        NA\n#> 2     2 b     x        NA\n#> 3     3 a     y        NA\n#> 4     4 a     x        NA\n#> 5    NA <NA>  <NA>      1\n\nThe result is a data frame with 5 rows, and a union of the columns coming from each of the two individual data frames. Without getting too much into it, the fact that the result is a “tibble with 4 columns: x (int), y (chr), z (chr), and w (dbl)” comes from the other half of what vctrs offers, the prototype. vec_c() found the “common type” between df and df_w, which is the union holding those 4 columns.\nWhat about combining df with something like the double vector, c(1, 2)? We’d expect a size of 6 (4 from df and 2 from the vector), but we actually get an error because the size is only half of the story.\n\nvec_c(df, c(1, 2))\n#> Error:\n#> ! Can't combine `..1` <tbl_df> and `..2` <double>.\n\nIn this case, there is no common type between a data frame and a double vector, so you can’t combine them together.\nCompare that with the result from c() which upholds its length invariant giving a result of length 5.\n\nc(df, c(1, 2))\n#> $x\n#> [1] 1 2 3 4\n#> \n#> $y\n#> [1] \"a\" \"b\" \"a\" \"a\"\n#> \n#> $z\n#> [1] \"x\" \"x\" \"y\" \"x\"\n#> \n#> [[4]]\n#> [1] 1\n#> \n#> [[5]]\n#> [1] 2"
  },
  {
    "objectID": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#vec_match",
    "href": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#vec_match",
    "title": "Data Frames as Vectors of Rows",
    "section": "vec_match()",
    "text": "vec_match()\nTreatment of a data frame as a vector of rows extends well past vec_c(), and bleeds into many other vctrs tools where we have been experimenting with this idea. As one more example, we’ll take a look at match(). If you aren’t familiar with match(), it tells you the location of x inside a table. Another way to think about this is that you want to find a needle in a haystack. More concretely:\n\n# Where is `\"a\"` inside the vector `c(\"b\", \"c\", \"a\", \"d\")`?\nmatch(x = \"a\", table = c(\"b\", \"c\", \"a\", \"d\"))\n#> [1] 3\n\nNow imagine that I want to use a data frame as my table. I might be interested in locating a few particular rows inside that table.\n\nneedles <- tibble(x = 3:4, y = c(\"a\", \"b\"), z = c(\"y\", \"y\"))\nhaystack <- df\n\nneedles\n#> # A tibble: 2 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     3 a     y    \n#> 2     4 b     y\n\nhaystack\n#> # A tibble: 4 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     1 a     x    \n#> 2     2 b     x    \n#> 3     3 a     y    \n#> 4     4 a     x\n\nHere, the first row of needles is row 3 of the haystack, and the second row of needles is not in the haystack at all. Let’s try with match().\n\nmatch(needles, haystack)\n#> [1] NA NA NA\n\n🤔 so what happened here? With R’s (completely reasonable) treatment of needles as a vector of columns, it essentially first converted needles and haystack into lists, and then tried to locate each column of needles inside haystack. There were 3 columns, and none of them were found, so NA was returned 3 times. To actually see a match, we could instead provide a list containing the y column of haystack.\n\nmatch(list(haystack$y), haystack)\n#> [1] 2\n\nIf we instead treat both needles and haystack as vectors of rows, what we are really trying to do is find one set of rows inside another set of rows. In vctrs, we’ve created vec_match() for this.\n\nvec_match(needles, haystack)\n#> [1]  3 NA\n\nAgain, it’s not that anything R is doing is wrong, or even that this is “better”. This just answers a different question by looking at a data frame from a different angle. Additionally, we don’t actually lose anything in vctrs by thinking about data frames in this way. If we want the match() behavior, we can just unclass() our data frames to turn them into explicit lists, and then vec_match() works exactly the same.\nEven though c() and match() treat data frames as vectors of columns, not all R functions do. At the end of the post I discuss how split() and unique() actually treat them as vectors of rows, and what the vctrs equivalents are."
  },
  {
    "objectID": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#slide",
    "href": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#slide",
    "title": "Data Frames as Vectors of Rows",
    "section": "slide()",
    "text": "slide()\nLastly, I’d like to show an example of a package that builds on top of vctrs principles. {slider} is my attempt at a package for working with “window functions”, functions that enable some kind of “rolling” analysis. A moving average, rolling regression, and even a cumulative sum are all examples of usage of window functions.\n\nlibrary(slider)\n\nslide() works similarly to purrr::map() in that you provide it a vector, .x, and a function, .f, to apply to each slice of .x. One difference is that you have additional options to control the window of .x you apply .f to. For example, below we construct a sliding window of size 3, asking for “the current value along with 2 values before this one”. The function we apply is to just print out the current value of .x so we can see what is happening.\n\nslide(1:5, ~.x, .before = 2)\n#> [[1]]\n#> [1] 1\n#> \n#> [[2]]\n#> [1] 1 2\n#> \n#> [[3]]\n#> [1] 1 2 3\n#> \n#> [[4]]\n#> [1] 2 3 4\n#> \n#> [[5]]\n#> [1] 3 4 5\n\nWe could also perform a rolling average by switching ~.x for mean, and, like with purrr, replacing slide() with slide_dbl().\n\nslide_dbl(1:5, mean, .before = 2)\n#> [1] 1.0 1.5 2.0 3.0 4.0\n\nBecause slide() builds on vctrs, it is meaningful to talk about the invariants of the function. For example, the size invariant of slide() is that:\n\n\nvec_size(slide(.x)) == vec_size(.x)\n\n\nIn other words, slide() always returns an output that has the same size as its input. This is similar to how map() works, with one major difference. Like c(), map() returns a vector with the same length as its input. This means that map() treats a data frame as a vector of columns.\n\nlibrary(purrr)\nmap(df, ~.x)\n#> $x\n#> [1] 1 2 3 4\n#> \n#> $y\n#> [1] \"a\" \"b\" \"a\" \"a\"\n#> \n#> $z\n#> [1] \"x\" \"x\" \"y\" \"x\"\n\nA major breakthrough for me was that, to uphold the invariant, slide() must treat a data frame as a vector of rows, meaning that it should iterate rowwise over .x.\n\nslide(df, ~.x)\n#> [[1]]\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     1 a     x    \n#> \n#> [[2]]\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     2 b     x    \n#> \n#> [[3]]\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     3 a     y    \n#> \n#> [[4]]\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     4 a     x\n\nThis provides an alternative to some pmap() solutions that have been used previously, like the ones in Jenny Bryan’s GitHub repo of row oriented workflows. Consider this example modified from the repo, where you have a data frame of parameters that you want to pass on to runif() in order to call it multiple times with different parameter combinations. Additionally, the column names don’t currently match the argument names of runif(), so you either have to rename on the fly, or wrap it with a function.\n\nlibrary(dplyr)\n\nparameters <- tibble(\n  n = 1:3,\n  minimum = c(0, 10, 100),\n  maximum = c(1, 100, 1000)\n)\n\nset.seed(12)\n\nparameters %>%\n  rename(min = minimum, max = maximum) %>%\n  pmap(runif)\n#> [[1]]\n#> [1] 0.06936092\n#> \n#> [[2]]\n#> [1] 83.59977 94.83596\n#> \n#> [[3]]\n#> [1] 342.4437 252.4133 130.5061\n\nset.seed(12)\n\nmy_runif <- function(n, minimum, maximum) {\n  runif(n, minimum, maximum)\n}\n\npmap(parameters, my_runif)\n#> [[1]]\n#> [1] 0.06936092\n#> \n#> [[2]]\n#> [1] 83.59977 94.83596\n#> \n#> [[3]]\n#> [1] 342.4437 252.4133 130.5061\n\nWith slide() being a row wise iterator, you have access to the entire data frame row at each iteration as .x, meaning you can just do:\n\nset.seed(12)\nslide(parameters, ~runif(n = .x$n, min = .x$minimum, max = .x$maximum))\n#> [[1]]\n#> [1] 0.06936092\n#> \n#> [[2]]\n#> [1] 83.59977 94.83596\n#> \n#> [[3]]\n#> [1] 342.4437 252.4133 130.5061"
  },
  {
    "objectID": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#conclusion",
    "href": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#conclusion",
    "title": "Data Frames as Vectors of Rows",
    "section": "Conclusion",
    "text": "Conclusion\nTreatment of a data frame as a vector of rows is a fairly novel concept in R, because of the way that data frames were originally implemented as a list of columns. But viewing them in this way can be incredibly powerful, especially for data analysis work. I, for one, am looking forward to seeing this concept explored more in the future, both in vctrs and in other packages built on top of it."
  },
  {
    "objectID": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#extra---unique-and-split",
    "href": "posts/2019-10-16-data-frames-as-a-vector-of-rows/index.html#extra---unique-and-split",
    "title": "Data Frames as Vectors of Rows",
    "section": "Extra - unique() and split()\n",
    "text": "Extra - unique() and split()\n\nOn the vctrs side, we are trying to be consistent in our treatment of data frames as vectors of rows. However, it is worth mentioning that there are some functions in R where data frames are already treated this way, rather than as a vector of columns. Two in particular are unique() and split().\nunique()\nWith unique(), uniqueness is actually determined using data frame rows, not columns. Looking at columns y and z of df, we can see that rows 1 and 4 are duplicates. Calling unique() on this removes the duplicate row.\n\ndf_yz <- df[, c(\"y\", \"z\")]\n\ndf_yz\n#> # A tibble: 4 × 2\n#>   y     z    \n#>   <chr> <chr>\n#> 1 a     x    \n#> 2 b     x    \n#> 3 a     y    \n#> 4 a     x\n\n\nunique(df_yz)\n#> # A tibble: 3 × 2\n#>   y     z    \n#>   <chr> <chr>\n#> 1 a     x    \n#> 2 b     x    \n#> 3 a     y\n\nIt’s actually pretty interesting to see how this one works. If you look into unique.data.frame(), you’ll see that it calls duplicated.data.frame(). In there is this somewhat cryptic line that actually does the rowwise check:\n\nduplicated(\n  do.call(Map, `names<-`(c(list, x), NULL)), \n  fromLast = fromLast\n)\n\nBreaking this down, it first combines the function list() with the data frame to end up with:\n\nc(list, df_yz)\n#> [[1]]\n#> function (...)  .Primitive(\"list\")\n#> \n#> $y\n#> [1] \"a\" \"b\" \"a\" \"a\"\n#> \n#> $z\n#> [1] \"x\" \"x\" \"y\" \"x\"\n\nwhich it then removes the names of:\n\n`names<-`(c(list, df_yz), NULL)\n#> [[1]]\n#> function (...)  .Primitive(\"list\")\n#> \n#> [[2]]\n#> [1] \"a\" \"b\" \"a\" \"a\"\n#> \n#> [[3]]\n#> [1] \"x\" \"x\" \"y\" \"x\"\n\nNext it uses do.call() to call Map(), which is a wrapper around mapply() meaning that it will repeatedly call list() on parallel elements of the columns of df_yz. Visually that means we end up with list elements holding the rows, which duplicated() is then run on to locate the duplicates.\n\ndo.call(Map, `names<-`(c(list, df_yz), NULL))\n#> $a\n#> $a[[1]]\n#> [1] \"a\"\n#> \n#> $a[[2]]\n#> [1] \"x\"\n#> \n#> \n#> $b\n#> $b[[1]]\n#> [1] \"b\"\n#> \n#> $b[[2]]\n#> [1] \"x\"\n#> \n#> \n#> $a\n#> $a[[1]]\n#> [1] \"a\"\n#> \n#> $a[[2]]\n#> [1] \"y\"\n#> \n#> \n#> $a\n#> $a[[1]]\n#> [1] \"a\"\n#> \n#> $a[[2]]\n#> [1] \"x\"\n\nduplicated(do.call(Map, `names<-`(c(list, df_yz), NULL)))\n#> [1] FALSE FALSE FALSE  TRUE\n\nIn vctrs there is vec_unique(). Because unique() already works row wise, they are essentially equivalent in terms of functionality with data frames. However, there are two key differences. First, because vec_unique()’s handling of data frames is in C, it does end up being faster.\n\n# row bind df_yz 10000 times, making a 40000 row data frame\nlarge_df <- vec_rbind(!!!rep_len(list(df_yz), 10000))\ndim(large_df)\n#> [1] 40000     2\n\n\nbench::mark(\n  unique(large_df),\n  vec_unique(large_df)\n)\n\n\n## # A tibble: 2 x 6\n##   expression                min   median `itr/sec` mem_alloc `gc/sec`\n##   <bch:expr>           <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n## 1 unique(large_df)      28.38ms   31.6ms      31.7    1.42MB     137.\n## 2 vec_unique(large_df)   4.34ms    4.6ms     216.   429.61KB       0\n\nSecond, unique() doesn’t handle the idea of a packed data frame well. This is a relatively new idea in the tidyverse, and it isn’t one that we want to expose users to very much yet, but it is powerful. A packed data frame is a data frame where one of the columns is another data frame. This is different from a list-column of data frames. You can create one by providing tibble() another tibble() as a column along with a name for that data frame column.\n\ndf_packed <- tibble(\n  x = tibble(\n    a = c(1, 1, 1, 3), \n    b = c(1, 1, 3, 3)\n  ), \n  y = c(1, 1, 1, 2)\n)\n\n# Both `$a` and `$b` are columns in the data frame column, `x`\ndf_packed\n#> # A tibble: 4 × 2\n#>     x$a    $b     y\n#>   <dbl> <dbl> <dbl>\n#> 1     1     1     1\n#> 2     1     1     1\n#> 3     1     3     1\n#> 4     3     3     2\n\n# `x` itself is another data frame\ndf_packed$x\n#> # A tibble: 4 × 2\n#>       a     b\n#>   <dbl> <dbl>\n#> 1     1     1\n#> 2     1     1\n#> 3     1     3\n#> 4     3     3\n\nEven though you can create one of these, as an end user there isn’t much yet that you can do with them, so you shouldn’t have to worry about this very much.\nLooking at df_packed, the unique rows are 1, 3, and 4, which vec_unique() can determine, but unique() doesn’t correctly pick up on.\n\nvec_unique(df_packed)\n#> # A tibble: 3 × 2\n#>     x$a    $b     y\n#>   <dbl> <dbl> <dbl>\n#> 1     1     1     1\n#> 2     1     3     1\n#> 3     3     3     2\n\nunique(df_packed)\n#> # A tibble: 3 × 2\n#>     x$a    $b     y\n#>   <dbl> <dbl> <dbl>\n#> 1     1     1     1\n#> 2     1     1     1\n#> 3     3     3     2\n\nAs packed data frames become more prevalent in the tidyverse, it will be nice to have tools that handle them consistently. For example, there is already tidyr::pack() and tidyr::unpack(), which helps power tidyr::unnest().\nsplit()\nsplit(x, by) will divide up x into groups using by to determine where the unique groups are. It assumes by is a factor, and will coerce your input to a factor if it isn’t already one. Like unique(), it will slice up a data frame by rows rather than by columns.\n\nsplit(df, df$y)\n#> $a\n#> # A tibble: 3 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     1 a     x    \n#> 2     3 a     y    \n#> 3     4 a     x    \n#> \n#> $b\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     2 b     x\n\nOne thing about split() is that it uses the unique values as labels on the list elements. To take a slightly different approach, vec_split() was created, which returns a data frame instead, holding the unique key values in their own parallel column. vec_split() returns a data frame to keep vctrs lightweight, but the print method for these can be a little complex, and I think tibble’s print method does a nicer job.\n\ndf_split <- vec_split(df, df$y)\ndf_split <- as_tibble(df_split)\n\ndf_split\n#> # A tibble: 2 × 2\n#>   key   val             \n#>   <chr> <list>          \n#> 1 a     <tibble [3 × 3]>\n#> 2 b     <tibble [1 × 3]>\n\ndf_split$val\n#> [[1]]\n#> # A tibble: 3 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     1 a     x    \n#> 2     3 a     y    \n#> 3     4 a     x    \n#> \n#> [[2]]\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     2 b     x\n\nOne useful feature of vec_split() is that it doesn’t expect a factor as the second argument, which means that a data frame can be provided to split by, and since uniqueness is determined row wise this allows us to split by multiple columns. The key ends up as the unique rows of the data frame, meaning that the key is actually a data frame column, creating a packed data frame!\n\ndf_multi_split <- vec_split(df, df[c(\"y\", \"z\")])\ndf_multi_split <- as_tibble(df_multi_split)\n\ndf_multi_split\n#> # A tibble: 3 × 2\n#>   key$y $z    val             \n#>   <chr> <chr> <list>          \n#> 1 a     x     <tibble [2 × 3]>\n#> 2 b     x     <tibble [1 × 3]>\n#> 3 a     y     <tibble [1 × 3]>\n\nTechnically you can provide split() a data frame to split by, but remember that it will try and treat it like a factor! Since the data frame is technically a list, it will run interaction() on it first to get a single factor it can use to split by. Notice that this gives a level for b.y which does not exist as a row in our data frame.\n\ndf[c(\"y\", \"z\")]\n#> # A tibble: 4 × 2\n#>   y     z    \n#>   <chr> <chr>\n#> 1 a     x    \n#> 2 b     x    \n#> 3 a     y    \n#> 4 a     x\n\ninteraction(df[c(\"y\", \"z\")])\n#> [1] a.x b.x a.y a.x\n#> Levels: a.x b.x a.y b.y\n\nThis results in the following split, with a b.y element with no rows which we may or may not have wanted.\n\nsplit(df, df[c(\"y\", \"z\")])\n#> $a.x\n#> # A tibble: 2 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     1 a     x    \n#> 2     4 a     x    \n#> \n#> $b.x\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     2 b     x    \n#> \n#> $a.y\n#> # A tibble: 1 × 3\n#>       x y     z    \n#>   <int> <chr> <chr>\n#> 1     3 a     y    \n#> \n#> $b.y\n#> # A tibble: 0 × 3\n#> # … with 3 variables: x <int>, y <chr>, z <chr>"
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html",
    "title": "Writing a paper with RStudio",
    "section": "",
    "text": "This semester I had to write a paper for my Financial Econometrics class. My topic was on analyzing the volatility of Bitcoin using GARCH modeling. I’m not particularly interested in Bitcoin, but with all the recent news around it, and with its highly volatile characteristics, I figured it would be a good candidate for analysis.\nI did the analysis in R, but I wanted to take it a step further. Could I write the entire paper in R and RStudio in a fairly professional format? Yes.\n\nI figured I would outline a few issues I had along the way, and talk about the experience for anyone that might do something similar."
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#github",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#github",
    "title": "Writing a paper with RStudio",
    "section": "Github",
    "text": "Github\nIf you want to go view the entire paper and analysis, it’s on Github. Check out the repo here. The PDF paper itself is buried here.\nIf you really want to follow the analysis steps, look in R/ to see the code that generates everything else. Do note that I had to keep the raw data zipped to get it on Github, so to run the data cleaning script, you will have to unzip the file in data/raw/. The paper is written in paper/."
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#tooling",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#tooling",
    "title": "Writing a paper with RStudio",
    "section": "Tooling",
    "text": "Tooling\nTo even begin thinking about this, I needed two R packages. One that allowed me to write the post in RMarkdown and render it into a journal style format, and one that created nice looking and customizable tables.\nFor the first, I initially looked into the rticles package from the RStudio team, which is amazing, but I wasn’t satisfied with the journal styles that they had as options. I then remembered that Dirk Eddelbuettel and James Balamuta had created pinp, or, Pinp Is Not PNAS, as an extension of rticles and in particular the PNAS journal format. This one fit the bill for me, as it provided an uncluttered journal layout with a bit of nice coloring as well.\nFor the second, there are plenty of options out there for creating LaTeX tables in R, xtable, stargazer, etc. However, I recently had found huxtable and was instantly drawn to its intuitive pipeable syntax for creating tables from data frames. Now that I have a good bit of experience with it, I can say for certain that it is incredibly flexible, and I doubt I’ll ever use another package for table creation.\nI was initially pretty worried about how well the two would play together. Sure enough, after rendering a table or two I got errors that (for a non LaTeX expert) seemed faily cryptic. However, it turned out to be a LaTeX package (not R package) dependency problem where huxtable needed packages that pinp didn’t use by default. That was easily fixed by adding the following to the top of the YAML header provided by pinp:\nheader-includes:\n   - \\usepackage{tabularx,colortbl,multirow,hhline,mathtools}\nI’ll also add in there that I used the rugarch package for all of my GARCH modeling, and it exceeded my expectations. Its author Alexios has created a fantastic S4 class system that makes trying different versions of GARCH models dead simple."
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#huxtable",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#huxtable",
    "title": "Writing a paper with RStudio",
    "section": "Huxtable",
    "text": "Huxtable\nI really wish huxtable had more publicity. It is a highly underused package for everything that it can do. For example, the following table was created using huxtable. Notice the math symbols in the first column, the fact that the standard errors are closer to the row above them due to smaller margins, and the bolding of certain cells.\n\nThe tibble used to create the core table looked like this:\n\n#> # A tibble: 14 × 4\n#>    metric                   `5-Min`    Daily `5-Min No Outlier`\n#>    <chr>                      <dbl>    <dbl>              <dbl>\n#>  1 MAPE                    0.0278   0.0372             0.0249  \n#>  2 RMSE                    0.00407  0.00285            0.00244 \n#>  3 MZ Intercept            0.00226  0.00167            0.00127 \n#>  4 MZ Intercept Std. Error 0.000271 0.000486           0.000314\n#>  5 MZ Slope                0.282    0.441              0.798   \n#>  6 MZ Slope Std. Error     0.0540   0.144              0.113   \n#>  7 MZ $R^2$                0.200    0.0792             0.316   \n#>  8 MAPE                    0.281    0.325              0.279   \n#>  9 RMSE                    0.00788  0.00711            0.00700 \n#> 10 MZ Intercept            0.00250  0.00232            0.00164 \n#> 11 MZ Intercept Std. Error 0.000756 0.00127            0.000963\n#> 12 MZ Slope                0.161    0.196              0.607   \n#> 13 MZ Slope Std. Error     0.151    0.376              0.347   \n#> 14 MZ $R^2$                0.0104   0.00249            0.0275\n\nAnd the R code to generate the table looked like:\n\nlibrary(huxtable)\n\ntriple_blank <- function() {\n  c(\"\", \"\", \"\")\n}\n\nht <- hux_data %>%\n  huxtable() %>%\n  \n  # Column names\n  add_colnames() %>%\n  \n  # Proxy rows\n  insert_row(\"Proxy: RV\",    triple_blank(), after = 1) %>%\n  insert_row(\"Proxy: $r^2$\", triple_blank(), after = 9) %>%\n  \n  # Number rounding\n  set_number_format(everywhere, everywhere, \"%5.4f\") %>%\n  \n  # Bold\n  set_bold(matrix(c(2,10)), 1, TRUE) %>%\n  set_bold(1, everywhere, TRUE) %>%\n  \n  # Alignment\n  set_align(everywhere, everywhere, 'center') %>%\n  set_align(everywhere, 1, 'right') %>%\n  \n  # Padding\n  set_all_padding(value = 10) %>%\n  set_top_padding(c(6, 8, 14, 16), everywhere, 0) %>%\n  set_bottom_padding(c(5, 7, 13, 15), everywhere, 0) %>%\n  set_left_padding(everywhere, 1, -40) %>%\n\n  # Borders\n  set_bottom_border(matrix(c(1, 9)), everywhere, value = .3) %>%\n  set_right_border(everywhere, 1, .3) %>%\n  \n  # Escape latex\n  set_escape_contents(everywhere, 1, FALSE) %>%\n  set_escape_contents(1, everywhere, FALSE) %>%\n  \n  set_caption(\"Out of sample performance of GARCH(1,1) Normal models. MAPE for 5-min is lower than for daily. Across the board, using RV as a proxy over $r^2$ gives more accurate results. Removing the 1 extreme forecast from the 5-min method results in a much higher MZ $R^2$, and a MZ slope much closer to 1.\") \n\nht[1,1] <- \"\"\n\nht\n\nYou even get to view your table in the console without knitting the document to check that you put lines / bolding / even coloring in the right place. This saves more time than you might think!\n\n\n\n\n\nI find the pipeable syntax very intuitive, and the family of set_*() functions allow for endless combinations. I didn’t even venture into the world of conditional cell formatting, but I hear that that is pretty powerful too."
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#adding-images",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#adding-images",
    "title": "Writing a paper with RStudio",
    "section": "Adding images",
    "text": "Adding images\nI had created a script in my analysis that generated a number of graphics that I wanted to include in the paper. The pinp vignette is an excellent source of examples for these common use cases, and the following allowed me to embed my images in the paper and add a caption with minimal effort:\n\\begin{figure*}\n  \\begin{center}\n    \\includegraphics[width=1.00\\textwidth, height=8.5in]{../../visualizations/returns} \n  \\end{center}\n  \\caption{Descriptive plots of 5-minute and daily returns}\\label{fig}\n\\end{figure*}\nNotice the relative path ../../visualizations/returns where I backtrack up two levels from the location of the RMarkdown paper document and then into my visualizations folder. My file structure looked a bit like this:\nfin-econ-project/\n\n  - fin-econ-project.Rproj\n  \n  - visualizations/\n    - returns.png\n    \n  - paper/\n    - forecasting-volatility/\n        - forecasting-volatility.Rmd\nIdeally, I would have set my directory to be the RStudio Project directory so I could have just done visualizations/returns, but pinp and rticles both dump a large number of files into whatever directory you render it from, and I didn’t want that cluttering things up. Perhaps specifying the location of that file dump can be a separate feature?"
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#adding-equations",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#adding-equations",
    "title": "Writing a paper with RStudio",
    "section": "Adding equations",
    "text": "Adding equations\nAdding mathematical equations can be done in two main ways.\nAs usual, you can add inline math with the use of a dollar sign, then the equation, then end with a dollar sign, it ends up looking like this: \\(x + y = z\\). Make sure that you don’t leave a space between the dollar signs and the equation, otherwise it doesn’t render and you end up with: $ x + y = z $.\nLarger chunks of equations that need their own lines can be specified using two dollar signs on each side:\n$$ \nr_{t_i}         = \\epsilon_{t_i} \\\\\n\\epsilon_{t_i}  = \\sigma_{t_i}  z_{t_i} \\\\\n\\sigma_{t_i}    = \\alpha \\epsilon_{t_{i-1}}  + \\beta \\sigma_{t_{i-1}}\n$$\n\\[\nr_{t_i}         = \\epsilon_{t_i} \\\\\n\\epsilon_{t_i}  = \\sigma_{t_i}  z_{t_i} \\\\\n\\sigma_{t_i}    = \\alpha \\epsilon_{t_{i-1}}  + \\beta \\sigma_{t_{i-1}}\n\\]\nUnfortunately, this doesn’t align the equations at the equal sign, and I think that that looks pretty nice. To do this, you can add &= instead of just = along with adding \\begin{aligned} and \\end{aligned} before and after the equation.\n$$\n\\begin{aligned}\n  r_{t_i}         &= \\epsilon_{t_i} \\\\\n  \\epsilon_{t_i}  &= \\sigma_{t_i}  z_{t_i} \\\\\n  \\sigma_{t_i}    &= \\alpha \\epsilon_{t_{i-1}}  + \\beta \\sigma_{t_{i-1}}\n\\end{aligned}\n$$\nto get:\n\\[\n\\begin{aligned}\n  r_{t_i}         &= \\epsilon_{t_i} \\\\\n  \\epsilon_{t_i}  &= \\sigma_{t_i}  z_{t_i} \\\\\n  \\sigma_{t_i}    &= \\alpha \\epsilon_{t_{i-1}}  + \\beta \\sigma_{t_{i-1}}\n\\end{aligned}\n\\]\nNice!"
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#overall",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#overall",
    "title": "Writing a paper with RStudio",
    "section": "Overall",
    "text": "Overall\nFor the most part, I enjoyed writing the paper straight from RStudio. Once I figured out a few of the pain points with directory locations, images, equations, and dependencies, the process was pretty smooth. The only other comment I have is that the pinp Knit process is a bit slow. I doubt this has too much to do with the implementation, but more with the underlying rendering engines. I wish there was some way to have Live Rendering like with blogdown so that I could just keep a rendered version of the paper up and have it reload every time I save. That would be the dream!"
  },
  {
    "objectID": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#update",
    "href": "posts/2017-12-09-writing-a-paper-with-rstudio/index.html#update",
    "title": "Writing a paper with RStudio",
    "section": "Update",
    "text": "Update\nThanks to Dirk, the dream has come true."
  },
  {
    "objectID": "posts/2018-03-06-copula-resources/index.html",
    "href": "posts/2018-03-06-copula-resources/index.html",
    "title": "Copula Resources",
    "section": "",
    "text": "NC State University lecture notes - A consise introduction into copulas. This is a good place to start but not a good place to find examples.\nCopulas for Finance: A Reading Guide and Some Applications - A much more intense survey of copulas. Math heavy but full of examples. Page 20 includes a few simulation techniques, but overcomplicates the simple ones (Gaussian copula) in order to stay general.\nWiki: Copulas - The mathematical definition section, along with Sklar’s theorem and the section of Gaussian Copulas makes this worthwhile to look at.\nStackExchange answer - This was the missing link for me. A great description of the “point of a copula.” The full explanation in the last paragraph of the answer made things click for me.\nDataScience+ - A good walkthrough of using the copula R package. Includes an example of using a t-copula with normal marginals.\ncopula - The copula R package. A one stop shop for your copula needs if you use R and don’t want to (or have to) implement it all yourself.\nMatlab example - Obviously the code is all in matlab, but they do a nice job of explaining how to simulate from copulas, without the theory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Davis Vaughan",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Davis, a Software Engineer at RStudio working on creating user friendly R packages with the rest of the tidyverse team."
  },
  {
    "objectID": "about.html#what-else",
    "href": "about.html#what-else",
    "title": "About",
    "section": "What else?",
    "text": "What else?\nI’ve developed a few R packages:\n\nfurrr\nslider\nclock\nalmanac\nhardhat\nworkflows\nprobably\n\nAnd I work on many more, including:\n\ntidyr\nvctrs\nyardstick"
  }
]